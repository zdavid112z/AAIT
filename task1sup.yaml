trainer:
  max_epochs: -1
  accumulate_grad_batches: 1
  num_sanity_val_steps: 2
  logger:
    class_path: lightning.pytorch.loggers.WandbLogger
    init_args:
      project: aait_imrec
      log_model: false
  log_every_n_steps: 50
  callbacks:
    - class_path: lightning.pytorch.callbacks.ModelCheckpoint
      init_args:
        dirpath: ./checkpoints
        auto_insert_metric_name: false
        filename: "epoch={epoch}_step={step}_val_0_loss={val_0/loss:.4f}"
        save_last: false
        save_top_k: 100
        every_n_epochs: 1
        monitor: val_0/loss
    - class_path: lightning.pytorch.callbacks.LearningRateMonitor
      init_args:
        logging_interval: step

model:
  class_path: models.SupervisedModel
  init_args:
    backbone:
      class_path: models.TimmModel
      init_args:
        model_name: resnet50
        pretrained: false

data:
  class_path: datamodules.Task1Datamodule
  init_args:
    path: C:/Data/AAIT/task1
    batch_size: 256
    num_workers: 0
    labeled: true
    unlabeled: false
    val_size: 0.2

optimizer:
  class_path: torch.optim.AdamW
  init_args:
    lr: 0.0003

#lr_scheduler:
#  class_path: torch.optim.lr_scheduler.CosineAnnealingWarmRestarts
#  init_args:
#    T_0: 1
#    T_mult: 2

lr_scheduler:
  class_path: lightning.pytorch.cli.ReduceLROnPlateau
  init_args:
    monitor: val_0/loss
    patience: 10
