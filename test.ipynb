{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T13:09:58.889027200Z",
     "start_time": "2023-12-28T13:09:58.868013200Z"
    }
   },
   "id": "99d9779170e3a374"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "data": {
      "text/plain": "['resnet50']"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "timm.list_models(\"resnet50\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:38:41.633630500Z",
     "start_time": "2023-12-28T15:38:36.543484400Z"
    }
   },
   "id": "889b9df4bf280ee1"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "resnet50 = timm.create_model(\"resnet50\")\n",
    "efficientnet_b0 = timm.create_model(\"efficientnet_b0\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:38:42.347685500Z",
     "start_time": "2023-12-28T15:38:41.639597200Z"
    }
   },
   "id": "66aa8a3baf33ba77"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "24.373085021972656"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad) / 1024 / 1024\n",
    "\n",
    "count_parameters(resnet50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:19.488872Z",
     "start_time": "2023-12-28T10:43:19.475873600Z"
    }
   },
   "id": "c38ddb077a6572d5"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "5.043552398681641"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(efficientnet_b0)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:19.562905400Z",
     "start_time": "2023-12-28T10:43:19.489870500Z"
    }
   },
   "id": "f17f24f07ec7178b"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.randn(12, 3, 224, 224)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:38:42.347685500Z",
     "start_time": "2023-12-28T15:38:42.059690100Z"
    }
   },
   "id": "d55a864e50c9eb57"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "927 ms ± 19 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit resnet50(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:27.346869Z",
     "start_time": "2023-12-28T10:43:19.536869700Z"
    }
   },
   "id": "462d84c5248568d3"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "efficientnet_v2 = timm.create_model(\"efficientnetv2_s\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:27.750871500Z",
     "start_time": "2023-12-28T10:43:27.348872500Z"
    }
   },
   "id": "60928451764ed32a"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "20.46440887451172"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_parameters(efficientnet_v2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:27.768882500Z",
     "start_time": "2023-12-28T10:43:27.754873300Z"
    }
   },
   "id": "de27b3a492232949"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.22 s ± 28.5 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit efficientnet_v2(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:37.745914Z",
     "start_time": "2023-12-28T10:43:27.770873800Z"
    }
   },
   "id": "9d54053da6f776b8"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "8.687967300415039"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "efficientnet_b2 = timm.create_model(\"efficientnet_b2\")\n",
    "count_parameters(efficientnet_b2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:37.989905100Z",
     "start_time": "2023-12-28T10:43:37.748905700Z"
    }
   },
   "id": "28328e00e90edf95"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "839 ms ± 27.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit efficientnet_b2(x)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:44.751420800Z",
     "start_time": "2023-12-28T10:43:37.976905100Z"
    }
   },
   "id": "f3f33a50a5da8172"
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "data": {
      "text/plain": "ResNet(\n  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (act1): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )\n  (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n)"
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:43:52.528613600Z",
     "start_time": "2023-12-28T10:43:52.497573600Z"
    }
   },
   "id": "452c0fb3bbb399f1"
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "['T_destination',\n '__annotations__',\n '__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattr__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_apply',\n '_backward_hooks',\n '_backward_pre_hooks',\n '_buffers',\n '_call_impl',\n '_compiled_call_impl',\n '_forward_hooks',\n '_forward_hooks_always_called',\n '_forward_hooks_with_kwargs',\n '_forward_pre_hooks',\n '_forward_pre_hooks_with_kwargs',\n '_get_backward_hooks',\n '_get_backward_pre_hooks',\n '_get_name',\n '_is_full_backward_hook',\n '_load_from_state_dict',\n '_load_state_dict_post_hooks',\n '_load_state_dict_pre_hooks',\n '_maybe_warn_non_full_backward_hook',\n '_modules',\n '_named_members',\n '_non_persistent_buffers_set',\n '_parameters',\n '_register_load_state_dict_pre_hook',\n '_register_state_dict_hook',\n '_replicate_for_data_parallel',\n '_save_to_state_dict',\n '_slow_forward',\n '_state_dict_hooks',\n '_state_dict_pre_hooks',\n '_version',\n '_wrapped_call_impl',\n 'act1',\n 'add_module',\n 'apply',\n 'bfloat16',\n 'bn1',\n 'buffers',\n 'call_super_init',\n 'children',\n 'compile',\n 'conv1',\n 'cpu',\n 'cuda',\n 'default_cfg',\n 'double',\n 'drop_rate',\n 'dump_patches',\n 'eval',\n 'extra_repr',\n 'fc',\n 'feature_info',\n 'float',\n 'forward',\n 'forward_features',\n 'forward_head',\n 'get_buffer',\n 'get_classifier',\n 'get_extra_state',\n 'get_parameter',\n 'get_submodule',\n 'global_pool',\n 'grad_checkpointing',\n 'group_matcher',\n 'half',\n 'init_weights',\n 'ipu',\n 'layer1',\n 'layer2',\n 'layer3',\n 'layer4',\n 'load_state_dict',\n 'maxpool',\n 'modules',\n 'named_buffers',\n 'named_children',\n 'named_modules',\n 'named_parameters',\n 'num_classes',\n 'num_features',\n 'parameters',\n 'pretrained_cfg',\n 'register_backward_hook',\n 'register_buffer',\n 'register_forward_hook',\n 'register_forward_pre_hook',\n 'register_full_backward_hook',\n 'register_full_backward_pre_hook',\n 'register_load_state_dict_post_hook',\n 'register_module',\n 'register_parameter',\n 'register_state_dict_pre_hook',\n 'requires_grad_',\n 'reset_classifier',\n 'set_extra_state',\n 'set_grad_checkpointing',\n 'share_memory',\n 'state_dict',\n 'to',\n 'to_empty',\n 'train',\n 'training',\n 'type',\n 'xpu',\n 'zero_grad']"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(resnet50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T10:44:10.274677800Z",
     "start_time": "2023-12-28T10:44:10.151648900Z"
    }
   },
   "id": "86a345dfa58b7f6e"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "['conv1.weight',\n 'bn1.weight',\n 'bn1.bias',\n 'layer1.0.conv1.weight',\n 'layer1.0.bn1.weight',\n 'layer1.0.bn1.bias',\n 'layer1.0.conv2.weight',\n 'layer1.0.bn2.weight',\n 'layer1.0.bn2.bias',\n 'layer1.0.conv3.weight',\n 'layer1.0.bn3.weight',\n 'layer1.0.bn3.bias',\n 'layer1.0.downsample.0.weight',\n 'layer1.0.downsample.1.weight',\n 'layer1.0.downsample.1.bias',\n 'layer1.1.conv1.weight',\n 'layer1.1.bn1.weight',\n 'layer1.1.bn1.bias',\n 'layer1.1.conv2.weight',\n 'layer1.1.bn2.weight',\n 'layer1.1.bn2.bias',\n 'layer1.1.conv3.weight',\n 'layer1.1.bn3.weight',\n 'layer1.1.bn3.bias',\n 'layer1.2.conv1.weight',\n 'layer1.2.bn1.weight',\n 'layer1.2.bn1.bias',\n 'layer1.2.conv2.weight',\n 'layer1.2.bn2.weight',\n 'layer1.2.bn2.bias',\n 'layer1.2.conv3.weight',\n 'layer1.2.bn3.weight',\n 'layer1.2.bn3.bias',\n 'layer2.0.conv1.weight',\n 'layer2.0.bn1.weight',\n 'layer2.0.bn1.bias',\n 'layer2.0.conv2.weight',\n 'layer2.0.bn2.weight',\n 'layer2.0.bn2.bias',\n 'layer2.0.conv3.weight',\n 'layer2.0.bn3.weight',\n 'layer2.0.bn3.bias',\n 'layer2.0.downsample.0.weight',\n 'layer2.0.downsample.1.weight',\n 'layer2.0.downsample.1.bias',\n 'layer2.1.conv1.weight',\n 'layer2.1.bn1.weight',\n 'layer2.1.bn1.bias',\n 'layer2.1.conv2.weight',\n 'layer2.1.bn2.weight',\n 'layer2.1.bn2.bias',\n 'layer2.1.conv3.weight',\n 'layer2.1.bn3.weight',\n 'layer2.1.bn3.bias',\n 'layer2.2.conv1.weight',\n 'layer2.2.bn1.weight',\n 'layer2.2.bn1.bias',\n 'layer2.2.conv2.weight',\n 'layer2.2.bn2.weight',\n 'layer2.2.bn2.bias',\n 'layer2.2.conv3.weight',\n 'layer2.2.bn3.weight',\n 'layer2.2.bn3.bias',\n 'layer2.3.conv1.weight',\n 'layer2.3.bn1.weight',\n 'layer2.3.bn1.bias',\n 'layer2.3.conv2.weight',\n 'layer2.3.bn2.weight',\n 'layer2.3.bn2.bias',\n 'layer2.3.conv3.weight',\n 'layer2.3.bn3.weight',\n 'layer2.3.bn3.bias',\n 'layer3.0.conv1.weight',\n 'layer3.0.bn1.weight',\n 'layer3.0.bn1.bias',\n 'layer3.0.conv2.weight',\n 'layer3.0.bn2.weight',\n 'layer3.0.bn2.bias',\n 'layer3.0.conv3.weight',\n 'layer3.0.bn3.weight',\n 'layer3.0.bn3.bias',\n 'layer3.0.downsample.0.weight',\n 'layer3.0.downsample.1.weight',\n 'layer3.0.downsample.1.bias',\n 'layer3.1.conv1.weight',\n 'layer3.1.bn1.weight',\n 'layer3.1.bn1.bias',\n 'layer3.1.conv2.weight',\n 'layer3.1.bn2.weight',\n 'layer3.1.bn2.bias',\n 'layer3.1.conv3.weight',\n 'layer3.1.bn3.weight',\n 'layer3.1.bn3.bias',\n 'layer3.2.conv1.weight',\n 'layer3.2.bn1.weight',\n 'layer3.2.bn1.bias',\n 'layer3.2.conv2.weight',\n 'layer3.2.bn2.weight',\n 'layer3.2.bn2.bias',\n 'layer3.2.conv3.weight',\n 'layer3.2.bn3.weight',\n 'layer3.2.bn3.bias',\n 'layer3.3.conv1.weight',\n 'layer3.3.bn1.weight',\n 'layer3.3.bn1.bias',\n 'layer3.3.conv2.weight',\n 'layer3.3.bn2.weight',\n 'layer3.3.bn2.bias',\n 'layer3.3.conv3.weight',\n 'layer3.3.bn3.weight',\n 'layer3.3.bn3.bias',\n 'layer3.4.conv1.weight',\n 'layer3.4.bn1.weight',\n 'layer3.4.bn1.bias',\n 'layer3.4.conv2.weight',\n 'layer3.4.bn2.weight',\n 'layer3.4.bn2.bias',\n 'layer3.4.conv3.weight',\n 'layer3.4.bn3.weight',\n 'layer3.4.bn3.bias',\n 'layer3.5.conv1.weight',\n 'layer3.5.bn1.weight',\n 'layer3.5.bn1.bias',\n 'layer3.5.conv2.weight',\n 'layer3.5.bn2.weight',\n 'layer3.5.bn2.bias',\n 'layer3.5.conv3.weight',\n 'layer3.5.bn3.weight',\n 'layer3.5.bn3.bias',\n 'layer4.0.conv1.weight',\n 'layer4.0.bn1.weight',\n 'layer4.0.bn1.bias',\n 'layer4.0.conv2.weight',\n 'layer4.0.bn2.weight',\n 'layer4.0.bn2.bias',\n 'layer4.0.conv3.weight',\n 'layer4.0.bn3.weight',\n 'layer4.0.bn3.bias',\n 'layer4.0.downsample.0.weight',\n 'layer4.0.downsample.1.weight',\n 'layer4.0.downsample.1.bias',\n 'layer4.1.conv1.weight',\n 'layer4.1.bn1.weight',\n 'layer4.1.bn1.bias',\n 'layer4.1.conv2.weight',\n 'layer4.1.bn2.weight',\n 'layer4.1.bn2.bias',\n 'layer4.1.conv3.weight',\n 'layer4.1.bn3.weight',\n 'layer4.1.bn3.bias',\n 'layer4.2.conv1.weight',\n 'layer4.2.bn1.weight',\n 'layer4.2.bn1.bias',\n 'layer4.2.conv2.weight',\n 'layer4.2.bn2.weight',\n 'layer4.2.bn2.bias',\n 'layer4.2.conv3.weight',\n 'layer4.2.bn3.weight',\n 'layer4.2.bn3.bias',\n 'fc.weight',\n 'fc.bias']"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[x[0] for x in resnet50.named_parameters()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T11:04:18.949743400Z",
     "start_time": "2023-12-28T11:04:18.827744600Z"
    }
   },
   "id": "bc2688a6216df11c"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "['H',\n 'T',\n '__abs__',\n '__add__',\n '__and__',\n '__array__',\n '__array_priority__',\n '__array_wrap__',\n '__bool__',\n '__class__',\n '__complex__',\n '__contains__',\n '__deepcopy__',\n '__delattr__',\n '__delitem__',\n '__dict__',\n '__dir__',\n '__div__',\n '__dlpack__',\n '__dlpack_device__',\n '__doc__',\n '__eq__',\n '__float__',\n '__floordiv__',\n '__format__',\n '__ge__',\n '__getattribute__',\n '__getitem__',\n '__gt__',\n '__hash__',\n '__iadd__',\n '__iand__',\n '__idiv__',\n '__ifloordiv__',\n '__ilshift__',\n '__imod__',\n '__imul__',\n '__index__',\n '__init__',\n '__init_subclass__',\n '__int__',\n '__invert__',\n '__ior__',\n '__ipow__',\n '__irshift__',\n '__isub__',\n '__iter__',\n '__itruediv__',\n '__ixor__',\n '__le__',\n '__len__',\n '__long__',\n '__lshift__',\n '__lt__',\n '__matmul__',\n '__mod__',\n '__module__',\n '__mul__',\n '__ne__',\n '__neg__',\n '__new__',\n '__nonzero__',\n '__or__',\n '__pos__',\n '__pow__',\n '__radd__',\n '__rand__',\n '__rdiv__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__reversed__',\n '__rfloordiv__',\n '__rlshift__',\n '__rmatmul__',\n '__rmod__',\n '__rmul__',\n '__ror__',\n '__rpow__',\n '__rrshift__',\n '__rshift__',\n '__rsub__',\n '__rtruediv__',\n '__rxor__',\n '__setattr__',\n '__setitem__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__sub__',\n '__subclasshook__',\n '__torch_dispatch__',\n '__torch_function__',\n '__truediv__',\n '__weakref__',\n '__xor__',\n '_addmm_activation',\n '_autocast_to_full_precision',\n '_autocast_to_reduced_precision',\n '_backward_hooks',\n '_base',\n '_cdata',\n '_coalesced_',\n '_conj',\n '_conj_physical',\n '_dimI',\n '_dimV',\n '_fix_weakref',\n '_grad',\n '_grad_fn',\n '_has_symbolic_sizes_strides',\n '_indices',\n '_is_all_true',\n '_is_any_true',\n '_is_view',\n '_is_zerotensor',\n '_make_subclass',\n '_make_wrapper_subclass',\n '_neg_view',\n '_nested_tensor_size',\n '_nested_tensor_storage_offsets',\n '_nested_tensor_strides',\n '_nnz',\n '_post_accumulate_grad_hooks',\n '_python_dispatch',\n '_reduce_ex_internal',\n '_sparse_mask_projection',\n '_to_dense',\n '_to_sparse',\n '_to_sparse_bsc',\n '_to_sparse_bsr',\n '_to_sparse_csc',\n '_to_sparse_csr',\n '_typed_storage',\n '_update_names',\n '_values',\n '_version',\n '_view_func',\n 'abs',\n 'abs_',\n 'absolute',\n 'absolute_',\n 'acos',\n 'acos_',\n 'acosh',\n 'acosh_',\n 'add',\n 'add_',\n 'addbmm',\n 'addbmm_',\n 'addcdiv',\n 'addcdiv_',\n 'addcmul',\n 'addcmul_',\n 'addmm',\n 'addmm_',\n 'addmv',\n 'addmv_',\n 'addr',\n 'addr_',\n 'adjoint',\n 'align_as',\n 'align_to',\n 'all',\n 'allclose',\n 'amax',\n 'amin',\n 'aminmax',\n 'angle',\n 'any',\n 'apply_',\n 'arccos',\n 'arccos_',\n 'arccosh',\n 'arccosh_',\n 'arcsin',\n 'arcsin_',\n 'arcsinh',\n 'arcsinh_',\n 'arctan',\n 'arctan2',\n 'arctan2_',\n 'arctan_',\n 'arctanh',\n 'arctanh_',\n 'argmax',\n 'argmin',\n 'argsort',\n 'argwhere',\n 'as_strided',\n 'as_strided_',\n 'as_strided_scatter',\n 'as_subclass',\n 'asin',\n 'asin_',\n 'asinh',\n 'asinh_',\n 'atan',\n 'atan2',\n 'atan2_',\n 'atan_',\n 'atanh',\n 'atanh_',\n 'backward',\n 'baddbmm',\n 'baddbmm_',\n 'bernoulli',\n 'bernoulli_',\n 'bfloat16',\n 'bincount',\n 'bitwise_and',\n 'bitwise_and_',\n 'bitwise_left_shift',\n 'bitwise_left_shift_',\n 'bitwise_not',\n 'bitwise_not_',\n 'bitwise_or',\n 'bitwise_or_',\n 'bitwise_right_shift',\n 'bitwise_right_shift_',\n 'bitwise_xor',\n 'bitwise_xor_',\n 'bmm',\n 'bool',\n 'broadcast_to',\n 'byte',\n 'cauchy_',\n 'ccol_indices',\n 'cdouble',\n 'ceil',\n 'ceil_',\n 'cfloat',\n 'chalf',\n 'char',\n 'cholesky',\n 'cholesky_inverse',\n 'cholesky_solve',\n 'chunk',\n 'clamp',\n 'clamp_',\n 'clamp_max',\n 'clamp_max_',\n 'clamp_min',\n 'clamp_min_',\n 'clip',\n 'clip_',\n 'clone',\n 'coalesce',\n 'col_indices',\n 'conj',\n 'conj_physical',\n 'conj_physical_',\n 'contiguous',\n 'copy_',\n 'copysign',\n 'copysign_',\n 'corrcoef',\n 'cos',\n 'cos_',\n 'cosh',\n 'cosh_',\n 'count_nonzero',\n 'cov',\n 'cpu',\n 'cross',\n 'crow_indices',\n 'cuda',\n 'cummax',\n 'cummin',\n 'cumprod',\n 'cumprod_',\n 'cumsum',\n 'cumsum_',\n 'data',\n 'data_ptr',\n 'deg2rad',\n 'deg2rad_',\n 'dense_dim',\n 'dequantize',\n 'det',\n 'detach',\n 'detach_',\n 'device',\n 'diag',\n 'diag_embed',\n 'diagflat',\n 'diagonal',\n 'diagonal_scatter',\n 'diff',\n 'digamma',\n 'digamma_',\n 'dim',\n 'dim_order',\n 'dist',\n 'div',\n 'div_',\n 'divide',\n 'divide_',\n 'dot',\n 'double',\n 'dsplit',\n 'dtype',\n 'eig',\n 'element_size',\n 'eq',\n 'eq_',\n 'equal',\n 'erf',\n 'erf_',\n 'erfc',\n 'erfc_',\n 'erfinv',\n 'erfinv_',\n 'exp',\n 'exp2',\n 'exp2_',\n 'exp_',\n 'expand',\n 'expand_as',\n 'expm1',\n 'expm1_',\n 'exponential_',\n 'fill_',\n 'fill_diagonal_',\n 'fix',\n 'fix_',\n 'flatten',\n 'flip',\n 'fliplr',\n 'flipud',\n 'float',\n 'float_power',\n 'float_power_',\n 'floor',\n 'floor_',\n 'floor_divide',\n 'floor_divide_',\n 'fmax',\n 'fmin',\n 'fmod',\n 'fmod_',\n 'frac',\n 'frac_',\n 'frexp',\n 'gather',\n 'gcd',\n 'gcd_',\n 'ge',\n 'ge_',\n 'geometric_',\n 'geqrf',\n 'ger',\n 'get_device',\n 'grad',\n 'grad_fn',\n 'greater',\n 'greater_',\n 'greater_equal',\n 'greater_equal_',\n 'gt',\n 'gt_',\n 'half',\n 'hardshrink',\n 'has_names',\n 'heaviside',\n 'heaviside_',\n 'histc',\n 'histogram',\n 'hsplit',\n 'hypot',\n 'hypot_',\n 'i0',\n 'i0_',\n 'igamma',\n 'igamma_',\n 'igammac',\n 'igammac_',\n 'imag',\n 'index_add',\n 'index_add_',\n 'index_copy',\n 'index_copy_',\n 'index_fill',\n 'index_fill_',\n 'index_put',\n 'index_put_',\n 'index_reduce',\n 'index_reduce_',\n 'index_select',\n 'indices',\n 'inner',\n 'int',\n 'int_repr',\n 'inverse',\n 'ipu',\n 'is_coalesced',\n 'is_complex',\n 'is_conj',\n 'is_contiguous',\n 'is_cpu',\n 'is_cuda',\n 'is_distributed',\n 'is_floating_point',\n 'is_inference',\n 'is_ipu',\n 'is_leaf',\n 'is_meta',\n 'is_mkldnn',\n 'is_mps',\n 'is_neg',\n 'is_nested',\n 'is_nonzero',\n 'is_ort',\n 'is_pinned',\n 'is_quantized',\n 'is_same_size',\n 'is_set_to',\n 'is_shared',\n 'is_signed',\n 'is_sparse',\n 'is_sparse_csr',\n 'is_vulkan',\n 'is_xla',\n 'is_xpu',\n 'isclose',\n 'isfinite',\n 'isinf',\n 'isnan',\n 'isneginf',\n 'isposinf',\n 'isreal',\n 'istft',\n 'item',\n 'itemsize',\n 'kron',\n 'kthvalue',\n 'layout',\n 'lcm',\n 'lcm_',\n 'ldexp',\n 'ldexp_',\n 'le',\n 'le_',\n 'lerp',\n 'lerp_',\n 'less',\n 'less_',\n 'less_equal',\n 'less_equal_',\n 'lgamma',\n 'lgamma_',\n 'log',\n 'log10',\n 'log10_',\n 'log1p',\n 'log1p_',\n 'log2',\n 'log2_',\n 'log_',\n 'log_normal_',\n 'log_softmax',\n 'logaddexp',\n 'logaddexp2',\n 'logcumsumexp',\n 'logdet',\n 'logical_and',\n 'logical_and_',\n 'logical_not',\n 'logical_not_',\n 'logical_or',\n 'logical_or_',\n 'logical_xor',\n 'logical_xor_',\n 'logit',\n 'logit_',\n 'logsumexp',\n 'long',\n 'lstsq',\n 'lt',\n 'lt_',\n 'lu',\n 'lu_solve',\n 'mH',\n 'mT',\n 'map2_',\n 'map_',\n 'masked_fill',\n 'masked_fill_',\n 'masked_scatter',\n 'masked_scatter_',\n 'masked_select',\n 'matmul',\n 'matrix_exp',\n 'matrix_power',\n 'max',\n 'maximum',\n 'mean',\n 'median',\n 'min',\n 'minimum',\n 'mm',\n 'mode',\n 'moveaxis',\n 'movedim',\n 'msort',\n 'mul',\n 'mul_',\n 'multinomial',\n 'multiply',\n 'multiply_',\n 'mv',\n 'mvlgamma',\n 'mvlgamma_',\n 'name',\n 'names',\n 'nan_to_num',\n 'nan_to_num_',\n 'nanmean',\n 'nanmedian',\n 'nanquantile',\n 'nansum',\n 'narrow',\n 'narrow_copy',\n 'nbytes',\n 'ndim',\n 'ndimension',\n 'ne',\n 'ne_',\n 'neg',\n 'neg_',\n 'negative',\n 'negative_',\n 'nelement',\n 'new',\n 'new_empty',\n 'new_empty_strided',\n 'new_full',\n 'new_ones',\n 'new_tensor',\n 'new_zeros',\n 'nextafter',\n 'nextafter_',\n 'nonzero',\n 'nonzero_static',\n 'norm',\n 'normal_',\n 'not_equal',\n 'not_equal_',\n 'numel',\n 'numpy',\n 'orgqr',\n 'ormqr',\n 'outer',\n 'output_nr',\n 'permute',\n 'pin_memory',\n 'pinverse',\n 'polygamma',\n 'polygamma_',\n 'positive',\n 'pow',\n 'pow_',\n 'prelu',\n 'prod',\n 'put',\n 'put_',\n 'q_per_channel_axis',\n 'q_per_channel_scales',\n 'q_per_channel_zero_points',\n 'q_scale',\n 'q_zero_point',\n 'qr',\n 'qscheme',\n 'quantile',\n 'rad2deg',\n 'rad2deg_',\n 'random_',\n 'ravel',\n 'real',\n 'reciprocal',\n 'reciprocal_',\n 'record_stream',\n 'refine_names',\n 'register_hook',\n 'register_post_accumulate_grad_hook',\n 'reinforce',\n 'relu',\n 'relu_',\n 'remainder',\n 'remainder_',\n 'rename',\n 'rename_',\n 'renorm',\n 'renorm_',\n 'repeat',\n 'repeat_interleave',\n 'requires_grad',\n 'requires_grad_',\n 'reshape',\n 'reshape_as',\n 'resize',\n 'resize_',\n 'resize_as',\n 'resize_as_',\n 'resize_as_sparse_',\n 'resolve_conj',\n 'resolve_neg',\n 'retain_grad',\n 'retains_grad',\n 'roll',\n 'rot90',\n 'round',\n 'round_',\n 'row_indices',\n 'rsqrt',\n 'rsqrt_',\n 'scatter',\n 'scatter_',\n 'scatter_add',\n 'scatter_add_',\n 'scatter_reduce',\n 'scatter_reduce_',\n 'select',\n 'select_scatter',\n 'set_',\n 'sgn',\n 'sgn_',\n 'shape',\n 'share_memory_',\n 'short',\n 'sigmoid',\n 'sigmoid_',\n 'sign',\n 'sign_',\n 'signbit',\n 'sin',\n 'sin_',\n 'sinc',\n 'sinc_',\n 'sinh',\n 'sinh_',\n 'size',\n 'slice_scatter',\n 'slogdet',\n 'smm',\n 'softmax',\n 'solve',\n 'sort',\n 'sparse_dim',\n 'sparse_mask',\n 'sparse_resize_',\n 'sparse_resize_and_clear_',\n 'split',\n 'split_with_sizes',\n 'sqrt',\n 'sqrt_',\n 'square',\n 'square_',\n 'squeeze',\n 'squeeze_',\n 'sspaddmm',\n 'std',\n 'stft',\n 'storage',\n 'storage_offset',\n 'storage_type',\n 'stride',\n 'sub',\n 'sub_',\n 'subtract',\n 'subtract_',\n 'sum',\n 'sum_to_size',\n 'svd',\n 'swapaxes',\n 'swapaxes_',\n 'swapdims',\n 'swapdims_',\n 'symeig',\n 't',\n 't_',\n 'take',\n 'take_along_dim',\n 'tan',\n 'tan_',\n 'tanh',\n 'tanh_',\n 'tensor_split',\n 'tile',\n 'to',\n 'to_dense',\n 'to_mkldnn',\n 'to_padded_tensor',\n 'to_sparse',\n 'to_sparse_bsc',\n 'to_sparse_bsr',\n 'to_sparse_coo',\n 'to_sparse_csc',\n 'to_sparse_csr',\n 'tolist',\n 'topk',\n 'trace',\n 'transpose',\n 'transpose_',\n 'triangular_solve',\n 'tril',\n 'tril_',\n 'triu',\n 'triu_',\n 'true_divide',\n 'true_divide_',\n 'trunc',\n 'trunc_',\n 'type',\n 'type_as',\n 'unbind',\n 'unflatten',\n 'unfold',\n 'uniform_',\n 'unique',\n 'unique_consecutive',\n 'unsafe_chunk',\n 'unsafe_split',\n 'unsafe_split_with_sizes',\n 'unsqueeze',\n 'unsqueeze_',\n 'untyped_storage',\n 'values',\n 'var',\n 'vdot',\n 'view',\n 'view_as',\n 'vsplit',\n 'where',\n 'xlogy',\n 'xlogy_',\n 'xpu',\n 'zero_']"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(next(resnet50.named_parameters())[1])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T11:05:54.236405500Z",
     "start_time": "2023-12-28T11:05:54.006430900Z"
    }
   },
   "id": "f2c3e1b5b834317"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([5, 10])"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((10, 10))\n",
    "a[:5].shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T18:05:34.980586Z",
     "start_time": "2023-12-28T18:05:33.924508200Z"
    }
   },
   "id": "bcdfa5eeb76ae993"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 2048, 7, 7])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x1 = resnet50.forward_features(x)\n",
    "x1.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T13:14:09.087456Z",
     "start_time": "2023-12-28T13:14:07.941931200Z"
    }
   },
   "id": "75abe02f8fe1eced"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([12, 2048])"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50.global_pool(x1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T13:14:15.630462800Z",
     "start_time": "2023-12-28T13:14:15.394465400Z"
    }
   },
   "id": "d0e6cf75fc0b1f41"
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "2048"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resnet50.num_features"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T13:15:37.774278300Z",
     "start_time": "2023-12-28T13:15:37.666562600Z"
    }
   },
   "id": "2b0678883d27eb7b"
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "['conv1.weight',\n 'bn1.weight',\n 'bn1.bias',\n 'layer1.0.conv1.weight',\n 'layer1.0.bn1.weight',\n 'layer1.0.bn1.bias',\n 'layer1.0.conv2.weight',\n 'layer1.0.bn2.weight',\n 'layer1.0.bn2.bias',\n 'layer1.0.conv3.weight',\n 'layer1.0.bn3.weight',\n 'layer1.0.bn3.bias',\n 'layer1.0.downsample.0.weight',\n 'layer1.0.downsample.1.weight',\n 'layer1.0.downsample.1.bias',\n 'layer1.1.conv1.weight',\n 'layer1.1.bn1.weight',\n 'layer1.1.bn1.bias',\n 'layer1.1.conv2.weight',\n 'layer1.1.bn2.weight',\n 'layer1.1.bn2.bias',\n 'layer1.1.conv3.weight',\n 'layer1.1.bn3.weight',\n 'layer1.1.bn3.bias',\n 'layer1.2.conv1.weight',\n 'layer1.2.bn1.weight',\n 'layer1.2.bn1.bias',\n 'layer1.2.conv2.weight',\n 'layer1.2.bn2.weight',\n 'layer1.2.bn2.bias',\n 'layer1.2.conv3.weight',\n 'layer1.2.bn3.weight',\n 'layer1.2.bn3.bias',\n 'layer2.0.conv1.weight',\n 'layer2.0.bn1.weight',\n 'layer2.0.bn1.bias',\n 'layer2.0.conv2.weight',\n 'layer2.0.bn2.weight',\n 'layer2.0.bn2.bias',\n 'layer2.0.conv3.weight',\n 'layer2.0.bn3.weight',\n 'layer2.0.bn3.bias',\n 'layer2.0.downsample.0.weight',\n 'layer2.0.downsample.1.weight',\n 'layer2.0.downsample.1.bias',\n 'layer2.1.conv1.weight',\n 'layer2.1.bn1.weight',\n 'layer2.1.bn1.bias',\n 'layer2.1.conv2.weight',\n 'layer2.1.bn2.weight',\n 'layer2.1.bn2.bias',\n 'layer2.1.conv3.weight',\n 'layer2.1.bn3.weight',\n 'layer2.1.bn3.bias',\n 'layer2.2.conv1.weight',\n 'layer2.2.bn1.weight',\n 'layer2.2.bn1.bias',\n 'layer2.2.conv2.weight',\n 'layer2.2.bn2.weight',\n 'layer2.2.bn2.bias',\n 'layer2.2.conv3.weight',\n 'layer2.2.bn3.weight',\n 'layer2.2.bn3.bias',\n 'layer2.3.conv1.weight',\n 'layer2.3.bn1.weight',\n 'layer2.3.bn1.bias',\n 'layer2.3.conv2.weight',\n 'layer2.3.bn2.weight',\n 'layer2.3.bn2.bias',\n 'layer2.3.conv3.weight',\n 'layer2.3.bn3.weight',\n 'layer2.3.bn3.bias',\n 'layer3.0.conv1.weight',\n 'layer3.0.bn1.weight',\n 'layer3.0.bn1.bias',\n 'layer3.0.conv2.weight',\n 'layer3.0.bn2.weight',\n 'layer3.0.bn2.bias',\n 'layer3.0.conv3.weight',\n 'layer3.0.bn3.weight',\n 'layer3.0.bn3.bias',\n 'layer3.0.downsample.0.weight',\n 'layer3.0.downsample.1.weight',\n 'layer3.0.downsample.1.bias',\n 'layer3.1.conv1.weight',\n 'layer3.1.bn1.weight',\n 'layer3.1.bn1.bias',\n 'layer3.1.conv2.weight',\n 'layer3.1.bn2.weight',\n 'layer3.1.bn2.bias',\n 'layer3.1.conv3.weight',\n 'layer3.1.bn3.weight',\n 'layer3.1.bn3.bias',\n 'layer3.2.conv1.weight',\n 'layer3.2.bn1.weight',\n 'layer3.2.bn1.bias',\n 'layer3.2.conv2.weight',\n 'layer3.2.bn2.weight',\n 'layer3.2.bn2.bias',\n 'layer3.2.conv3.weight',\n 'layer3.2.bn3.weight',\n 'layer3.2.bn3.bias',\n 'layer3.3.conv1.weight',\n 'layer3.3.bn1.weight',\n 'layer3.3.bn1.bias',\n 'layer3.3.conv2.weight',\n 'layer3.3.bn2.weight',\n 'layer3.3.bn2.bias',\n 'layer3.3.conv3.weight',\n 'layer3.3.bn3.weight',\n 'layer3.3.bn3.bias',\n 'layer3.4.conv1.weight',\n 'layer3.4.bn1.weight',\n 'layer3.4.bn1.bias',\n 'layer3.4.conv2.weight',\n 'layer3.4.bn2.weight',\n 'layer3.4.bn2.bias',\n 'layer3.4.conv3.weight',\n 'layer3.4.bn3.weight',\n 'layer3.4.bn3.bias',\n 'layer3.5.conv1.weight',\n 'layer3.5.bn1.weight',\n 'layer3.5.bn1.bias',\n 'layer3.5.conv2.weight',\n 'layer3.5.bn2.weight',\n 'layer3.5.bn2.bias',\n 'layer3.5.conv3.weight',\n 'layer3.5.bn3.weight',\n 'layer3.5.bn3.bias',\n 'layer4.0.conv1.weight',\n 'layer4.0.bn1.weight',\n 'layer4.0.bn1.bias',\n 'layer4.0.conv2.weight',\n 'layer4.0.bn2.weight',\n 'layer4.0.bn2.bias',\n 'layer4.0.conv3.weight',\n 'layer4.0.bn3.weight',\n 'layer4.0.bn3.bias',\n 'layer4.0.downsample.0.weight',\n 'layer4.0.downsample.1.weight',\n 'layer4.0.downsample.1.bias',\n 'layer4.1.conv1.weight',\n 'layer4.1.bn1.weight',\n 'layer4.1.bn1.bias',\n 'layer4.1.conv2.weight',\n 'layer4.1.bn2.weight',\n 'layer4.1.bn2.bias',\n 'layer4.1.conv3.weight',\n 'layer4.1.bn3.weight',\n 'layer4.1.bn3.bias',\n 'layer4.2.conv1.weight',\n 'layer4.2.bn1.weight',\n 'layer4.2.bn1.bias',\n 'layer4.2.conv2.weight',\n 'layer4.2.bn2.weight',\n 'layer4.2.bn2.bias',\n 'layer4.2.conv3.weight',\n 'layer4.2.bn3.weight',\n 'layer4.2.bn3.bias',\n 'fc.weight',\n 'fc.bias']"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m for m, k in resnet50.named_parameters()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T14:31:39.288373100Z",
     "start_time": "2023-12-28T14:31:38.837268300Z"
    }
   },
   "id": "3585eabc1e883d38"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "[('',\n  ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n  )),\n ('conv1',\n  Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)),\n ('bn1',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('act1', ReLU(inplace=True)),\n ('maxpool',\n  MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)),\n ('layer1',\n  Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )),\n ('layer1.0',\n  Bottleneck(\n    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )),\n ('layer1.0.conv1',\n  Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.0.bn1',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.0.act1', ReLU(inplace=True)),\n ('layer1.0.conv2',\n  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer1.0.bn2',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.0.drop_block', Identity()),\n ('layer1.0.act2', ReLU(inplace=True)),\n ('layer1.0.aa', Identity()),\n ('layer1.0.conv3',\n  Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.0.bn3',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.0.act3', ReLU(inplace=True)),\n ('layer1.0.downsample',\n  Sequential(\n    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )),\n ('layer1.0.downsample.0',\n  Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.0.downsample.1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.1',\n  Bottleneck(\n    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer1.1.conv1',\n  Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.1.bn1',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.1.act1', ReLU(inplace=True)),\n ('layer1.1.conv2',\n  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer1.1.bn2',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.1.drop_block', Identity()),\n ('layer1.1.act2', ReLU(inplace=True)),\n ('layer1.1.aa', Identity()),\n ('layer1.1.conv3',\n  Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.1.bn3',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.1.act3', ReLU(inplace=True)),\n ('layer1.2',\n  Bottleneck(\n    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer1.2.conv1',\n  Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.2.bn1',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.2.act1', ReLU(inplace=True)),\n ('layer1.2.conv2',\n  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer1.2.bn2',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.2.drop_block', Identity()),\n ('layer1.2.act2', ReLU(inplace=True)),\n ('layer1.2.aa', Identity()),\n ('layer1.2.conv3',\n  Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.2.bn3',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.2.act3', ReLU(inplace=True)),\n ('layer2',\n  Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )),\n ('layer2.0',\n  Bottleneck(\n    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )),\n ('layer2.0.conv1',\n  Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.0.bn1',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.0.act1', ReLU(inplace=True)),\n ('layer2.0.conv2',\n  Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n ('layer2.0.bn2',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.0.drop_block', Identity()),\n ('layer2.0.act2', ReLU(inplace=True)),\n ('layer2.0.aa', Identity()),\n ('layer2.0.conv3',\n  Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.0.bn3',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.0.act3', ReLU(inplace=True)),\n ('layer2.0.downsample',\n  Sequential(\n    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )),\n ('layer2.0.downsample.0',\n  Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)),\n ('layer2.0.downsample.1',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.1',\n  Bottleneck(\n    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer2.1.conv1',\n  Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.1.bn1',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.1.act1', ReLU(inplace=True)),\n ('layer2.1.conv2',\n  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer2.1.bn2',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.1.drop_block', Identity()),\n ('layer2.1.act2', ReLU(inplace=True)),\n ('layer2.1.aa', Identity()),\n ('layer2.1.conv3',\n  Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.1.bn3',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.1.act3', ReLU(inplace=True)),\n ('layer2.2',\n  Bottleneck(\n    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer2.2.conv1',\n  Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.2.bn1',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.2.act1', ReLU(inplace=True)),\n ('layer2.2.conv2',\n  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer2.2.bn2',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.2.drop_block', Identity()),\n ('layer2.2.act2', ReLU(inplace=True)),\n ('layer2.2.aa', Identity()),\n ('layer2.2.conv3',\n  Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.2.bn3',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.2.act3', ReLU(inplace=True)),\n ('layer2.3',\n  Bottleneck(\n    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer2.3.conv1',\n  Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.3.bn1',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.3.act1', ReLU(inplace=True)),\n ('layer2.3.conv2',\n  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer2.3.bn2',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.3.drop_block', Identity()),\n ('layer2.3.act2', ReLU(inplace=True)),\n ('layer2.3.aa', Identity()),\n ('layer2.3.conv3',\n  Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.3.bn3',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.3.act3', ReLU(inplace=True)),\n ('layer3',\n  Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )),\n ('layer3.0',\n  Bottleneck(\n    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )),\n ('layer3.0.conv1',\n  Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.0.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.0.act1', ReLU(inplace=True)),\n ('layer3.0.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n ('layer3.0.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.0.drop_block', Identity()),\n ('layer3.0.act2', ReLU(inplace=True)),\n ('layer3.0.aa', Identity()),\n ('layer3.0.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.0.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.0.act3', ReLU(inplace=True)),\n ('layer3.0.downsample',\n  Sequential(\n    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )),\n ('layer3.0.downsample.0',\n  Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)),\n ('layer3.0.downsample.1',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.1',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.1.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.1.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.1.act1', ReLU(inplace=True)),\n ('layer3.1.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.1.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.1.drop_block', Identity()),\n ('layer3.1.act2', ReLU(inplace=True)),\n ('layer3.1.aa', Identity()),\n ('layer3.1.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.1.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.1.act3', ReLU(inplace=True)),\n ('layer3.2',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.2.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.2.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.2.act1', ReLU(inplace=True)),\n ('layer3.2.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.2.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.2.drop_block', Identity()),\n ('layer3.2.act2', ReLU(inplace=True)),\n ('layer3.2.aa', Identity()),\n ('layer3.2.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.2.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.2.act3', ReLU(inplace=True)),\n ('layer3.3',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.3.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.3.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.3.act1', ReLU(inplace=True)),\n ('layer3.3.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.3.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.3.drop_block', Identity()),\n ('layer3.3.act2', ReLU(inplace=True)),\n ('layer3.3.aa', Identity()),\n ('layer3.3.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.3.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.3.act3', ReLU(inplace=True)),\n ('layer3.4',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.4.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.4.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.4.act1', ReLU(inplace=True)),\n ('layer3.4.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.4.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.4.drop_block', Identity()),\n ('layer3.4.act2', ReLU(inplace=True)),\n ('layer3.4.aa', Identity()),\n ('layer3.4.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.4.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.4.act3', ReLU(inplace=True)),\n ('layer3.5',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.5.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.5.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.5.act1', ReLU(inplace=True)),\n ('layer3.5.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.5.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.5.drop_block', Identity()),\n ('layer3.5.act2', ReLU(inplace=True)),\n ('layer3.5.aa', Identity()),\n ('layer3.5.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.5.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.5.act3', ReLU(inplace=True)),\n ('layer4',\n  Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )),\n ('layer4.0',\n  Bottleneck(\n    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )),\n ('layer4.0.conv1',\n  Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.0.bn1',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.0.act1', ReLU(inplace=True)),\n ('layer4.0.conv2',\n  Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n ('layer4.0.bn2',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.0.drop_block', Identity()),\n ('layer4.0.act2', ReLU(inplace=True)),\n ('layer4.0.aa', Identity()),\n ('layer4.0.conv3',\n  Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.0.bn3',\n  BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.0.act3', ReLU(inplace=True)),\n ('layer4.0.downsample',\n  Sequential(\n    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )),\n ('layer4.0.downsample.0',\n  Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)),\n ('layer4.0.downsample.1',\n  BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.1',\n  Bottleneck(\n    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer4.1.conv1',\n  Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.1.bn1',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.1.act1', ReLU(inplace=True)),\n ('layer4.1.conv2',\n  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer4.1.bn2',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.1.drop_block', Identity()),\n ('layer4.1.act2', ReLU(inplace=True)),\n ('layer4.1.aa', Identity()),\n ('layer4.1.conv3',\n  Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.1.bn3',\n  BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.1.act3', ReLU(inplace=True)),\n ('layer4.2',\n  Bottleneck(\n    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer4.2.conv1',\n  Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.2.bn1',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.2.act1', ReLU(inplace=True)),\n ('layer4.2.conv2',\n  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer4.2.bn2',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.2.drop_block', Identity()),\n ('layer4.2.act2', ReLU(inplace=True)),\n ('layer4.2.aa', Identity()),\n ('layer4.2.conv3',\n  Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.2.bn3',\n  BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.2.act3', ReLU(inplace=True)),\n ('global_pool',\n  SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))),\n ('global_pool.pool', AdaptiveAvgPool2d(output_size=1)),\n ('global_pool.flatten', Flatten(start_dim=1, end_dim=-1)),\n ('fc', Linear(in_features=2048, out_features=1000, bias=True))]"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(m, k) for m, k in resnet50.named_modules()]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:11:40.874466Z",
     "start_time": "2023-12-28T15:11:40.486874400Z"
    }
   },
   "id": "9cf4be64c4cdd3f6"
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['T_destination',\n '__annotations__',\n '__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattr__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_apply',\n '_backward_hooks',\n '_backward_pre_hooks',\n '_buffers',\n '_call_impl',\n '_compiled_call_impl',\n '_forward_hooks',\n '_forward_hooks_always_called',\n '_forward_hooks_with_kwargs',\n '_forward_pre_hooks',\n '_forward_pre_hooks_with_kwargs',\n '_get_backward_hooks',\n '_get_backward_pre_hooks',\n '_get_name',\n '_is_full_backward_hook',\n '_load_from_state_dict',\n '_load_state_dict_post_hooks',\n '_load_state_dict_pre_hooks',\n '_maybe_warn_non_full_backward_hook',\n '_modules',\n '_named_members',\n '_non_persistent_buffers_set',\n '_parameters',\n '_register_load_state_dict_pre_hook',\n '_register_state_dict_hook',\n '_replicate_for_data_parallel',\n '_save_to_state_dict',\n '_slow_forward',\n '_state_dict_hooks',\n '_state_dict_pre_hooks',\n '_version',\n '_wrapped_call_impl',\n 'act1',\n 'add_module',\n 'apply',\n 'bfloat16',\n 'bn1',\n 'buffers',\n 'call_super_init',\n 'children',\n 'compile',\n 'conv1',\n 'cpu',\n 'cuda',\n 'default_cfg',\n 'double',\n 'drop_rate',\n 'dump_patches',\n 'eval',\n 'extra_repr',\n 'fc',\n 'feature_info',\n 'float',\n 'forward',\n 'forward_features',\n 'forward_head',\n 'get_buffer',\n 'get_classifier',\n 'get_extra_state',\n 'get_parameter',\n 'get_submodule',\n 'global_pool',\n 'grad_checkpointing',\n 'group_matcher',\n 'half',\n 'init_weights',\n 'ipu',\n 'layer1',\n 'layer2',\n 'layer3',\n 'layer4',\n 'load_state_dict',\n 'maxpool',\n 'modules',\n 'named_buffers',\n 'named_children',\n 'named_modules',\n 'named_parameters',\n 'num_classes',\n 'num_features',\n 'parameters',\n 'pretrained_cfg',\n 'register_backward_hook',\n 'register_buffer',\n 'register_forward_hook',\n 'register_forward_pre_hook',\n 'register_full_backward_hook',\n 'register_full_backward_pre_hook',\n 'register_load_state_dict_post_hook',\n 'register_module',\n 'register_parameter',\n 'register_state_dict_pre_hook',\n 'requires_grad_',\n 'reset_classifier',\n 'set_extra_state',\n 'set_grad_checkpointing',\n 'share_memory',\n 'state_dict',\n 'to',\n 'to_empty',\n 'train',\n 'training',\n 'type',\n 'xpu',\n 'zero_grad']"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(resnet50)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:11:16.406114700Z",
     "start_time": "2023-12-28T15:11:16.030064400Z"
    }
   },
   "id": "ef30b8d4f11871c0"
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "[('',\n  ResNet(\n    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n    (layer1): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer2): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer3): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (3): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (4): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (5): Bottleneck(\n        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (layer4): Sequential(\n      (0): Bottleneck(\n        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n        (downsample): Sequential(\n          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        )\n      )\n      (1): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n      (2): Bottleneck(\n        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act1): ReLU(inplace=True)\n        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (drop_block): Identity()\n        (act2): ReLU(inplace=True)\n        (aa): Identity()\n        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n        (act3): ReLU(inplace=True)\n      )\n    )\n    (global_pool): SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))\n    (fc): Linear(in_features=2048, out_features=1000, bias=True)\n  )),\n ('conv1',\n  Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)),\n ('bn1',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('act1', ReLU(inplace=True)),\n ('maxpool',\n  MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)),\n ('layer1',\n  Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )),\n ('layer1.0',\n  Bottleneck(\n    (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )),\n ('layer1.0.conv1',\n  Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.0.bn1',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.0.act1', ReLU(inplace=True)),\n ('layer1.0.conv2',\n  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer1.0.bn2',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.0.drop_block', Identity()),\n ('layer1.0.act2', ReLU(inplace=True)),\n ('layer1.0.aa', Identity()),\n ('layer1.0.conv3',\n  Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.0.bn3',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.0.act3', ReLU(inplace=True)),\n ('layer1.0.downsample',\n  Sequential(\n    (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )),\n ('layer1.0.downsample.0',\n  Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.0.downsample.1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.1',\n  Bottleneck(\n    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer1.1.conv1',\n  Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.1.bn1',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.1.act1', ReLU(inplace=True)),\n ('layer1.1.conv2',\n  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer1.1.bn2',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.1.drop_block', Identity()),\n ('layer1.1.act2', ReLU(inplace=True)),\n ('layer1.1.aa', Identity()),\n ('layer1.1.conv3',\n  Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.1.bn3',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.1.act3', ReLU(inplace=True)),\n ('layer1.2',\n  Bottleneck(\n    (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer1.2.conv1',\n  Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.2.bn1',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.2.act1', ReLU(inplace=True)),\n ('layer1.2.conv2',\n  Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer1.2.bn2',\n  BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.2.drop_block', Identity()),\n ('layer1.2.act2', ReLU(inplace=True)),\n ('layer1.2.aa', Identity()),\n ('layer1.2.conv3',\n  Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer1.2.bn3',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer1.2.act3', ReLU(inplace=True)),\n ('layer2',\n  Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )),\n ('layer2.0',\n  Bottleneck(\n    (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )),\n ('layer2.0.conv1',\n  Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.0.bn1',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.0.act1', ReLU(inplace=True)),\n ('layer2.0.conv2',\n  Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n ('layer2.0.bn2',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.0.drop_block', Identity()),\n ('layer2.0.act2', ReLU(inplace=True)),\n ('layer2.0.aa', Identity()),\n ('layer2.0.conv3',\n  Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.0.bn3',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.0.act3', ReLU(inplace=True)),\n ('layer2.0.downsample',\n  Sequential(\n    (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )),\n ('layer2.0.downsample.0',\n  Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)),\n ('layer2.0.downsample.1',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.1',\n  Bottleneck(\n    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer2.1.conv1',\n  Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.1.bn1',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.1.act1', ReLU(inplace=True)),\n ('layer2.1.conv2',\n  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer2.1.bn2',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.1.drop_block', Identity()),\n ('layer2.1.act2', ReLU(inplace=True)),\n ('layer2.1.aa', Identity()),\n ('layer2.1.conv3',\n  Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.1.bn3',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.1.act3', ReLU(inplace=True)),\n ('layer2.2',\n  Bottleneck(\n    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer2.2.conv1',\n  Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.2.bn1',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.2.act1', ReLU(inplace=True)),\n ('layer2.2.conv2',\n  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer2.2.bn2',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.2.drop_block', Identity()),\n ('layer2.2.act2', ReLU(inplace=True)),\n ('layer2.2.aa', Identity()),\n ('layer2.2.conv3',\n  Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.2.bn3',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.2.act3', ReLU(inplace=True)),\n ('layer2.3',\n  Bottleneck(\n    (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer2.3.conv1',\n  Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.3.bn1',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.3.act1', ReLU(inplace=True)),\n ('layer2.3.conv2',\n  Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer2.3.bn2',\n  BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.3.drop_block', Identity()),\n ('layer2.3.act2', ReLU(inplace=True)),\n ('layer2.3.aa', Identity()),\n ('layer2.3.conv3',\n  Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer2.3.bn3',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer2.3.act3', ReLU(inplace=True)),\n ('layer3',\n  Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (3): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (4): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (5): Bottleneck(\n      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )),\n ('layer3.0',\n  Bottleneck(\n    (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )),\n ('layer3.0.conv1',\n  Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.0.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.0.act1', ReLU(inplace=True)),\n ('layer3.0.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n ('layer3.0.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.0.drop_block', Identity()),\n ('layer3.0.act2', ReLU(inplace=True)),\n ('layer3.0.aa', Identity()),\n ('layer3.0.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.0.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.0.act3', ReLU(inplace=True)),\n ('layer3.0.downsample',\n  Sequential(\n    (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )),\n ('layer3.0.downsample.0',\n  Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)),\n ('layer3.0.downsample.1',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.1',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.1.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.1.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.1.act1', ReLU(inplace=True)),\n ('layer3.1.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.1.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.1.drop_block', Identity()),\n ('layer3.1.act2', ReLU(inplace=True)),\n ('layer3.1.aa', Identity()),\n ('layer3.1.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.1.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.1.act3', ReLU(inplace=True)),\n ('layer3.2',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.2.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.2.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.2.act1', ReLU(inplace=True)),\n ('layer3.2.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.2.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.2.drop_block', Identity()),\n ('layer3.2.act2', ReLU(inplace=True)),\n ('layer3.2.aa', Identity()),\n ('layer3.2.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.2.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.2.act3', ReLU(inplace=True)),\n ('layer3.3',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.3.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.3.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.3.act1', ReLU(inplace=True)),\n ('layer3.3.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.3.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.3.drop_block', Identity()),\n ('layer3.3.act2', ReLU(inplace=True)),\n ('layer3.3.aa', Identity()),\n ('layer3.3.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.3.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.3.act3', ReLU(inplace=True)),\n ('layer3.4',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.4.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.4.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.4.act1', ReLU(inplace=True)),\n ('layer3.4.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.4.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.4.drop_block', Identity()),\n ('layer3.4.act2', ReLU(inplace=True)),\n ('layer3.4.aa', Identity()),\n ('layer3.4.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.4.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.4.act3', ReLU(inplace=True)),\n ('layer3.5',\n  Bottleneck(\n    (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer3.5.conv1',\n  Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.5.bn1',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.5.act1', ReLU(inplace=True)),\n ('layer3.5.conv2',\n  Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer3.5.bn2',\n  BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.5.drop_block', Identity()),\n ('layer3.5.act2', ReLU(inplace=True)),\n ('layer3.5.aa', Identity()),\n ('layer3.5.conv3',\n  Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer3.5.bn3',\n  BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer3.5.act3', ReLU(inplace=True)),\n ('layer4',\n  Sequential(\n    (0): Bottleneck(\n      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n      (downsample): Sequential(\n        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n    (2): Bottleneck(\n      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act1): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (drop_block): Identity()\n      (act2): ReLU(inplace=True)\n      (aa): Identity()\n      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (act3): ReLU(inplace=True)\n    )\n  )),\n ('layer4.0',\n  Bottleneck(\n    (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n    (downsample): Sequential(\n      (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n      (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )),\n ('layer4.0.conv1',\n  Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.0.bn1',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.0.act1', ReLU(inplace=True)),\n ('layer4.0.conv2',\n  Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)),\n ('layer4.0.bn2',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.0.drop_block', Identity()),\n ('layer4.0.act2', ReLU(inplace=True)),\n ('layer4.0.aa', Identity()),\n ('layer4.0.conv3',\n  Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.0.bn3',\n  BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.0.act3', ReLU(inplace=True)),\n ('layer4.0.downsample',\n  Sequential(\n    (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n    (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  )),\n ('layer4.0.downsample.0',\n  Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)),\n ('layer4.0.downsample.1',\n  BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.1',\n  Bottleneck(\n    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer4.1.conv1',\n  Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.1.bn1',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.1.act1', ReLU(inplace=True)),\n ('layer4.1.conv2',\n  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer4.1.bn2',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.1.drop_block', Identity()),\n ('layer4.1.act2', ReLU(inplace=True)),\n ('layer4.1.aa', Identity()),\n ('layer4.1.conv3',\n  Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.1.bn3',\n  BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.1.act3', ReLU(inplace=True)),\n ('layer4.2',\n  Bottleneck(\n    (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act1): ReLU(inplace=True)\n    (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n    (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (drop_block): Identity()\n    (act2): ReLU(inplace=True)\n    (aa): Identity()\n    (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n    (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (act3): ReLU(inplace=True)\n  )),\n ('layer4.2.conv1',\n  Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.2.bn1',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.2.act1', ReLU(inplace=True)),\n ('layer4.2.conv2',\n  Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)),\n ('layer4.2.bn2',\n  BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.2.drop_block', Identity()),\n ('layer4.2.act2', ReLU(inplace=True)),\n ('layer4.2.aa', Identity()),\n ('layer4.2.conv3',\n  Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)),\n ('layer4.2.bn3',\n  BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)),\n ('layer4.2.act3', ReLU(inplace=True)),\n ('global_pool',\n  SelectAdaptivePool2d(pool_type=avg, flatten=Flatten(start_dim=1, end_dim=-1))),\n ('global_pool.pool', AdaptiveAvgPool2d(output_size=1)),\n ('global_pool.flatten', Flatten(start_dim=1, end_dim=-1)),\n ('fc', Linear(in_features=2048, out_features=1000, bias=True))]"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(name, m) for name, m in resnet50.named_modules()][0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:17:40.421823700Z",
     "start_time": "2023-12-28T15:17:39.981823900Z"
    }
   },
   "id": "742f27bdba1c5676"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "[('conv1.weight',\n  Parameter containing:\n  tensor([[[[ 2.0205e-02, -2.3905e-03, -2.0692e-03,  ..., -2.1685e-02,\n             -2.6140e-02,  3.5137e-02],\n            [ 1.1919e-02, -3.9269e-02,  1.0223e-02,  ...,  1.5147e-02,\n              2.0462e-02,  3.0057e-02],\n            [-5.4055e-06,  3.5655e-02, -3.1106e-02,  ...,  2.5908e-02,\n             -8.2757e-03,  9.2408e-03],\n            ...,\n            [ 3.1922e-03, -2.9917e-02, -6.8023e-04,  ...,  8.9421e-03,\n              2.0288e-02, -1.6921e-02],\n            [ 9.2214e-03,  2.8107e-02, -3.1560e-03,  ...,  5.4679e-02,\n              4.0747e-02,  7.1635e-03],\n            [-1.4354e-02,  1.2380e-02,  3.8668e-03,  ...,  1.0404e-02,\n              2.7850e-03, -2.4757e-02]],\n  \n           [[ 1.5083e-02, -4.7006e-02,  1.2246e-03,  ..., -1.4765e-02,\n              4.3166e-02,  1.3303e-02],\n            [ 3.0001e-02, -1.1653e-02,  2.1985e-02,  ..., -1.8013e-03,\n              1.0435e-02, -1.1959e-02],\n            [ 7.8141e-03, -1.2133e-02,  1.5068e-02,  ...,  3.5813e-02,\n             -1.2394e-02,  1.2446e-02],\n            ...,\n            [ 8.6308e-03,  2.7011e-02,  4.3209e-03,  ..., -2.6139e-02,\n             -3.6308e-02, -5.5036e-03],\n            [-2.3940e-02,  2.6287e-03, -2.3679e-03,  ...,  2.8741e-02,\n              4.8739e-02,  1.8239e-02],\n            [-8.5269e-03, -3.5215e-02,  2.1482e-02,  ...,  4.1576e-02,\n              1.9360e-02,  1.3195e-02]],\n  \n           [[ 3.1602e-02, -2.4201e-02, -3.5680e-03,  ..., -1.5726e-02,\n              2.6034e-02, -7.8508e-03],\n            [-1.5276e-02,  1.1890e-02, -1.4376e-03,  ..., -5.4867e-03,\n              2.0695e-03,  2.8955e-02],\n            [-2.2527e-02, -5.8025e-04,  2.0670e-02,  ...,  3.4252e-02,\n             -2.5036e-03, -1.2419e-02],\n            ...,\n            [ 2.7202e-03,  1.5048e-02, -1.5641e-02,  ..., -1.0753e-02,\n              1.1381e-02,  5.9074e-02],\n            [ 2.0294e-02, -3.4114e-02, -1.4550e-03,  ..., -8.5207e-03,\n              3.1333e-02,  3.1698e-02],\n            [-3.4379e-02, -1.2889e-02,  3.1761e-02,  ...,  3.4554e-02,\n              2.3809e-02, -1.6432e-02]]],\n  \n  \n          [[[ 5.7036e-03,  7.0347e-03,  6.1319e-02,  ...,  2.7848e-02,\n             -6.6134e-03,  1.3288e-02],\n            [ 1.1711e-02, -1.4123e-02,  5.0940e-02,  ..., -8.8190e-03,\n              5.8778e-03, -1.8456e-02],\n            [ 1.4140e-02,  2.3809e-02,  1.8546e-02,  ..., -5.7463e-03,\n              1.2362e-03,  1.6378e-02],\n            ...,\n            [-1.6107e-02, -2.9566e-03,  3.8190e-02,  ...,  3.5610e-02,\n             -2.1969e-02, -4.0934e-02],\n            [ 3.9458e-02,  4.3749e-02, -7.5573e-03,  ..., -1.4327e-03,\n             -1.9045e-02,  2.3340e-02],\n            [ 3.1615e-02, -3.2027e-02,  9.8964e-03,  ...,  3.8477e-02,\n             -2.0704e-02,  1.5991e-02]],\n  \n           [[ 5.5651e-02, -5.1981e-02,  5.8360e-03,  ..., -2.8437e-02,\n              2.1817e-02,  3.8454e-02],\n            [-5.1833e-03, -4.1325e-03,  6.5840e-03,  ..., -1.6406e-02,\n              2.7683e-02,  1.2991e-02],\n            [ 3.0617e-02,  6.3705e-02,  1.4437e-03,  ..., -1.6320e-03,\n             -1.5508e-02, -1.4128e-02],\n            ...,\n            [-2.9388e-02, -1.7309e-02, -2.9122e-02,  ...,  1.4107e-02,\n              2.8365e-02,  8.2677e-03],\n            [ 1.3414e-02,  1.0735e-02, -3.2278e-03,  ...,  9.1359e-03,\n             -3.9283e-02, -2.0034e-02],\n            [ 1.7134e-02, -1.7056e-02, -2.5197e-02,  ...,  2.5079e-03,\n              2.1150e-02, -3.2634e-02]],\n  \n           [[ 1.1848e-02, -2.5567e-03,  1.3417e-02,  ..., -1.1782e-02,\n             -6.7100e-03,  1.3251e-02],\n            [-1.3709e-02,  2.0225e-02,  3.3524e-02,  ..., -4.2197e-03,\n              1.0391e-02, -1.3892e-02],\n            [-1.0397e-02, -4.1764e-02,  4.0211e-02,  ...,  2.9085e-02,\n             -2.0469e-02, -7.0829e-03],\n            ...,\n            [ 5.4001e-02,  5.0901e-02, -1.8122e-02,  ...,  1.6229e-02,\n             -4.9057e-02,  7.3125e-03],\n            [ 3.6255e-02, -1.1498e-02, -3.0255e-03,  ...,  4.9947e-02,\n              7.9241e-03,  2.1390e-02],\n            [-1.1954e-02,  2.5755e-02, -4.2196e-03,  ..., -9.7114e-03,\n             -4.3517e-02, -2.1725e-02]]],\n  \n  \n          [[[ 1.9441e-02, -6.3138e-03, -2.8066e-02,  ...,  2.0818e-02,\n              2.8176e-03, -2.0657e-02],\n            [-1.0248e-02,  2.3290e-02, -2.1973e-04,  ..., -3.7387e-03,\n             -9.0462e-03, -5.1965e-02],\n            [ 1.7139e-03, -9.5017e-04,  2.7556e-02,  ..., -1.3388e-03,\n             -4.5942e-02, -6.7081e-03],\n            ...,\n            [-5.5261e-03,  1.1537e-02, -3.7207e-03,  ...,  5.0064e-03,\n              2.3429e-02,  1.4456e-02],\n            [ 5.1691e-02, -6.1622e-03, -9.1988e-04,  ...,  2.5942e-02,\n             -6.2018e-02, -1.5029e-02],\n            [-2.8239e-02, -9.4054e-03, -4.7031e-03,  ..., -5.9704e-02,\n             -2.1722e-02,  1.6728e-02]],\n  \n           [[ 1.2953e-02, -2.1262e-02, -1.7656e-02,  ..., -3.3261e-02,\n              8.1959e-03,  1.2398e-02],\n            [-1.0173e-02, -1.6911e-02,  1.0614e-02,  ...,  3.2688e-02,\n             -1.8414e-02, -3.5086e-02],\n            [ 2.2271e-02, -4.4208e-02,  1.1553e-02,  ...,  7.8297e-03,\n             -3.5210e-02, -2.1322e-02],\n            ...,\n            [-5.5005e-02,  8.4252e-03,  1.1382e-02,  ..., -3.5067e-02,\n             -2.5263e-03, -4.1092e-02],\n            [ 4.4021e-02,  3.7711e-02,  2.5611e-02,  ..., -1.7772e-02,\n             -3.4644e-02, -2.7270e-02],\n            [-1.9713e-02, -9.6579e-03, -4.4251e-03,  ...,  2.3424e-03,\n             -1.6846e-02,  2.0349e-04]],\n  \n           [[ 4.1014e-02,  3.1967e-03, -1.5065e-02,  ..., -2.9923e-02,\n              2.3460e-02,  8.7633e-03],\n            [-1.1828e-02, -3.9624e-02,  1.0316e-02,  ...,  1.6076e-02,\n              3.1923e-03, -8.4323e-03],\n            [ 2.0841e-02,  1.8511e-02, -1.8693e-02,  ...,  1.8843e-02,\n              1.4783e-02, -8.2095e-03],\n            ...,\n            [-8.6175e-03,  4.2012e-03, -2.5294e-02,  ...,  5.6677e-03,\n             -5.6201e-02,  1.2731e-02],\n            [-4.8295e-03,  2.1909e-02,  4.9577e-03,  ..., -4.0834e-03,\n              2.8378e-02,  5.1507e-02],\n            [ 1.2024e-02,  2.4257e-02, -1.7502e-02,  ..., -2.4755e-02,\n              6.6706e-03, -1.9776e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 1.6508e-02,  3.0904e-02, -2.9062e-02,  ..., -2.9244e-02,\n              2.9432e-02,  5.0518e-03],\n            [ 1.1300e-02,  2.7522e-02,  2.7583e-02,  ..., -4.9017e-02,\n              4.5015e-02,  1.4122e-03],\n            [-2.9580e-02, -2.1407e-02,  3.4995e-02,  ..., -4.8124e-03,\n             -1.2429e-02, -1.6051e-02],\n            ...,\n            [-5.2089e-02, -2.6876e-02, -2.0504e-02,  ..., -2.5680e-03,\n             -3.0872e-04,  2.9511e-02],\n            [-1.3734e-02, -6.4345e-03,  1.1440e-02,  ..., -2.9836e-02,\n             -1.4929e-02,  1.2519e-02],\n            [-2.3012e-03,  3.5196e-02, -3.6532e-03,  ...,  1.9822e-02,\n             -2.1415e-02, -9.7333e-03]],\n  \n           [[ 1.2571e-02, -4.4891e-02, -5.0075e-03,  ...,  2.1384e-02,\n             -2.4733e-02,  5.5107e-03],\n            [ 1.2609e-02,  9.9834e-03,  7.1271e-03,  ..., -1.7604e-02,\n              1.4591e-02, -1.1797e-02],\n            [-3.5269e-02,  8.7674e-03,  3.6029e-02,  ..., -8.4927e-03,\n             -3.3031e-02, -5.0926e-02],\n            ...,\n            [-4.1607e-02,  1.7718e-02, -4.0338e-02,  ...,  1.0660e-02,\n             -5.8099e-02,  1.2495e-02],\n            [ 3.3713e-02,  2.1249e-02,  1.0700e-02,  ..., -7.9082e-02,\n             -2.9711e-02, -1.7088e-02],\n            [ 1.6588e-02,  3.4100e-02, -1.5876e-02,  ...,  3.2419e-02,\n             -2.8723e-02, -4.2624e-02]],\n  \n           [[-1.7437e-02,  3.9583e-02,  2.4515e-02,  ...,  2.8756e-02,\n             -2.3897e-02,  1.3300e-03],\n            [-8.8991e-03, -2.0622e-02, -2.3253e-02,  ...,  9.0366e-03,\n              2.7050e-04, -1.0827e-02],\n            [ 2.2254e-02,  2.3756e-02,  3.3088e-03,  ..., -1.7154e-02,\n             -1.8019e-02,  2.2955e-02],\n            ...,\n            [ 1.0251e-02, -3.0935e-02,  3.1150e-03,  ..., -1.1252e-02,\n              1.0673e-03,  2.5444e-02],\n            [ 8.2619e-03, -2.6377e-02,  2.7242e-02,  ..., -3.0989e-03,\n             -2.5490e-02,  3.0521e-02],\n            [-2.6266e-02,  3.0251e-02,  9.1010e-03,  ...,  1.5956e-02,\n             -4.6953e-03, -2.0908e-02]]],\n  \n  \n          [[[-7.4897e-03,  3.2680e-02,  2.0491e-02,  ...,  5.9584e-02,\n             -1.9463e-02,  3.4768e-02],\n            [-6.7759e-03, -2.4649e-02, -2.3188e-02,  ..., -3.0446e-03,\n              5.9373e-02,  4.3334e-03],\n            [-1.9322e-02,  4.4271e-02,  1.3837e-02,  ..., -2.4099e-02,\n             -5.0359e-03, -3.8554e-03],\n            ...,\n            [ 1.3820e-02, -7.0580e-03,  1.6318e-03,  ...,  1.6081e-02,\n             -3.8098e-02,  1.2108e-02],\n            [ 2.7690e-03, -1.7610e-02, -3.5126e-02,  ...,  4.7488e-03,\n             -5.1283e-02, -2.3960e-02],\n            [ 2.0676e-02, -1.5036e-02,  2.5873e-02,  ..., -3.0309e-02,\n             -1.1146e-02,  1.2897e-02]],\n  \n           [[-1.8908e-02, -6.6267e-03, -1.5720e-02,  ...,  5.6900e-02,\n              5.7605e-03,  2.0269e-02],\n            [-1.2481e-02, -2.2107e-03, -5.7907e-02,  ..., -1.1214e-03,\n             -3.9593e-02,  1.6617e-02],\n            [ 1.6587e-02,  1.6491e-02, -3.6589e-03,  ...,  2.4241e-02,\n             -1.0081e-02,  4.5497e-02],\n            ...,\n            [ 6.1241e-03, -1.5664e-02, -2.1847e-02,  ..., -1.9028e-02,\n              1.3661e-02, -1.2527e-02],\n            [ 1.8520e-02,  3.0073e-02, -1.5109e-02,  ..., -4.1136e-03,\n             -2.4439e-02,  2.8699e-02],\n            [ 8.1089e-03,  3.7619e-02, -1.2772e-02,  ...,  2.7639e-02,\n             -3.7560e-02, -5.0457e-02]],\n  \n           [[ 5.3597e-02, -3.0653e-02,  9.3787e-03,  ..., -2.6674e-02,\n              1.8336e-02,  6.6573e-03],\n            [ 2.6618e-02, -5.0091e-02,  1.4210e-03,  ..., -2.7360e-02,\n             -1.8585e-02,  1.0534e-03],\n            [-2.5282e-02, -2.2415e-02, -4.4411e-03,  ..., -8.7972e-03,\n              2.6992e-02, -9.2498e-03],\n            ...,\n            [-1.4720e-02,  1.8461e-02,  2.7196e-02,  ...,  2.1060e-02,\n             -1.9724e-02,  1.9480e-02],\n            [ 4.9876e-02,  1.5148e-03, -1.0424e-02,  ..., -2.8956e-02,\n             -1.0594e-02,  7.2250e-03],\n            [ 4.2594e-03,  6.1981e-03,  1.1039e-02,  ..., -5.8941e-03,\n             -5.8101e-03,  7.1369e-03]]],\n  \n  \n          [[[ 2.4372e-02,  1.5509e-02, -2.0794e-02,  ...,  3.9159e-02,\n             -3.8332e-02, -1.8619e-02],\n            [-3.7797e-02, -3.6067e-02,  2.2466e-02,  ...,  3.8821e-03,\n              6.1536e-03,  4.1941e-02],\n            [-1.6141e-03, -1.2965e-03,  1.1418e-03,  ..., -3.4387e-03,\n             -4.2280e-03, -1.7032e-02],\n            ...,\n            [ 2.2596e-02, -4.5576e-03, -3.6427e-02,  ..., -1.7975e-02,\n             -2.9819e-02,  3.0867e-02],\n            [-1.3974e-02, -2.3854e-02, -2.4647e-02,  ..., -2.7752e-02,\n             -6.6875e-02,  7.4949e-03],\n            [-4.1861e-02,  1.9089e-02,  2.2661e-02,  ..., -1.0697e-02,\n             -2.7676e-03,  1.0051e-02]],\n  \n           [[-2.9766e-02,  1.9077e-02,  1.0466e-02,  ...,  1.8746e-02,\n              3.3052e-02, -3.3485e-02],\n            [ 4.4829e-02, -1.3520e-02,  2.8138e-02,  ...,  6.1510e-03,\n             -6.1483e-03, -1.9508e-02],\n            [ 1.9231e-02,  1.9132e-02, -2.1079e-02,  ...,  8.7133e-03,\n              1.5779e-02, -2.4805e-02],\n            ...,\n            [ 3.6441e-02, -4.4241e-02,  8.3280e-03,  ..., -2.9894e-02,\n             -2.7145e-02, -2.3498e-02],\n            [-7.5467e-03, -1.9729e-02, -1.3039e-03,  ...,  3.5926e-03,\n             -1.6647e-03, -3.0189e-02],\n            [ 4.7935e-03,  2.0252e-02, -2.7340e-02,  ...,  4.3277e-03,\n              1.8503e-02,  4.7513e-02]],\n  \n           [[-6.1403e-02,  1.4447e-02, -2.9383e-02,  ...,  2.0712e-03,\n             -3.2192e-02, -9.2196e-03],\n            [ 2.5356e-03, -2.8008e-02,  6.3749e-03,  ...,  1.0467e-03,\n             -2.9662e-02,  3.6803e-02],\n            [-1.5732e-02, -4.8111e-04,  6.4498e-03,  ..., -1.5811e-02,\n              4.2455e-03, -3.5927e-03],\n            ...,\n            [ 9.1832e-03,  3.7838e-03, -1.3741e-03,  ..., -1.8789e-03,\n             -3.0376e-03,  2.7973e-02],\n            [ 1.1624e-02,  2.0685e-02, -8.5090e-03,  ...,  5.1945e-03,\n             -3.5213e-02, -1.0521e-02],\n            [ 1.4765e-02, -2.5245e-02, -2.3807e-02,  ...,  9.2163e-03,\n             -1.8324e-02, -1.8349e-02]]]], requires_grad=True)),\n ('bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.0.conv1.weight',\n  Parameter containing:\n  tensor([[[[ 0.2290]],\n  \n           [[ 0.3313]],\n  \n           [[ 0.2362]],\n  \n           ...,\n  \n           [[-0.1768]],\n  \n           [[-0.1309]],\n  \n           [[-0.0832]]],\n  \n  \n          [[[-0.3692]],\n  \n           [[ 0.0679]],\n  \n           [[-0.0160]],\n  \n           ...,\n  \n           [[-0.3867]],\n  \n           [[-0.0800]],\n  \n           [[ 0.0887]]],\n  \n  \n          [[[ 0.0321]],\n  \n           [[ 0.0199]],\n  \n           [[ 0.1873]],\n  \n           ...,\n  \n           [[-0.0699]],\n  \n           [[-0.0163]],\n  \n           [[-0.1113]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.1393]],\n  \n           [[-0.4019]],\n  \n           [[-0.3346]],\n  \n           ...,\n  \n           [[-0.2075]],\n  \n           [[ 0.0176]],\n  \n           [[ 0.0399]]],\n  \n  \n          [[[-0.0983]],\n  \n           [[ 0.0173]],\n  \n           [[-0.1739]],\n  \n           ...,\n  \n           [[-0.0412]],\n  \n           [[-0.1317]],\n  \n           [[-0.1213]]],\n  \n  \n          [[[-0.3507]],\n  \n           [[ 0.1722]],\n  \n           [[ 0.0333]],\n  \n           ...,\n  \n           [[-0.1155]],\n  \n           [[-0.0241]],\n  \n           [[ 0.0263]]]], requires_grad=True)),\n ('layer1.0.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer1.0.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.0.conv2.weight',\n  Parameter containing:\n  tensor([[[[-8.5353e-02,  4.2442e-02, -1.9138e-03],\n            [-1.4604e-02, -8.3252e-02, -1.5588e-02],\n            [-2.2293e-02,  7.0880e-03, -2.1843e-02]],\n  \n           [[ 1.2756e-02, -5.8847e-02, -2.8348e-02],\n            [ 4.7153e-02, -9.5202e-02,  1.3774e-01],\n            [-6.3986e-02,  1.3814e-01,  4.2420e-02]],\n  \n           [[ 4.5159e-02,  9.7097e-02,  1.9015e-02],\n            [-1.1463e-01,  1.5932e-02, -5.9595e-02],\n            [-2.8581e-02, -6.7515e-02, -8.0742e-02]],\n  \n           ...,\n  \n           [[ 1.3945e-01, -3.7596e-02, -8.4238e-02],\n            [ 9.0191e-02,  3.0739e-02, -8.3794e-03],\n            [-2.9000e-03,  2.9931e-03,  9.9263e-02]],\n  \n           [[-9.4439e-02, -7.9937e-02,  1.1385e-01],\n            [ 1.0231e-02, -5.5462e-02, -3.1065e-02],\n            [-2.7779e-02,  8.5615e-04, -2.8768e-02]],\n  \n           [[ 5.6608e-02, -3.2057e-03, -5.5263e-02],\n            [-8.3164e-02, -4.3020e-03,  2.5182e-02],\n            [-4.3467e-02, -1.0800e-01, -3.3470e-02]]],\n  \n  \n          [[[ 6.6404e-02, -1.4066e-02,  6.1656e-02],\n            [-2.8239e-02,  3.9282e-02,  4.5282e-02],\n            [ 9.5756e-02,  4.7193e-02, -3.2316e-02]],\n  \n           [[-4.0710e-02, -4.9333e-02, -5.9756e-02],\n            [-8.1629e-02, -2.4763e-02, -3.1091e-02],\n            [-3.9833e-03, -4.6530e-02,  9.1292e-03]],\n  \n           [[-1.1391e-02,  2.1991e-02, -6.1529e-03],\n            [-5.5138e-02,  7.5181e-03,  3.8683e-02],\n            [-8.3326e-03, -4.9492e-02,  1.4074e-01]],\n  \n           ...,\n  \n           [[-9.3771e-02,  1.0060e-02, -4.4185e-02],\n            [ 1.1426e-02, -4.9989e-02,  5.2989e-02],\n            [-3.5208e-02, -8.4469e-02,  2.2738e-02]],\n  \n           [[-9.3101e-03,  5.8356e-02, -6.1444e-03],\n            [-2.3899e-02, -9.9446e-03, -1.6147e-03],\n            [-1.1913e-01, -6.7731e-04,  6.3689e-02]],\n  \n           [[ 5.8050e-02,  1.0001e-01, -2.0225e-02],\n            [ 8.8134e-04,  3.7452e-02, -6.9153e-02],\n            [ 6.8750e-02, -4.4218e-02,  4.4150e-02]]],\n  \n  \n          [[[ 6.8471e-02, -3.5197e-02, -1.4969e-02],\n            [-6.3379e-02,  1.0913e-01, -4.1560e-02],\n            [ 2.6482e-02,  4.8737e-02, -2.8062e-02]],\n  \n           [[-5.5004e-02,  9.2907e-03,  1.2454e-02],\n            [ 1.4338e-02, -8.4946e-02,  5.0657e-02],\n            [-5.1343e-02, -1.8879e-02,  6.4731e-02]],\n  \n           [[-1.2408e-03,  7.7626e-02, -2.3187e-02],\n            [-3.9377e-03, -3.2439e-02, -4.1149e-03],\n            [ 5.1900e-02, -7.4798e-02,  2.9986e-02]],\n  \n           ...,\n  \n           [[ 8.0931e-02,  1.2289e-01, -8.4175e-03],\n            [ 7.5686e-02,  3.4618e-03, -5.7763e-03],\n            [-9.1568e-02, -6.6277e-02,  8.0384e-02]],\n  \n           [[-2.8032e-02,  1.3912e-02, -3.4598e-02],\n            [ 2.5609e-03, -1.0839e-01,  6.4283e-02],\n            [-2.4107e-03,  2.2428e-02, -1.7362e-02]],\n  \n           [[ 2.9961e-02,  2.0006e-02, -5.3228e-03],\n            [ 4.6292e-03, -8.1682e-02, -5.6117e-03],\n            [ 8.6949e-02,  2.0804e-02,  1.1615e-01]]],\n  \n  \n          ...,\n  \n  \n          [[[ 1.4988e-02,  1.5524e-01, -1.2087e-01],\n            [-2.7630e-02, -5.5988e-02, -4.6526e-02],\n            [-1.5355e-01, -4.5186e-02, -1.0135e-01]],\n  \n           [[ 5.7352e-03, -4.8523e-02,  1.6264e-01],\n            [-7.0921e-02,  7.9327e-03, -2.5498e-02],\n            [ 4.3608e-02, -3.4504e-02,  4.2297e-02]],\n  \n           [[ 1.6033e-02, -8.8280e-03,  2.7813e-02],\n            [ 4.6721e-02,  4.7857e-02, -4.3148e-02],\n            [-4.4466e-04, -6.5594e-02,  1.7914e-02]],\n  \n           ...,\n  \n           [[-1.0129e-02, -2.9319e-02,  4.2583e-02],\n            [-1.2842e-01,  1.3437e-04, -5.2542e-02],\n            [ 9.0520e-02,  2.2589e-02, -3.6202e-02]],\n  \n           [[ 5.1656e-02, -8.9629e-02,  8.8598e-02],\n            [ 2.6451e-02,  7.1046e-02, -8.4856e-03],\n            [-1.0367e-01,  8.8904e-02, -3.7408e-02]],\n  \n           [[ 4.7444e-02,  2.9483e-02,  1.0843e-01],\n            [ 1.4208e-01,  3.0472e-02, -3.7760e-03],\n            [ 2.1232e-02,  2.1296e-03,  1.0800e-02]]],\n  \n  \n          [[[-7.9437e-02, -4.7137e-02,  3.2004e-02],\n            [ 3.4575e-03, -8.6786e-02, -1.1978e-01],\n            [-1.0690e-01, -1.7928e-02, -2.0724e-02]],\n  \n           [[-5.8107e-02, -5.7194e-02,  1.7888e-01],\n            [ 4.1697e-02,  5.3274e-02, -1.0917e-01],\n            [ 3.3153e-02, -4.3406e-03,  2.8869e-02]],\n  \n           [[-1.0578e-01,  2.4505e-02, -3.9027e-02],\n            [ 1.0042e-01,  1.8018e-02, -6.0525e-02],\n            [-3.1232e-02,  2.0422e-02,  3.9941e-02]],\n  \n           ...,\n  \n           [[-8.5304e-03,  1.2059e-01,  1.0680e-02],\n            [-3.6759e-02,  6.6216e-03, -6.9169e-02],\n            [-3.9427e-02,  4.9240e-02, -7.6128e-02]],\n  \n           [[-2.2726e-03, -3.5626e-02, -4.9596e-02],\n            [-9.9419e-02, -2.6064e-02,  3.2886e-02],\n            [ 3.7754e-02, -4.9676e-02,  3.7577e-02]],\n  \n           [[ 1.1832e-01,  4.5453e-03,  4.4521e-02],\n            [ 5.2852e-02, -1.2235e-02, -3.3951e-02],\n            [-2.3239e-02,  4.8260e-02,  5.4950e-02]]],\n  \n  \n          [[[ 9.1501e-03,  4.3327e-04,  6.2598e-02],\n            [-1.3631e-02, -1.3479e-01,  1.0200e-01],\n            [-1.6141e-02,  3.0046e-02,  6.8658e-02]],\n  \n           [[ 2.1715e-02,  7.0487e-02,  9.3667e-02],\n            [ 1.1754e-02, -5.1640e-02, -1.1317e-02],\n            [ 2.0853e-03, -3.5191e-02, -2.5556e-02]],\n  \n           [[ 1.1396e-01, -1.7334e-02,  1.1268e-01],\n            [ 2.0064e-02,  1.0131e-01, -1.1123e-01],\n            [-9.5416e-02, -8.4598e-02,  9.5427e-02]],\n  \n           ...,\n  \n           [[ 1.1394e-01,  2.4958e-02, -8.8216e-03],\n            [ 6.2458e-02, -1.2677e-02, -4.4292e-02],\n            [-1.2797e-01,  6.6629e-04, -8.6260e-02]],\n  \n           [[ 6.5938e-03, -6.4041e-02,  2.2439e-02],\n            [-2.1487e-03,  5.1897e-02,  8.9510e-02],\n            [ 3.5998e-02,  4.4589e-02,  1.1359e-02]],\n  \n           [[ 1.3785e-02, -4.7691e-02,  1.1545e-01],\n            [ 8.9681e-02,  1.9578e-02,  1.0743e-02],\n            [ 1.7497e-02,  1.4757e-02, -8.0502e-02]]]], requires_grad=True)),\n ('layer1.0.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer1.0.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.0.conv3.weight',\n  Parameter containing:\n  tensor([[[[ 0.0413]],\n  \n           [[ 0.0872]],\n  \n           [[-0.1247]],\n  \n           ...,\n  \n           [[-0.1366]],\n  \n           [[-0.0578]],\n  \n           [[ 0.0542]]],\n  \n  \n          [[[-0.0034]],\n  \n           [[ 0.1183]],\n  \n           [[ 0.0281]],\n  \n           ...,\n  \n           [[-0.1057]],\n  \n           [[ 0.0387]],\n  \n           [[ 0.0256]]],\n  \n  \n          [[[ 0.0482]],\n  \n           [[-0.0497]],\n  \n           [[-0.0084]],\n  \n           ...,\n  \n           [[-0.0399]],\n  \n           [[-0.0327]],\n  \n           [[-0.0984]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.1554]],\n  \n           [[ 0.1927]],\n  \n           [[ 0.0380]],\n  \n           ...,\n  \n           [[-0.0671]],\n  \n           [[-0.0318]],\n  \n           [[-0.0181]]],\n  \n  \n          [[[ 0.0598]],\n  \n           [[ 0.0496]],\n  \n           [[ 0.0455]],\n  \n           ...,\n  \n           [[-0.0472]],\n  \n           [[ 0.1054]],\n  \n           [[ 0.0357]]],\n  \n  \n          [[[ 0.0682]],\n  \n           [[ 0.0668]],\n  \n           [[ 0.1178]],\n  \n           ...,\n  \n           [[ 0.0272]],\n  \n           [[ 0.0508]],\n  \n           [[ 0.0411]]]], requires_grad=True)),\n ('layer1.0.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.0.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.0.downsample.0.weight',\n  Parameter containing:\n  tensor([[[[ 0.2302]],\n  \n           [[ 0.0115]],\n  \n           [[ 0.3872]],\n  \n           ...,\n  \n           [[ 0.0548]],\n  \n           [[ 0.1249]],\n  \n           [[-0.0445]]],\n  \n  \n          [[[ 0.1131]],\n  \n           [[ 0.0776]],\n  \n           [[ 0.0465]],\n  \n           ...,\n  \n           [[ 0.0942]],\n  \n           [[-0.0379]],\n  \n           [[-0.0159]]],\n  \n  \n          [[[ 0.0495]],\n  \n           [[ 0.1434]],\n  \n           [[-0.0829]],\n  \n           ...,\n  \n           [[ 0.0878]],\n  \n           [[-0.0151]],\n  \n           [[ 0.0321]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.1055]],\n  \n           [[-0.2234]],\n  \n           [[ 0.0616]],\n  \n           ...,\n  \n           [[ 0.0533]],\n  \n           [[ 0.0322]],\n  \n           [[ 0.0866]]],\n  \n  \n          [[[-0.1012]],\n  \n           [[-0.0883]],\n  \n           [[ 0.1247]],\n  \n           ...,\n  \n           [[-0.0140]],\n  \n           [[ 0.1246]],\n  \n           [[ 0.0889]]],\n  \n  \n          [[[-0.0217]],\n  \n           [[-0.2150]],\n  \n           [[-0.1290]],\n  \n           ...,\n  \n           [[-0.0731]],\n  \n           [[-0.0035]],\n  \n           [[ 0.0307]]]], requires_grad=True)),\n ('layer1.0.downsample.1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer1.0.downsample.1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.1.conv1.weight',\n  Parameter containing:\n  tensor([[[[-0.2611]],\n  \n           [[ 0.1386]],\n  \n           [[-0.0270]],\n  \n           ...,\n  \n           [[ 0.1107]],\n  \n           [[ 0.2469]],\n  \n           [[-0.2767]]],\n  \n  \n          [[[-0.0702]],\n  \n           [[ 0.2575]],\n  \n           [[-0.1334]],\n  \n           ...,\n  \n           [[-0.0319]],\n  \n           [[ 0.1562]],\n  \n           [[-0.0166]]],\n  \n  \n          [[[-0.0584]],\n  \n           [[ 0.0050]],\n  \n           [[-0.0705]],\n  \n           ...,\n  \n           [[-0.0733]],\n  \n           [[-0.0407]],\n  \n           [[ 0.0555]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.4171]],\n  \n           [[ 0.1203]],\n  \n           [[-0.0161]],\n  \n           ...,\n  \n           [[ 0.0299]],\n  \n           [[ 0.0792]],\n  \n           [[-0.1504]]],\n  \n  \n          [[[ 0.1538]],\n  \n           [[ 0.4424]],\n  \n           [[-0.2124]],\n  \n           ...,\n  \n           [[ 0.0160]],\n  \n           [[-0.0894]],\n  \n           [[-0.0954]]],\n  \n  \n          [[[ 0.2508]],\n  \n           [[ 0.0981]],\n  \n           [[ 0.1517]],\n  \n           ...,\n  \n           [[-0.0328]],\n  \n           [[ 0.0270]],\n  \n           [[-0.5096]]]], requires_grad=True)),\n ('layer1.1.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer1.1.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.1.conv2.weight',\n  Parameter containing:\n  tensor([[[[-0.0761, -0.0680,  0.1190],\n            [-0.0220,  0.0170,  0.1041],\n            [-0.1102, -0.0218,  0.0276]],\n  \n           [[-0.0449,  0.0087, -0.0510],\n            [ 0.0544,  0.0295, -0.0865],\n            [ 0.0440, -0.0022,  0.0425]],\n  \n           [[ 0.0111, -0.0058, -0.0094],\n            [-0.0358, -0.0008,  0.1008],\n            [-0.1092, -0.0342, -0.0803]],\n  \n           ...,\n  \n           [[-0.0304, -0.0038,  0.0153],\n            [-0.0090, -0.0025,  0.1180],\n            [ 0.0202, -0.0396,  0.0919]],\n  \n           [[ 0.0480, -0.0301,  0.0307],\n            [ 0.0235,  0.0152,  0.0921],\n            [-0.0579,  0.0070, -0.0962]],\n  \n           [[-0.0308, -0.0284,  0.0663],\n            [-0.1260,  0.0022,  0.0029],\n            [ 0.0653,  0.0226, -0.0320]]],\n  \n  \n          [[[ 0.0536,  0.0759,  0.0370],\n            [ 0.0595,  0.0877, -0.1594],\n            [ 0.0149,  0.0525,  0.1823]],\n  \n           [[ 0.0306, -0.0366,  0.0387],\n            [-0.0209, -0.0651, -0.0072],\n            [ 0.1328,  0.0492,  0.0556]],\n  \n           [[ 0.0480, -0.0496, -0.1207],\n            [-0.0277, -0.0344, -0.0428],\n            [-0.1035, -0.0300,  0.0669]],\n  \n           ...,\n  \n           [[ 0.0179,  0.0032,  0.0266],\n            [ 0.0428, -0.0178, -0.0024],\n            [ 0.0625,  0.0131,  0.0136]],\n  \n           [[ 0.0190, -0.0036,  0.1004],\n            [-0.1120, -0.0176,  0.0060],\n            [-0.0099, -0.1138, -0.0483]],\n  \n           [[-0.0273,  0.0170,  0.0589],\n            [-0.0367, -0.0033,  0.0358],\n            [ 0.0008, -0.1269,  0.0433]]],\n  \n  \n          [[[-0.0309,  0.0196,  0.0111],\n            [-0.0149,  0.0639,  0.0440],\n            [-0.0299, -0.0455, -0.0161]],\n  \n           [[ 0.1299,  0.0826,  0.0206],\n            [-0.0026, -0.0634, -0.0422],\n            [-0.0221,  0.0322,  0.0217]],\n  \n           [[-0.0822,  0.0262, -0.0140],\n            [-0.1324, -0.0146, -0.0364],\n            [ 0.0331, -0.0344,  0.0996]],\n  \n           ...,\n  \n           [[-0.0180, -0.0299,  0.0010],\n            [-0.0444, -0.0364,  0.0259],\n            [-0.0607, -0.0079, -0.0413]],\n  \n           [[-0.0712, -0.0613, -0.0099],\n            [-0.0326,  0.0697, -0.1162],\n            [ 0.0466, -0.0305,  0.0081]],\n  \n           [[-0.0030, -0.0225,  0.0573],\n            [-0.0579,  0.0566, -0.0257],\n            [ 0.0212, -0.0669,  0.0343]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0075,  0.0533, -0.0252],\n            [-0.0002,  0.0185, -0.0457],\n            [ 0.0575, -0.0279, -0.0231]],\n  \n           [[-0.0025, -0.0967, -0.0964],\n            [ 0.0077, -0.0874,  0.0063],\n            [ 0.0214, -0.0251,  0.0103]],\n  \n           [[-0.1013,  0.0515,  0.0510],\n            [ 0.0216,  0.0175, -0.0474],\n            [-0.0415,  0.0455, -0.0269]],\n  \n           ...,\n  \n           [[ 0.0778, -0.0191, -0.0706],\n            [ 0.0573,  0.0219, -0.0233],\n            [ 0.0186,  0.0261, -0.0428]],\n  \n           [[-0.0150,  0.0065,  0.1109],\n            [ 0.0983, -0.0402, -0.0577],\n            [ 0.0291,  0.0630,  0.0320]],\n  \n           [[ 0.1271,  0.0369, -0.0152],\n            [ 0.0477,  0.0073, -0.0525],\n            [-0.0083,  0.0534, -0.0394]]],\n  \n  \n          [[[ 0.0219,  0.0118, -0.0518],\n            [-0.0484, -0.0842,  0.0487],\n            [-0.0443,  0.0006, -0.0173]],\n  \n           [[-0.0271, -0.0089,  0.0509],\n            [ 0.0078, -0.0025, -0.0208],\n            [-0.0011, -0.0077, -0.0575]],\n  \n           [[ 0.0249, -0.0075, -0.0122],\n            [ 0.0842,  0.0048, -0.0334],\n            [ 0.0288,  0.0258,  0.0602]],\n  \n           ...,\n  \n           [[ 0.1379, -0.0036,  0.0563],\n            [-0.0203, -0.0756,  0.0138],\n            [-0.0086, -0.0492, -0.0315]],\n  \n           [[-0.0203,  0.0524,  0.0591],\n            [ 0.0119, -0.0162,  0.0316],\n            [-0.0638, -0.0052, -0.0103]],\n  \n           [[ 0.0402,  0.0109,  0.0299],\n            [ 0.0133,  0.0444,  0.0548],\n            [ 0.0247, -0.1040,  0.0890]]],\n  \n  \n          [[[-0.0281, -0.0561,  0.0212],\n            [ 0.0479,  0.0497, -0.0372],\n            [-0.0873,  0.0718, -0.0501]],\n  \n           [[-0.0032, -0.0296,  0.0230],\n            [-0.0203, -0.0180,  0.0390],\n            [ 0.1703,  0.0383,  0.1231]],\n  \n           [[-0.0237,  0.0717,  0.0343],\n            [ 0.0196, -0.0072,  0.0537],\n            [ 0.0045,  0.0019, -0.0254]],\n  \n           ...,\n  \n           [[-0.0180,  0.0322,  0.0653],\n            [-0.0365,  0.0197,  0.0718],\n            [-0.0059,  0.0274,  0.0364]],\n  \n           [[ 0.0402, -0.1024,  0.0109],\n            [-0.1263,  0.0491,  0.0409],\n            [-0.0649,  0.1100,  0.0808]],\n  \n           [[ 0.0184, -0.1448, -0.0140],\n            [-0.0274,  0.0083, -0.0272],\n            [ 0.0826, -0.0148, -0.0603]]]], requires_grad=True)),\n ('layer1.1.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer1.1.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.1.conv3.weight',\n  Parameter containing:\n  tensor([[[[ 0.0495]],\n  \n           [[ 0.1060]],\n  \n           [[ 0.0348]],\n  \n           ...,\n  \n           [[ 0.0021]],\n  \n           [[ 0.0008]],\n  \n           [[-0.0600]]],\n  \n  \n          [[[-0.0754]],\n  \n           [[ 0.0518]],\n  \n           [[-0.1467]],\n  \n           ...,\n  \n           [[ 0.0112]],\n  \n           [[ 0.1063]],\n  \n           [[-0.1654]]],\n  \n  \n          [[[-0.0530]],\n  \n           [[ 0.1251]],\n  \n           [[ 0.0805]],\n  \n           ...,\n  \n           [[ 0.0032]],\n  \n           [[ 0.0285]],\n  \n           [[ 0.0513]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.1960]],\n  \n           [[ 0.0398]],\n  \n           [[ 0.0405]],\n  \n           ...,\n  \n           [[-0.0247]],\n  \n           [[ 0.0511]],\n  \n           [[ 0.1070]]],\n  \n  \n          [[[ 0.0446]],\n  \n           [[ 0.0621]],\n  \n           [[ 0.1272]],\n  \n           ...,\n  \n           [[-0.0241]],\n  \n           [[ 0.0305]],\n  \n           [[ 0.0363]]],\n  \n  \n          [[[-0.2069]],\n  \n           [[ 0.0013]],\n  \n           [[-0.1190]],\n  \n           ...,\n  \n           [[-0.0037]],\n  \n           [[-0.0831]],\n  \n           [[-0.1900]]]], requires_grad=True)),\n ('layer1.1.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.1.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.2.conv1.weight',\n  Parameter containing:\n  tensor([[[[-0.1843]],\n  \n           [[-0.1826]],\n  \n           [[-0.2861]],\n  \n           ...,\n  \n           [[ 0.0362]],\n  \n           [[ 0.2487]],\n  \n           [[-0.1056]]],\n  \n  \n          [[[-0.1508]],\n  \n           [[-0.2946]],\n  \n           [[ 0.1051]],\n  \n           ...,\n  \n           [[ 0.1143]],\n  \n           [[ 0.1828]],\n  \n           [[-0.1549]]],\n  \n  \n          [[[ 0.0008]],\n  \n           [[-0.4955]],\n  \n           [[ 0.0088]],\n  \n           ...,\n  \n           [[-0.0345]],\n  \n           [[ 0.2328]],\n  \n           [[ 0.1095]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0046]],\n  \n           [[ 0.0422]],\n  \n           [[ 0.1480]],\n  \n           ...,\n  \n           [[-0.3768]],\n  \n           [[ 0.1282]],\n  \n           [[-0.1075]]],\n  \n  \n          [[[ 0.0360]],\n  \n           [[-0.0755]],\n  \n           [[-0.0259]],\n  \n           ...,\n  \n           [[ 0.0494]],\n  \n           [[-0.3663]],\n  \n           [[ 0.0763]]],\n  \n  \n          [[[ 0.0492]],\n  \n           [[ 0.0731]],\n  \n           [[-0.0499]],\n  \n           ...,\n  \n           [[-0.1093]],\n  \n           [[ 0.1039]],\n  \n           [[-0.0451]]]], requires_grad=True)),\n ('layer1.2.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer1.2.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.2.conv2.weight',\n  Parameter containing:\n  tensor([[[[ 2.7623e-02, -2.4443e-02,  4.5454e-02],\n            [ 8.7865e-02,  2.6252e-02,  6.0390e-03],\n            [ 6.4354e-02,  1.5069e-02,  2.8763e-02]],\n  \n           [[ 1.0857e-01,  9.5209e-03, -3.1717e-02],\n            [-8.3909e-02,  3.4287e-02, -1.4324e-02],\n            [-5.9549e-03, -1.5819e-02, -3.5980e-02]],\n  \n           [[-8.5880e-02,  1.6646e-02,  1.2425e-01],\n            [-2.6203e-02, -5.9565e-02, -3.9389e-02],\n            [-2.6639e-02, -4.4188e-02, -1.3339e-03]],\n  \n           ...,\n  \n           [[-2.4837e-02,  5.7556e-02, -3.5952e-02],\n            [ 1.8752e-02,  3.6821e-02, -5.4847e-02],\n            [-2.1440e-02,  3.2884e-02,  3.4099e-02]],\n  \n           [[-3.2155e-02, -8.0906e-02,  1.5429e-02],\n            [ 1.9014e-02,  3.3896e-03,  6.8020e-02],\n            [ 9.0992e-02,  1.3429e-01,  3.1759e-02]],\n  \n           [[ 2.1567e-02,  3.0925e-02, -7.6576e-02],\n            [-1.1861e-02,  8.9780e-02, -5.2691e-03],\n            [-7.9270e-02, -2.1960e-02, -2.5949e-02]]],\n  \n  \n          [[[-9.3808e-02, -3.4036e-02,  7.0871e-02],\n            [-3.6460e-02,  2.6389e-02,  1.2293e-02],\n            [-3.4808e-02, -8.4648e-02, -2.3852e-02]],\n  \n           [[-9.1580e-02, -1.3380e-01,  1.0249e-01],\n            [-3.5538e-02, -8.9153e-02,  7.2105e-02],\n            [-9.4974e-02,  1.1773e-01,  1.0579e-02]],\n  \n           [[ 9.7100e-03, -1.3482e-02,  1.0436e-01],\n            [-4.5951e-02,  1.1003e-02, -9.9243e-05],\n            [-5.9685e-02, -1.0033e-01, -4.7110e-02]],\n  \n           ...,\n  \n           [[ 1.0462e-02,  1.0273e-02,  5.1620e-02],\n            [-2.6795e-02,  2.0875e-02, -5.0219e-02],\n            [-1.6426e-01, -7.4874e-02, -1.2192e-01]],\n  \n           [[-5.0368e-03,  3.4347e-02, -8.3174e-02],\n            [ 9.7084e-02,  8.0580e-02, -8.3466e-02],\n            [-5.2946e-03, -8.7937e-02,  6.9336e-02]],\n  \n           [[ 7.6833e-02, -3.7339e-02,  3.1383e-02],\n            [-4.4461e-02, -1.7069e-02,  4.0680e-02],\n            [ 5.5677e-02,  8.6088e-02, -9.6315e-02]]],\n  \n  \n          [[[ 7.4762e-02, -7.1960e-02, -2.5068e-02],\n            [ 3.5040e-02, -2.2360e-02, -4.9041e-02],\n            [-7.8669e-03,  2.9328e-02,  6.2925e-02]],\n  \n           [[ 3.2424e-02, -5.5091e-02,  4.0217e-03],\n            [ 2.4502e-02,  1.2622e-02, -1.3791e-02],\n            [ 2.5224e-02,  2.3484e-02,  2.0312e-02]],\n  \n           [[-2.2768e-02,  8.3269e-02, -3.7646e-02],\n            [-5.0028e-02,  5.5880e-02,  5.2385e-02],\n            [-1.2832e-02, -2.7209e-02,  3.0309e-02]],\n  \n           ...,\n  \n           [[ 8.0386e-02, -2.6481e-02, -3.0677e-02],\n            [-5.6881e-02,  9.4150e-03, -4.5432e-02],\n            [-6.4330e-02,  6.6453e-02, -2.4158e-03]],\n  \n           [[ 1.8041e-02, -2.0965e-02,  6.2920e-02],\n            [ 5.2443e-02, -1.3662e-01,  2.3951e-02],\n            [ 2.3993e-02, -4.7842e-02,  1.8909e-02]],\n  \n           [[-2.2260e-02, -2.1502e-02, -1.3355e-04],\n            [ 1.4345e-01, -1.4556e-03,  9.0243e-02],\n            [ 2.6895e-02, -1.3526e-02, -9.9742e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[-4.0446e-02,  4.8274e-02, -1.0063e-01],\n            [-5.6964e-02,  1.8800e-02,  6.5169e-02],\n            [ 1.3205e-01,  4.2361e-02,  4.9500e-02]],\n  \n           [[-6.9711e-02, -9.0528e-02,  5.0771e-02],\n            [-3.9869e-02,  6.1646e-03,  1.4484e-02],\n            [ 2.3159e-02, -8.5628e-03, -1.9791e-02]],\n  \n           [[-6.9742e-02, -8.4354e-02,  2.0633e-02],\n            [ 4.0481e-02, -4.6717e-02, -3.5373e-02],\n            [ 6.4515e-02,  1.0619e-01, -4.2886e-02]],\n  \n           ...,\n  \n           [[ 2.9073e-04, -9.5342e-02,  1.1611e-01],\n            [-3.4193e-02, -5.3242e-02, -6.3283e-02],\n            [-2.5843e-02,  9.4612e-03, -5.5869e-02]],\n  \n           [[ 6.6120e-02, -5.6313e-02,  1.4887e-01],\n            [-1.7104e-04, -1.5355e-02, -2.9736e-02],\n            [ 3.9859e-02,  7.0039e-02, -8.8858e-02]],\n  \n           [[-7.3081e-03,  9.9355e-03,  1.4254e-02],\n            [ 2.2108e-03, -6.6447e-02, -1.1148e-02],\n            [ 6.7862e-02, -3.3462e-02, -4.2591e-02]]],\n  \n  \n          [[[-1.3626e-02, -1.2138e-01,  5.2448e-02],\n            [-7.2336e-03,  1.5065e-02,  7.5403e-02],\n            [-1.0493e-01, -2.1835e-02, -3.9315e-03]],\n  \n           [[ 1.2141e-02, -7.1064e-02, -6.8463e-02],\n            [-6.7753e-02,  2.4702e-02, -1.7162e-01],\n            [ 8.9272e-02,  3.0368e-02, -4.8349e-02]],\n  \n           [[ 9.4632e-02, -1.1466e-02,  2.5121e-03],\n            [ 1.7472e-02,  1.3335e-02,  2.6825e-02],\n            [ 1.3707e-02, -6.1328e-02, -7.7781e-02]],\n  \n           ...,\n  \n           [[ 2.6461e-02,  5.7461e-02, -3.1987e-03],\n            [ 1.1933e-02,  3.9273e-02,  5.4511e-02],\n            [-9.3806e-02,  5.2417e-03, -1.4411e-02]],\n  \n           [[-2.6175e-02,  4.1200e-02,  5.7196e-02],\n            [-7.8570e-04, -1.5683e-02, -2.3434e-02],\n            [ 2.1435e-02,  2.2791e-02,  4.6291e-02]],\n  \n           [[-8.0804e-02,  3.7061e-02, -3.3381e-02],\n            [ 3.4563e-02, -4.9236e-02,  5.5186e-02],\n            [ 6.3069e-02, -1.0868e-01,  5.3660e-02]]],\n  \n  \n          [[[ 1.2605e-01,  6.6433e-02,  8.8640e-02],\n            [ 8.5893e-03, -3.3182e-02,  6.5961e-02],\n            [ 7.2643e-03, -1.8267e-02, -1.8358e-02]],\n  \n           [[ 2.4428e-02,  8.6418e-03, -5.2183e-04],\n            [ 6.5911e-02, -7.1729e-02, -1.8189e-02],\n            [-2.8079e-02,  2.0210e-02, -4.4632e-02]],\n  \n           [[-6.8029e-02, -8.9508e-02,  6.0479e-02],\n            [-4.0031e-02,  3.7867e-02, -8.2941e-02],\n            [-1.1101e-01,  2.2200e-02,  3.4795e-03]],\n  \n           ...,\n  \n           [[-8.1502e-04, -1.7430e-02, -1.3901e-02],\n            [-6.0020e-02, -3.3643e-02, -1.5537e-02],\n            [ 5.1050e-03,  9.4597e-02, -2.7701e-02]],\n  \n           [[-7.1083e-03,  6.4714e-02, -1.9855e-02],\n            [ 6.6565e-02,  1.0231e-01,  3.2664e-02],\n            [-9.1783e-02,  6.9543e-02, -2.8385e-02]],\n  \n           [[ 1.5205e-02,  9.6691e-02,  4.7726e-02],\n            [-2.0648e-02, -8.0649e-02, -1.5861e-01],\n            [-3.9811e-02,  7.8259e-02,  2.6619e-02]]]], requires_grad=True)),\n ('layer1.2.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer1.2.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.2.conv3.weight',\n  Parameter containing:\n  tensor([[[[-0.0306]],\n  \n           [[ 0.0725]],\n  \n           [[ 0.0381]],\n  \n           ...,\n  \n           [[ 0.0420]],\n  \n           [[ 0.0278]],\n  \n           [[ 0.0542]]],\n  \n  \n          [[[ 0.0608]],\n  \n           [[ 0.0121]],\n  \n           [[ 0.0613]],\n  \n           ...,\n  \n           [[ 0.0566]],\n  \n           [[-0.0026]],\n  \n           [[-0.1931]]],\n  \n  \n          [[[-0.0136]],\n  \n           [[-0.1026]],\n  \n           [[ 0.0474]],\n  \n           ...,\n  \n           [[ 0.1794]],\n  \n           [[-0.0550]],\n  \n           [[ 0.0323]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0182]],\n  \n           [[-0.0238]],\n  \n           [[ 0.0016]],\n  \n           ...,\n  \n           [[ 0.0263]],\n  \n           [[ 0.0003]],\n  \n           [[ 0.0154]]],\n  \n  \n          [[[ 0.0377]],\n  \n           [[ 0.1148]],\n  \n           [[ 0.1470]],\n  \n           ...,\n  \n           [[ 0.1437]],\n  \n           [[-0.0267]],\n  \n           [[-0.3001]]],\n  \n  \n          [[[-0.1186]],\n  \n           [[ 0.1915]],\n  \n           [[-0.1796]],\n  \n           ...,\n  \n           [[-0.1408]],\n  \n           [[ 0.1165]],\n  \n           [[ 0.0756]]]], requires_grad=True)),\n ('layer1.2.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer1.2.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer2.0.conv1.weight',\n  Parameter containing:\n  tensor([[[[ 0.0222]],\n  \n           [[ 0.2770]],\n  \n           [[ 0.1094]],\n  \n           ...,\n  \n           [[ 0.1907]],\n  \n           [[-0.0701]],\n  \n           [[-0.1194]]],\n  \n  \n          [[[-0.0121]],\n  \n           [[ 0.0138]],\n  \n           [[ 0.2060]],\n  \n           ...,\n  \n           [[-0.2114]],\n  \n           [[ 0.0572]],\n  \n           [[-0.0050]]],\n  \n  \n          [[[ 0.1011]],\n  \n           [[-0.1271]],\n  \n           [[-0.1459]],\n  \n           ...,\n  \n           [[-0.1627]],\n  \n           [[-0.1899]],\n  \n           [[ 0.1883]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0880]],\n  \n           [[ 0.1103]],\n  \n           [[-0.0390]],\n  \n           ...,\n  \n           [[-0.1149]],\n  \n           [[-0.0555]],\n  \n           [[-0.1955]]],\n  \n  \n          [[[ 0.1186]],\n  \n           [[ 0.1267]],\n  \n           [[ 0.0955]],\n  \n           ...,\n  \n           [[ 0.0880]],\n  \n           [[ 0.0887]],\n  \n           [[-0.2007]]],\n  \n  \n          [[[ 0.0411]],\n  \n           [[ 0.0024]],\n  \n           [[ 0.1000]],\n  \n           ...,\n  \n           [[-0.0377]],\n  \n           [[-0.0060]],\n  \n           [[-0.0530]]]], requires_grad=True)),\n ('layer2.0.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1.], requires_grad=True)),\n ('layer2.0.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.0.conv2.weight',\n  Parameter containing:\n  tensor([[[[ 0.0459, -0.0034, -0.0008],\n            [-0.0396, -0.0395,  0.0005],\n            [-0.0316, -0.0243, -0.0672]],\n  \n           [[ 0.0192, -0.0147,  0.0301],\n            [ 0.1093,  0.0164,  0.0399],\n            [ 0.0598,  0.0002,  0.0135]],\n  \n           [[ 0.0201,  0.0344,  0.0135],\n            [ 0.0144, -0.0456,  0.0735],\n            [-0.0339,  0.0573,  0.0477]],\n  \n           ...,\n  \n           [[ 0.0435, -0.0236, -0.0284],\n            [ 0.0211,  0.0043,  0.0005],\n            [-0.0228, -0.0126,  0.0513]],\n  \n           [[ 0.0647, -0.0524, -0.0100],\n            [ 0.0410,  0.0557,  0.0147],\n            [-0.0017,  0.0921,  0.0840]],\n  \n           [[ 0.0164,  0.1197, -0.0890],\n            [-0.0572, -0.0042, -0.0885],\n            [-0.0037,  0.0158, -0.0699]]],\n  \n  \n          [[[ 0.0265,  0.0215, -0.0373],\n            [ 0.0767,  0.0298,  0.0485],\n            [ 0.0372, -0.0689, -0.0186]],\n  \n           [[-0.0938,  0.0625,  0.0155],\n            [ 0.0498,  0.0062,  0.0048],\n            [-0.0567,  0.0217, -0.0220]],\n  \n           [[ 0.0081, -0.0235,  0.0215],\n            [-0.0356, -0.0169, -0.0391],\n            [ 0.0068,  0.0768,  0.0498]],\n  \n           ...,\n  \n           [[-0.0472,  0.0282,  0.0278],\n            [-0.0012, -0.0464,  0.0440],\n            [ 0.0007, -0.0512,  0.0297]],\n  \n           [[-0.0236,  0.0265,  0.0195],\n            [-0.0392, -0.0330,  0.0327],\n            [-0.0023,  0.0147,  0.0240]],\n  \n           [[ 0.0101,  0.0304, -0.0005],\n            [ 0.0485, -0.0303, -0.0300],\n            [-0.0166, -0.0461,  0.0785]]],\n  \n  \n          [[[-0.0284,  0.0963,  0.0404],\n            [-0.0235, -0.0824, -0.0377],\n            [-0.0293, -0.0681, -0.0122]],\n  \n           [[ 0.0284, -0.0063,  0.0279],\n            [ 0.0388, -0.0102, -0.0715],\n            [-0.0182, -0.0489, -0.0375]],\n  \n           [[-0.0382, -0.0039,  0.0391],\n            [ 0.0471, -0.0328, -0.0249],\n            [ 0.0348,  0.0374,  0.0814]],\n  \n           ...,\n  \n           [[-0.0059,  0.0133,  0.0117],\n            [ 0.0089,  0.0103, -0.0292],\n            [-0.0197, -0.0197, -0.0087]],\n  \n           [[ 0.0375, -0.0097, -0.0687],\n            [-0.0213,  0.0612,  0.0328],\n            [-0.0070, -0.0330, -0.0250]],\n  \n           [[-0.0470, -0.0106, -0.0020],\n            [-0.0389,  0.0292,  0.0454],\n            [ 0.0285,  0.0224,  0.0723]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0314, -0.0212, -0.0029],\n            [ 0.0197,  0.0302, -0.0148],\n            [ 0.0212, -0.0400,  0.0698]],\n  \n           [[ 0.0419,  0.0249,  0.0140],\n            [ 0.0248,  0.0332, -0.0442],\n            [ 0.0208,  0.0679, -0.0792]],\n  \n           [[ 0.0793, -0.0401, -0.0305],\n            [ 0.0447,  0.0551,  0.0005],\n            [-0.0061,  0.0462, -0.0102]],\n  \n           ...,\n  \n           [[-0.0075,  0.0076, -0.0007],\n            [-0.0213, -0.0259, -0.0432],\n            [ 0.0091, -0.0829, -0.0728]],\n  \n           [[ 0.0095, -0.0104,  0.0188],\n            [-0.0384,  0.0506, -0.0284],\n            [-0.0421,  0.0226, -0.0147]],\n  \n           [[-0.0179,  0.0427,  0.0326],\n            [-0.0329, -0.1014,  0.0071],\n            [-0.0402,  0.0005,  0.0096]]],\n  \n  \n          [[[-0.0064,  0.0191, -0.0647],\n            [-0.0410, -0.0393,  0.0005],\n            [-0.0262,  0.0384,  0.0445]],\n  \n           [[ 0.0269, -0.0841,  0.0206],\n            [ 0.0238, -0.0238, -0.0691],\n            [ 0.0919,  0.0441, -0.0491]],\n  \n           [[ 0.0330, -0.0181,  0.0003],\n            [-0.0159, -0.0187, -0.0862],\n            [ 0.0613,  0.0316,  0.0280]],\n  \n           ...,\n  \n           [[-0.0695,  0.0080,  0.0633],\n            [-0.0551, -0.0281, -0.0252],\n            [ 0.0252,  0.1216,  0.0565]],\n  \n           [[-0.0836, -0.0271, -0.0065],\n            [-0.0389,  0.0459, -0.0051],\n            [-0.0039,  0.0881, -0.0274]],\n  \n           [[-0.0313,  0.0383,  0.0551],\n            [-0.0350, -0.0206,  0.0670],\n            [-0.0388, -0.0122, -0.0261]]],\n  \n  \n          [[[ 0.0081, -0.0048,  0.0090],\n            [ 0.0304,  0.0288, -0.0414],\n            [ 0.1180,  0.0236, -0.0033]],\n  \n           [[-0.0795,  0.0286,  0.0035],\n            [-0.0153, -0.0263,  0.0210],\n            [ 0.0239,  0.0126, -0.0494]],\n  \n           [[ 0.0545,  0.0846, -0.0489],\n            [-0.0914,  0.0092,  0.0175],\n            [ 0.0086, -0.0188, -0.0282]],\n  \n           ...,\n  \n           [[-0.0406, -0.0584,  0.0323],\n            [ 0.0215,  0.0307,  0.0278],\n            [-0.0338, -0.0218, -0.0216]],\n  \n           [[-0.0008,  0.0049,  0.0406],\n            [-0.0054, -0.0384, -0.0151],\n            [ 0.0143,  0.0061,  0.0194]],\n  \n           [[-0.0364,  0.0218, -0.0532],\n            [ 0.0114, -0.0278,  0.0892],\n            [ 0.0109,  0.0297, -0.0421]]]], requires_grad=True)),\n ('layer2.0.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1.], requires_grad=True)),\n ('layer2.0.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.0.conv3.weight',\n  Parameter containing:\n  tensor([[[[-0.0322]],\n  \n           [[-0.0207]],\n  \n           [[ 0.0017]],\n  \n           ...,\n  \n           [[-0.0589]],\n  \n           [[-0.0791]],\n  \n           [[ 0.0407]]],\n  \n  \n          [[[-0.0468]],\n  \n           [[-0.1143]],\n  \n           [[-0.0532]],\n  \n           ...,\n  \n           [[ 0.0797]],\n  \n           [[ 0.0507]],\n  \n           [[ 0.0112]]],\n  \n  \n          [[[-0.0296]],\n  \n           [[ 0.1012]],\n  \n           [[-0.0236]],\n  \n           ...,\n  \n           [[ 0.0637]],\n  \n           [[ 0.0361]],\n  \n           [[-0.0102]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0948]],\n  \n           [[ 0.0369]],\n  \n           [[ 0.0102]],\n  \n           ...,\n  \n           [[ 0.0161]],\n  \n           [[-0.0379]],\n  \n           [[-0.0404]]],\n  \n  \n          [[[-0.0224]],\n  \n           [[-0.0473]],\n  \n           [[ 0.0513]],\n  \n           ...,\n  \n           [[-0.0477]],\n  \n           [[ 0.1113]],\n  \n           [[-0.0506]]],\n  \n  \n          [[[ 0.0291]],\n  \n           [[-0.0379]],\n  \n           [[-0.0768]],\n  \n           ...,\n  \n           [[ 0.0835]],\n  \n           [[ 0.0320]],\n  \n           [[ 0.0158]]]], requires_grad=True)),\n ('layer2.0.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.0.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.0.downsample.0.weight',\n  Parameter containing:\n  tensor([[[[ 0.0449]],\n  \n           [[-0.0063]],\n  \n           [[ 0.0182]],\n  \n           ...,\n  \n           [[-0.1253]],\n  \n           [[ 0.1364]],\n  \n           [[-0.0247]]],\n  \n  \n          [[[-0.0724]],\n  \n           [[ 0.0317]],\n  \n           [[ 0.0669]],\n  \n           ...,\n  \n           [[ 0.0403]],\n  \n           [[-0.0421]],\n  \n           [[-0.0267]]],\n  \n  \n          [[[-0.0560]],\n  \n           [[ 0.0098]],\n  \n           [[-0.0215]],\n  \n           ...,\n  \n           [[-0.1559]],\n  \n           [[-0.0150]],\n  \n           [[ 0.0436]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.1543]],\n  \n           [[ 0.0630]],\n  \n           [[ 0.0007]],\n  \n           ...,\n  \n           [[ 0.0226]],\n  \n           [[ 0.0238]],\n  \n           [[ 0.0149]]],\n  \n  \n          [[[ 0.0068]],\n  \n           [[-0.0364]],\n  \n           [[ 0.1170]],\n  \n           ...,\n  \n           [[-0.1112]],\n  \n           [[-0.0013]],\n  \n           [[-0.0032]]],\n  \n  \n          [[[-0.1066]],\n  \n           [[-0.0325]],\n  \n           [[-0.0539]],\n  \n           ...,\n  \n           [[ 0.0266]],\n  \n           [[ 0.0531]],\n  \n           [[-0.0026]]]], requires_grad=True)),\n ('layer2.0.downsample.1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer2.0.downsample.1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.1.conv1.weight',\n  Parameter containing:\n  tensor([[[[-0.1880]],\n  \n           [[-0.0538]],\n  \n           [[-0.2180]],\n  \n           ...,\n  \n           [[-0.1800]],\n  \n           [[-0.0178]],\n  \n           [[-0.1162]]],\n  \n  \n          [[[-0.1225]],\n  \n           [[ 0.2978]],\n  \n           [[ 0.1373]],\n  \n           ...,\n  \n           [[ 0.2007]],\n  \n           [[-0.0082]],\n  \n           [[-0.0192]]],\n  \n  \n          [[[-0.0777]],\n  \n           [[-0.0042]],\n  \n           [[-0.0100]],\n  \n           ...,\n  \n           [[-0.0537]],\n  \n           [[-0.2876]],\n  \n           [[-0.1438]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0339]],\n  \n           [[-0.1204]],\n  \n           [[ 0.0436]],\n  \n           ...,\n  \n           [[ 0.0552]],\n  \n           [[ 0.0879]],\n  \n           [[-0.0777]]],\n  \n  \n          [[[ 0.0013]],\n  \n           [[ 0.0958]],\n  \n           [[-0.0824]],\n  \n           ...,\n  \n           [[ 0.0171]],\n  \n           [[-0.2543]],\n  \n           [[-0.2877]]],\n  \n  \n          [[[ 0.0900]],\n  \n           [[-0.0857]],\n  \n           [[-0.0475]],\n  \n           ...,\n  \n           [[-0.0014]],\n  \n           [[ 0.0152]],\n  \n           [[-0.1122]]]], requires_grad=True)),\n ('layer2.1.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1.], requires_grad=True)),\n ('layer2.1.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.1.conv2.weight',\n  Parameter containing:\n  tensor([[[[ 0.0697, -0.0207,  0.0424],\n            [-0.0059,  0.0507, -0.0814],\n            [-0.0417, -0.0154,  0.0132]],\n  \n           [[-0.0258, -0.0465, -0.0457],\n            [ 0.0372, -0.0680, -0.0374],\n            [-0.0301, -0.0081, -0.0351]],\n  \n           [[-0.0193, -0.0110, -0.0002],\n            [-0.0390,  0.0357,  0.0522],\n            [-0.0389,  0.0299,  0.0367]],\n  \n           ...,\n  \n           [[-0.1265, -0.0454, -0.0114],\n            [ 0.0473,  0.0496, -0.0003],\n            [-0.0332,  0.0307, -0.0427]],\n  \n           [[-0.0225, -0.0058, -0.0834],\n            [ 0.0620,  0.0053, -0.0505],\n            [ 0.0710, -0.0108,  0.0254]],\n  \n           [[-0.0273, -0.0076, -0.0290],\n            [ 0.0241,  0.0014,  0.0442],\n            [-0.0790, -0.0174, -0.0151]]],\n  \n  \n          [[[-0.0019,  0.0191, -0.0472],\n            [ 0.0267,  0.0205,  0.0074],\n            [ 0.0248,  0.0130, -0.0469]],\n  \n           [[-0.0381, -0.0643,  0.0027],\n            [ 0.0258,  0.0631, -0.0204],\n            [-0.0046,  0.0454,  0.0085]],\n  \n           [[ 0.0311,  0.0633,  0.0191],\n            [-0.0072,  0.0212, -0.0019],\n            [-0.0466,  0.0196,  0.0286]],\n  \n           ...,\n  \n           [[-0.0078,  0.0295, -0.0582],\n            [ 0.0045,  0.0275, -0.0264],\n            [-0.0286,  0.0302, -0.0584]],\n  \n           [[ 0.0498,  0.0007, -0.0533],\n            [ 0.0945,  0.0666, -0.0047],\n            [ 0.1063,  0.0066,  0.0272]],\n  \n           [[ 0.0832, -0.0737,  0.0477],\n            [ 0.0309,  0.0125,  0.0192],\n            [ 0.0421, -0.0530,  0.0883]]],\n  \n  \n          [[[ 0.0365, -0.0164, -0.0016],\n            [ 0.0158, -0.0202,  0.0894],\n            [ 0.1136, -0.0163,  0.0893]],\n  \n           [[-0.0863, -0.0738, -0.0006],\n            [-0.0699,  0.0486,  0.0100],\n            [ 0.0218,  0.0386, -0.0201]],\n  \n           [[-0.0002, -0.0538, -0.0078],\n            [-0.0222,  0.0270,  0.0419],\n            [ 0.0388,  0.0020,  0.0384]],\n  \n           ...,\n  \n           [[ 0.0267, -0.0315, -0.0036],\n            [-0.0022, -0.0926, -0.0231],\n            [ 0.0106, -0.0011, -0.0009]],\n  \n           [[-0.0474, -0.0170,  0.0376],\n            [-0.0174,  0.0227, -0.0525],\n            [-0.0545,  0.0113,  0.0588]],\n  \n           [[ 0.0293, -0.0805,  0.0377],\n            [-0.0455, -0.0063,  0.0448],\n            [ 0.0345,  0.0225,  0.0062]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0295, -0.0362, -0.0464],\n            [ 0.0396, -0.0192,  0.0273],\n            [ 0.0292, -0.0852, -0.0028]],\n  \n           [[-0.0506, -0.0345, -0.0019],\n            [-0.0079, -0.1070,  0.0582],\n            [ 0.0787,  0.0382, -0.0125]],\n  \n           [[-0.0539,  0.0216, -0.0077],\n            [ 0.0373, -0.0785,  0.0263],\n            [ 0.0144, -0.0461, -0.0421]],\n  \n           ...,\n  \n           [[-0.0006, -0.0576, -0.0467],\n            [ 0.1064,  0.0119,  0.0112],\n            [ 0.0612,  0.0200,  0.0106]],\n  \n           [[-0.0252,  0.0513, -0.0062],\n            [ 0.0400, -0.0129, -0.0335],\n            [-0.0027, -0.0090, -0.0146]],\n  \n           [[-0.0305, -0.0492, -0.0475],\n            [ 0.0320,  0.0528, -0.0613],\n            [-0.0368, -0.0261,  0.0014]]],\n  \n  \n          [[[-0.0793, -0.0052,  0.0399],\n            [-0.0179, -0.0864, -0.0072],\n            [-0.0391,  0.0013, -0.0240]],\n  \n           [[-0.0257,  0.0225,  0.0207],\n            [-0.0646, -0.0004,  0.0172],\n            [-0.0188,  0.0042, -0.0151]],\n  \n           [[-0.0744,  0.0360, -0.0780],\n            [ 0.0219, -0.0330, -0.0226],\n            [ 0.0457, -0.0446,  0.0450]],\n  \n           ...,\n  \n           [[-0.0271,  0.0807, -0.0412],\n            [ 0.0576,  0.0096,  0.0036],\n            [ 0.0467,  0.0229, -0.0051]],\n  \n           [[ 0.0264, -0.0025,  0.0217],\n            [-0.0352,  0.0220,  0.0567],\n            [-0.0093, -0.0367,  0.0265]],\n  \n           [[ 0.0039,  0.0721,  0.0782],\n            [-0.0299, -0.0014, -0.0043],\n            [-0.0312,  0.0334,  0.0384]]],\n  \n  \n          [[[ 0.0109, -0.0010,  0.0662],\n            [ 0.0110, -0.0225,  0.0210],\n            [-0.0561,  0.0550,  0.0840]],\n  \n           [[-0.0490,  0.0154, -0.0172],\n            [-0.0187,  0.0792, -0.0032],\n            [-0.0927,  0.0010, -0.0085]],\n  \n           [[ 0.0193, -0.0105,  0.0865],\n            [ 0.0297,  0.0200,  0.0195],\n            [ 0.0324, -0.0830,  0.0709]],\n  \n           ...,\n  \n           [[ 0.1229, -0.0067,  0.0117],\n            [-0.0083, -0.0332,  0.0481],\n            [ 0.0130, -0.0207,  0.0043]],\n  \n           [[-0.0346, -0.0098, -0.0292],\n            [ 0.0550, -0.0351,  0.0193],\n            [-0.0082, -0.0281,  0.0923]],\n  \n           [[ 0.0032,  0.0028, -0.0271],\n            [-0.0214,  0.0660, -0.0070],\n            [-0.0941,  0.0405, -0.0653]]]], requires_grad=True)),\n ('layer2.1.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1.], requires_grad=True)),\n ('layer2.1.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.1.conv3.weight',\n  Parameter containing:\n  tensor([[[[ 0.0560]],\n  \n           [[-0.1670]],\n  \n           [[-0.0752]],\n  \n           ...,\n  \n           [[ 0.1228]],\n  \n           [[ 0.0277]],\n  \n           [[-0.0409]]],\n  \n  \n          [[[ 0.1189]],\n  \n           [[-0.0691]],\n  \n           [[ 0.1002]],\n  \n           ...,\n  \n           [[-0.0274]],\n  \n           [[ 0.0465]],\n  \n           [[ 0.0040]]],\n  \n  \n          [[[-0.0044]],\n  \n           [[ 0.1422]],\n  \n           [[ 0.1064]],\n  \n           ...,\n  \n           [[-0.0558]],\n  \n           [[ 0.0187]],\n  \n           [[-0.0482]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0750]],\n  \n           [[-0.0004]],\n  \n           [[ 0.0955]],\n  \n           ...,\n  \n           [[-0.0196]],\n  \n           [[ 0.1160]],\n  \n           [[ 0.0207]]],\n  \n  \n          [[[-0.0193]],\n  \n           [[ 0.0702]],\n  \n           [[ 0.0392]],\n  \n           ...,\n  \n           [[ 0.0019]],\n  \n           [[ 0.0543]],\n  \n           [[ 0.0146]]],\n  \n  \n          [[[ 0.0219]],\n  \n           [[ 0.1291]],\n  \n           [[ 0.0310]],\n  \n           ...,\n  \n           [[-0.0031]],\n  \n           [[-0.0277]],\n  \n           [[-0.0419]]]], requires_grad=True)),\n ('layer2.1.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.1.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.2.conv1.weight',\n  Parameter containing:\n  tensor([[[[ 0.0384]],\n  \n           [[ 0.1299]],\n  \n           [[ 0.0837]],\n  \n           ...,\n  \n           [[ 0.0894]],\n  \n           [[-0.0530]],\n  \n           [[ 0.0014]]],\n  \n  \n          [[[ 0.0026]],\n  \n           [[-0.1249]],\n  \n           [[-0.1714]],\n  \n           ...,\n  \n           [[-0.0187]],\n  \n           [[ 0.0268]],\n  \n           [[ 0.0785]]],\n  \n  \n          [[[ 0.0553]],\n  \n           [[-0.0484]],\n  \n           [[-0.0217]],\n  \n           ...,\n  \n           [[-0.0288]],\n  \n           [[ 0.0248]],\n  \n           [[-0.0360]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0118]],\n  \n           [[ 0.0336]],\n  \n           [[-0.0985]],\n  \n           ...,\n  \n           [[ 0.1601]],\n  \n           [[-0.0942]],\n  \n           [[-0.1221]]],\n  \n  \n          [[[-0.2731]],\n  \n           [[-0.0064]],\n  \n           [[ 0.1716]],\n  \n           ...,\n  \n           [[ 0.0496]],\n  \n           [[-0.0848]],\n  \n           [[-0.2240]]],\n  \n  \n          [[[ 0.0347]],\n  \n           [[-0.0766]],\n  \n           [[-0.1705]],\n  \n           ...,\n  \n           [[-0.1784]],\n  \n           [[ 0.2293]],\n  \n           [[ 0.0554]]]], requires_grad=True)),\n ('layer2.2.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1.], requires_grad=True)),\n ('layer2.2.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.2.conv2.weight',\n  Parameter containing:\n  tensor([[[[ 0.0283, -0.0086, -0.0251],\n            [ 0.0379,  0.0064, -0.0373],\n            [ 0.0057,  0.0254,  0.0114]],\n  \n           [[-0.0042,  0.0328,  0.0400],\n            [-0.0651, -0.0091, -0.0554],\n            [ 0.0600,  0.0040, -0.0140]],\n  \n           [[-0.0101,  0.0116, -0.0645],\n            [ 0.0504, -0.0051,  0.0361],\n            [-0.0397,  0.0335,  0.0076]],\n  \n           ...,\n  \n           [[-0.0962, -0.0075,  0.0111],\n            [ 0.0313, -0.1087,  0.0211],\n            [-0.0724,  0.0631, -0.0254]],\n  \n           [[-0.0311,  0.0312, -0.0380],\n            [-0.0102,  0.0101,  0.0551],\n            [ 0.0347,  0.0022,  0.0444]],\n  \n           [[ 0.0020,  0.0667,  0.0252],\n            [-0.0106, -0.0077, -0.0387],\n            [-0.0065,  0.0677, -0.0109]]],\n  \n  \n          [[[-0.0504,  0.0417, -0.0186],\n            [-0.0630,  0.0230,  0.0427],\n            [-0.0606,  0.0079, -0.0260]],\n  \n           [[ 0.1051, -0.0155,  0.0787],\n            [ 0.0057, -0.0031,  0.0214],\n            [-0.0173,  0.0480, -0.0346]],\n  \n           [[ 0.0376, -0.0142, -0.0480],\n            [ 0.0172, -0.0145, -0.0020],\n            [-0.0701, -0.0420, -0.0118]],\n  \n           ...,\n  \n           [[-0.0969, -0.0152, -0.0184],\n            [ 0.0343, -0.0159,  0.0073],\n            [ 0.0414,  0.0106,  0.0629]],\n  \n           [[-0.0123, -0.0211, -0.0688],\n            [ 0.0630, -0.0037, -0.0073],\n            [ 0.0103, -0.0098, -0.0576]],\n  \n           [[ 0.0090, -0.1192,  0.0524],\n            [-0.0284, -0.0032,  0.0520],\n            [-0.0287, -0.0519, -0.0449]]],\n  \n  \n          [[[ 0.0029, -0.0154, -0.0670],\n            [-0.0440,  0.0343, -0.0365],\n            [ 0.0011, -0.0143,  0.0193]],\n  \n           [[-0.0285, -0.0358, -0.0228],\n            [ 0.0019,  0.0353,  0.0193],\n            [ 0.0438, -0.0138, -0.0229]],\n  \n           [[-0.0375,  0.0184,  0.0193],\n            [ 0.0020,  0.0197, -0.0362],\n            [-0.0333,  0.0209, -0.0349]],\n  \n           ...,\n  \n           [[-0.0027, -0.0524, -0.0370],\n            [-0.0046,  0.0526, -0.0067],\n            [ 0.0699, -0.0392,  0.0017]],\n  \n           [[ 0.0294,  0.0721,  0.0472],\n            [ 0.1187,  0.0225,  0.0294],\n            [ 0.0083, -0.0029,  0.0037]],\n  \n           [[ 0.0320, -0.0643,  0.0195],\n            [ 0.0228, -0.0562,  0.0468],\n            [ 0.0328,  0.0985,  0.0232]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0738, -0.0710,  0.0809],\n            [-0.0434, -0.0367,  0.0057],\n            [-0.0123, -0.0829, -0.0476]],\n  \n           [[ 0.0291,  0.0154,  0.0456],\n            [ 0.0028,  0.0174, -0.0142],\n            [-0.0321, -0.0878, -0.0010]],\n  \n           [[-0.0370,  0.0556, -0.1078],\n            [ 0.0193, -0.0553, -0.0638],\n            [ 0.0655,  0.0499, -0.0274]],\n  \n           ...,\n  \n           [[ 0.0242,  0.0300,  0.0074],\n            [ 0.0274,  0.1339, -0.0017],\n            [-0.0769,  0.0013, -0.0217]],\n  \n           [[-0.0350, -0.0242, -0.0117],\n            [ 0.0550,  0.0094, -0.0628],\n            [-0.0487, -0.0520,  0.0693]],\n  \n           [[-0.0263, -0.0601,  0.0016],\n            [ 0.0156,  0.0512, -0.0267],\n            [-0.0325,  0.0731,  0.0257]]],\n  \n  \n          [[[ 0.0271,  0.0446,  0.1001],\n            [-0.0054, -0.0047, -0.0456],\n            [ 0.0371, -0.0517,  0.1276]],\n  \n           [[-0.0575,  0.0331, -0.0117],\n            [-0.0017, -0.0364, -0.0445],\n            [ 0.0034,  0.0205, -0.0380]],\n  \n           [[-0.0173,  0.0739, -0.0237],\n            [-0.0091,  0.0360, -0.0104],\n            [-0.0084,  0.0452,  0.0560]],\n  \n           ...,\n  \n           [[ 0.0254,  0.0016, -0.0377],\n            [ 0.0135,  0.0478, -0.0337],\n            [-0.0415,  0.0179,  0.0147]],\n  \n           [[ 0.0534,  0.0890,  0.0614],\n            [ 0.0066, -0.0319,  0.0080],\n            [ 0.0287, -0.1043,  0.0457]],\n  \n           [[-0.0073,  0.0077, -0.0046],\n            [-0.0071, -0.0004, -0.0222],\n            [ 0.0181,  0.0302, -0.0201]]],\n  \n  \n          [[[-0.0232,  0.0041, -0.0263],\n            [-0.0091, -0.0048, -0.0287],\n            [ 0.0334,  0.0379,  0.0182]],\n  \n           [[-0.0761, -0.0387,  0.0591],\n            [-0.0179, -0.0439,  0.0401],\n            [ 0.0172, -0.0011,  0.0724]],\n  \n           [[-0.0144, -0.0295, -0.0064],\n            [ 0.0074,  0.0074, -0.0048],\n            [ 0.0519, -0.0245, -0.0054]],\n  \n           ...,\n  \n           [[ 0.0578, -0.0086, -0.0474],\n            [ 0.1343,  0.0067,  0.0305],\n            [ 0.0289, -0.0083,  0.0291]],\n  \n           [[ 0.0026,  0.0274,  0.0551],\n            [-0.0163, -0.0592,  0.0110],\n            [-0.0158,  0.0292, -0.0385]],\n  \n           [[-0.0391, -0.0098,  0.0770],\n            [-0.0342, -0.0147,  0.0069],\n            [-0.0116,  0.0462,  0.0754]]]], requires_grad=True)),\n ('layer2.2.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1.], requires_grad=True)),\n ('layer2.2.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.2.conv3.weight',\n  Parameter containing:\n  tensor([[[[-0.0363]],\n  \n           [[ 0.0423]],\n  \n           [[ 0.0530]],\n  \n           ...,\n  \n           [[-0.1036]],\n  \n           [[ 0.0047]],\n  \n           [[-0.0443]]],\n  \n  \n          [[[ 0.1291]],\n  \n           [[-0.0241]],\n  \n           [[-0.0485]],\n  \n           ...,\n  \n           [[-0.0795]],\n  \n           [[ 0.0691]],\n  \n           [[ 0.0883]]],\n  \n  \n          [[[-0.0461]],\n  \n           [[ 0.0429]],\n  \n           [[-0.0257]],\n  \n           ...,\n  \n           [[-0.1092]],\n  \n           [[-0.0334]],\n  \n           [[ 0.0070]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0474]],\n  \n           [[-0.0008]],\n  \n           [[-0.0384]],\n  \n           ...,\n  \n           [[-0.0271]],\n  \n           [[ 0.0627]],\n  \n           [[-0.0937]]],\n  \n  \n          [[[-0.0280]],\n  \n           [[-0.0236]],\n  \n           [[ 0.0240]],\n  \n           ...,\n  \n           [[-0.0514]],\n  \n           [[-0.1152]],\n  \n           [[ 0.0925]]],\n  \n  \n          [[[-0.0773]],\n  \n           [[-0.0597]],\n  \n           [[-0.0563]],\n  \n           ...,\n  \n           [[-0.0086]],\n  \n           [[ 0.0817]],\n  \n           [[-0.0288]]]], requires_grad=True)),\n ('layer2.2.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.2.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.3.conv1.weight',\n  Parameter containing:\n  tensor([[[[ 0.1351]],\n  \n           [[-0.0165]],\n  \n           [[ 0.0364]],\n  \n           ...,\n  \n           [[ 0.1438]],\n  \n           [[ 0.1376]],\n  \n           [[ 0.0278]]],\n  \n  \n          [[[-0.0258]],\n  \n           [[-0.1271]],\n  \n           [[-0.0459]],\n  \n           ...,\n  \n           [[-0.0265]],\n  \n           [[-0.0015]],\n  \n           [[ 0.0292]]],\n  \n  \n          [[[ 0.0698]],\n  \n           [[ 0.0084]],\n  \n           [[ 0.0487]],\n  \n           ...,\n  \n           [[ 0.1092]],\n  \n           [[-0.0199]],\n  \n           [[-0.0309]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0391]],\n  \n           [[-0.0047]],\n  \n           [[ 0.0156]],\n  \n           ...,\n  \n           [[-0.1524]],\n  \n           [[-0.0501]],\n  \n           [[ 0.0380]]],\n  \n  \n          [[[ 0.0221]],\n  \n           [[ 0.1337]],\n  \n           [[ 0.0129]],\n  \n           ...,\n  \n           [[ 0.0332]],\n  \n           [[-0.0457]],\n  \n           [[ 0.0148]]],\n  \n  \n          [[[ 0.2395]],\n  \n           [[-0.0683]],\n  \n           [[-0.1396]],\n  \n           ...,\n  \n           [[-0.1805]],\n  \n           [[-0.1780]],\n  \n           [[ 0.1194]]]], requires_grad=True)),\n ('layer2.3.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1.], requires_grad=True)),\n ('layer2.3.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.3.conv2.weight',\n  Parameter containing:\n  tensor([[[[-0.0073,  0.0520,  0.0548],\n            [ 0.0040,  0.0403,  0.0211],\n            [-0.0578,  0.0497,  0.0180]],\n  \n           [[ 0.0189,  0.0223,  0.0626],\n            [ 0.0312,  0.0362,  0.0353],\n            [-0.0422, -0.0257, -0.0790]],\n  \n           [[-0.0327, -0.0641, -0.0637],\n            [-0.0511, -0.0103,  0.0735],\n            [ 0.0605,  0.0090, -0.0300]],\n  \n           ...,\n  \n           [[-0.0444,  0.0627,  0.0221],\n            [-0.0393,  0.0187,  0.0006],\n            [-0.0093,  0.0199, -0.0565]],\n  \n           [[-0.0045,  0.0320,  0.0038],\n            [ 0.0013, -0.0049, -0.0257],\n            [-0.0081,  0.0301,  0.0079]],\n  \n           [[ 0.0277, -0.0151,  0.0886],\n            [-0.0095,  0.0878,  0.0358],\n            [ 0.0305, -0.0608, -0.0559]]],\n  \n  \n          [[[ 0.0010,  0.0141, -0.0225],\n            [-0.0602,  0.0477, -0.0099],\n            [ 0.0131,  0.0205, -0.0037]],\n  \n           [[-0.0206, -0.0359, -0.0222],\n            [-0.0702, -0.0584,  0.0468],\n            [ 0.0012, -0.0727,  0.0528]],\n  \n           [[-0.0695,  0.0403, -0.0044],\n            [ 0.0875,  0.0189, -0.0036],\n            [ 0.0501,  0.0033, -0.0536]],\n  \n           ...,\n  \n           [[ 0.0326,  0.0822, -0.0254],\n            [ 0.0017,  0.0460, -0.0290],\n            [-0.0508,  0.0129, -0.0162]],\n  \n           [[-0.0657, -0.0051, -0.0715],\n            [-0.0045, -0.0377, -0.0350],\n            [-0.0570, -0.0265, -0.0569]],\n  \n           [[ 0.0020, -0.0142, -0.0388],\n            [ 0.0353, -0.0005, -0.0785],\n            [ 0.0823,  0.0712,  0.0533]]],\n  \n  \n          [[[ 0.0128, -0.0433,  0.0540],\n            [-0.0595,  0.0259, -0.0504],\n            [-0.0454, -0.0084, -0.0523]],\n  \n           [[-0.0467, -0.0532, -0.0509],\n            [-0.0811,  0.0432, -0.0183],\n            [ 0.0647,  0.0472, -0.0170]],\n  \n           [[ 0.0592, -0.0101,  0.0119],\n            [-0.0629,  0.0740, -0.0644],\n            [-0.0277, -0.0129,  0.0175]],\n  \n           ...,\n  \n           [[-0.0039,  0.0180,  0.0230],\n            [ 0.0606, -0.0631, -0.0547],\n            [ 0.0126,  0.0264, -0.0034]],\n  \n           [[ 0.0862, -0.0133,  0.0045],\n            [-0.0723, -0.0645,  0.0223],\n            [-0.0339,  0.0375,  0.0730]],\n  \n           [[ 0.0141,  0.0063,  0.0485],\n            [ 0.0263,  0.0067, -0.0128],\n            [-0.0127, -0.0401,  0.0715]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0315, -0.0157, -0.0132],\n            [ 0.0126, -0.0006, -0.0056],\n            [ 0.0032, -0.0068,  0.0433]],\n  \n           [[ 0.0019,  0.0576,  0.0565],\n            [ 0.0407, -0.0128,  0.0261],\n            [-0.0354,  0.0435, -0.0025]],\n  \n           [[ 0.0320, -0.0250, -0.0632],\n            [-0.0817, -0.0318, -0.0892],\n            [ 0.0732,  0.0240, -0.0044]],\n  \n           ...,\n  \n           [[ 0.0030, -0.0015,  0.0322],\n            [-0.0061,  0.0394, -0.0152],\n            [ 0.0132,  0.0190,  0.0827]],\n  \n           [[-0.0328,  0.0581, -0.1011],\n            [-0.0410,  0.0429,  0.0073],\n            [ 0.0265, -0.0546, -0.0213]],\n  \n           [[-0.0333,  0.0039, -0.0616],\n            [-0.0069, -0.0302,  0.1083],\n            [ 0.0170,  0.0722,  0.0251]]],\n  \n  \n          [[[-0.0253,  0.0277,  0.0622],\n            [-0.0139,  0.0313,  0.0236],\n            [ 0.0577, -0.0536, -0.0909]],\n  \n           [[-0.0404,  0.0462,  0.0667],\n            [-0.0343, -0.0044, -0.0513],\n            [ 0.0028, -0.0044,  0.0319]],\n  \n           [[-0.0820, -0.0195,  0.0255],\n            [ 0.0844,  0.0114, -0.0194],\n            [-0.0471, -0.0683, -0.0249]],\n  \n           ...,\n  \n           [[-0.0422,  0.0309,  0.0158],\n            [ 0.0096,  0.0806, -0.0519],\n            [-0.0339, -0.0548, -0.0273]],\n  \n           [[ 0.0053, -0.0002, -0.0454],\n            [ 0.0202,  0.0161, -0.0682],\n            [ 0.0171, -0.0238,  0.0222]],\n  \n           [[-0.0357, -0.0057,  0.0194],\n            [-0.0502,  0.0502, -0.0157],\n            [-0.0215,  0.0491, -0.0350]]],\n  \n  \n          [[[ 0.0306,  0.0423, -0.0321],\n            [ 0.0886,  0.0168,  0.0512],\n            [-0.0074,  0.0351,  0.0989]],\n  \n           [[-0.0089, -0.0338,  0.0168],\n            [-0.0595, -0.0074,  0.0128],\n            [-0.0228,  0.0381, -0.0301]],\n  \n           [[ 0.0143, -0.0234,  0.0008],\n            [-0.0219,  0.0162,  0.0315],\n            [ 0.0114, -0.0047,  0.0579]],\n  \n           ...,\n  \n           [[-0.0654, -0.0516, -0.0372],\n            [ 0.0358,  0.0456,  0.0205],\n            [-0.0522,  0.0409,  0.0256]],\n  \n           [[ 0.0075, -0.0188, -0.0007],\n            [-0.0089,  0.0377, -0.0398],\n            [-0.0165, -0.0047,  0.0945]],\n  \n           [[ 0.0698,  0.0023,  0.0263],\n            [-0.0633,  0.0509,  0.0175],\n            [ 0.0969, -0.0350, -0.0396]]]], requires_grad=True)),\n ('layer2.3.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1.], requires_grad=True)),\n ('layer2.3.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.3.conv3.weight',\n  Parameter containing:\n  tensor([[[[ 0.0659]],\n  \n           [[-0.0745]],\n  \n           [[-0.0380]],\n  \n           ...,\n  \n           [[-0.0046]],\n  \n           [[-0.0849]],\n  \n           [[-0.0789]]],\n  \n  \n          [[[ 0.0376]],\n  \n           [[-0.0662]],\n  \n           [[ 0.0721]],\n  \n           ...,\n  \n           [[-0.0350]],\n  \n           [[-0.0621]],\n  \n           [[-0.0565]]],\n  \n  \n          [[[-0.0551]],\n  \n           [[ 0.0231]],\n  \n           [[ 0.0595]],\n  \n           ...,\n  \n           [[ 0.0934]],\n  \n           [[ 0.0222]],\n  \n           [[ 0.0139]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0666]],\n  \n           [[-0.0092]],\n  \n           [[-0.0635]],\n  \n           ...,\n  \n           [[-0.0297]],\n  \n           [[ 0.0985]],\n  \n           [[-0.0780]]],\n  \n  \n          [[[ 0.0325]],\n  \n           [[ 0.1274]],\n  \n           [[ 0.0181]],\n  \n           ...,\n  \n           [[ 0.0114]],\n  \n           [[ 0.0021]],\n  \n           [[-0.0567]]],\n  \n  \n          [[[-0.0292]],\n  \n           [[ 0.0744]],\n  \n           [[-0.0323]],\n  \n           ...,\n  \n           [[-0.1086]],\n  \n           [[ 0.0314]],\n  \n           [[ 0.0771]]]], requires_grad=True)),\n ('layer2.3.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer2.3.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer3.0.conv1.weight',\n  Parameter containing:\n  tensor([[[[-0.0845]],\n  \n           [[ 0.0102]],\n  \n           [[ 0.0012]],\n  \n           ...,\n  \n           [[-0.1678]],\n  \n           [[-0.0670]],\n  \n           [[ 0.0408]]],\n  \n  \n          [[[-0.0171]],\n  \n           [[ 0.0401]],\n  \n           [[-0.0633]],\n  \n           ...,\n  \n           [[ 0.0030]],\n  \n           [[-0.0883]],\n  \n           [[-0.0824]]],\n  \n  \n          [[[-0.0342]],\n  \n           [[ 0.0686]],\n  \n           [[-0.0105]],\n  \n           ...,\n  \n           [[ 0.1833]],\n  \n           [[-0.0033]],\n  \n           [[ 0.1529]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0085]],\n  \n           [[ 0.0042]],\n  \n           [[ 0.0311]],\n  \n           ...,\n  \n           [[ 0.1536]],\n  \n           [[-0.0888]],\n  \n           [[-0.0247]]],\n  \n  \n          [[[-0.0876]],\n  \n           [[-0.1136]],\n  \n           [[ 0.1498]],\n  \n           ...,\n  \n           [[ 0.1551]],\n  \n           [[ 0.0631]],\n  \n           [[ 0.0742]]],\n  \n  \n          [[[ 0.1216]],\n  \n           [[-0.0451]],\n  \n           [[-0.1252]],\n  \n           ...,\n  \n           [[ 0.2079]],\n  \n           [[ 0.0335]],\n  \n           [[-0.0232]]]], requires_grad=True)),\n ('layer3.0.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.0.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.0.conv2.weight',\n  Parameter containing:\n  tensor([[[[ 0.0054, -0.0260, -0.0133],\n            [-0.0465, -0.0140, -0.0472],\n            [-0.0405,  0.0130,  0.0056]],\n  \n           [[ 0.0220,  0.0407, -0.0025],\n            [ 0.0508,  0.0268, -0.0190],\n            [ 0.0133,  0.0175,  0.0226]],\n  \n           [[ 0.0222, -0.0246,  0.0335],\n            [-0.0154, -0.0267, -0.0054],\n            [ 0.0276,  0.0136, -0.0168]],\n  \n           ...,\n  \n           [[-0.0054, -0.0477, -0.0600],\n            [-0.0005, -0.0187,  0.0213],\n            [ 0.0345,  0.0113, -0.0234]],\n  \n           [[ 0.0102,  0.0144, -0.0012],\n            [-0.0023,  0.0300, -0.0282],\n            [-0.0173, -0.0372,  0.0399]],\n  \n           [[ 0.0396, -0.0156,  0.0292],\n            [-0.0092, -0.0046,  0.0060],\n            [-0.0300, -0.0011, -0.0062]]],\n  \n  \n          [[[ 0.0107,  0.0487, -0.0230],\n            [ 0.0641,  0.0255,  0.0138],\n            [-0.0256, -0.0052, -0.0233]],\n  \n           [[ 0.0314, -0.0036, -0.0070],\n            [-0.0024, -0.0479,  0.0019],\n            [ 0.0074,  0.0068, -0.0608]],\n  \n           [[-0.0187, -0.0072,  0.0091],\n            [-0.0052,  0.0140, -0.0011],\n            [ 0.0081,  0.0171, -0.0402]],\n  \n           ...,\n  \n           [[ 0.0009, -0.0404,  0.0238],\n            [ 0.0034, -0.0412,  0.0418],\n            [ 0.0213,  0.0003,  0.0007]],\n  \n           [[-0.0129,  0.0403,  0.0498],\n            [-0.0462,  0.0423,  0.0315],\n            [-0.0382,  0.0109, -0.0138]],\n  \n           [[ 0.0412,  0.0036,  0.0171],\n            [ 0.0204, -0.0161,  0.0444],\n            [ 0.0504,  0.0005, -0.0174]]],\n  \n  \n          [[[-0.0145, -0.0327, -0.0086],\n            [-0.0458,  0.0330, -0.0358],\n            [-0.0094, -0.0360,  0.0425]],\n  \n           [[-0.0657,  0.0022,  0.0333],\n            [ 0.0245, -0.0385,  0.0436],\n            [ 0.0133, -0.0165, -0.0262]],\n  \n           [[ 0.0351, -0.0199,  0.0338],\n            [-0.0155, -0.0337,  0.0400],\n            [-0.0282, -0.0275, -0.0318]],\n  \n           ...,\n  \n           [[ 0.0278, -0.0112,  0.0329],\n            [ 0.0502, -0.0210,  0.0336],\n            [ 0.0124,  0.0124,  0.0531]],\n  \n           [[-0.0016,  0.0408,  0.0217],\n            [ 0.0054,  0.0088,  0.0254],\n            [ 0.0151, -0.0240,  0.0151]],\n  \n           [[ 0.0216, -0.0051,  0.0314],\n            [ 0.0353,  0.0032,  0.0223],\n            [-0.0267, -0.0099, -0.0234]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0210, -0.0026, -0.0172],\n            [-0.0343,  0.0718,  0.0502],\n            [-0.0075, -0.0264,  0.0600]],\n  \n           [[ 0.0394,  0.0233, -0.0046],\n            [ 0.0189,  0.0276, -0.0078],\n            [ 0.0299,  0.0374, -0.0133]],\n  \n           [[-0.0130,  0.0592, -0.0115],\n            [-0.0140,  0.0114, -0.0427],\n            [ 0.0023,  0.0143, -0.0040]],\n  \n           ...,\n  \n           [[ 0.0712, -0.0429,  0.0040],\n            [-0.0558, -0.0347,  0.0403],\n            [ 0.0137, -0.0193,  0.0369]],\n  \n           [[ 0.0283,  0.0017,  0.0157],\n            [ 0.0405,  0.0147,  0.0313],\n            [ 0.0185,  0.0040, -0.0031]],\n  \n           [[ 0.0103, -0.0265,  0.0189],\n            [-0.0207,  0.0019, -0.0503],\n            [ 0.0391,  0.0102, -0.0204]]],\n  \n  \n          [[[-0.0681,  0.0341,  0.0467],\n            [-0.0279, -0.0049,  0.0164],\n            [ 0.0105,  0.0570, -0.0042]],\n  \n           [[-0.0522, -0.0258, -0.0114],\n            [-0.0650,  0.0012, -0.0082],\n            [-0.0170,  0.0125,  0.0222]],\n  \n           [[-0.0010, -0.0199,  0.0739],\n            [ 0.0021, -0.0230,  0.0196],\n            [ 0.0085,  0.0183,  0.0127]],\n  \n           ...,\n  \n           [[-0.0054,  0.0165,  0.0731],\n            [ 0.0181, -0.0219,  0.0036],\n            [ 0.0333, -0.0221,  0.0187]],\n  \n           [[ 0.0410, -0.0073, -0.0103],\n            [-0.0206, -0.0148, -0.0488],\n            [-0.0231, -0.0477, -0.0302]],\n  \n           [[ 0.0091, -0.0188,  0.0103],\n            [-0.0295,  0.0108, -0.0184],\n            [-0.0366,  0.0315, -0.0020]]],\n  \n  \n          [[[-0.0297,  0.0054,  0.0063],\n            [-0.0064, -0.0519,  0.0216],\n            [-0.0371,  0.0482,  0.0331]],\n  \n           [[-0.0189, -0.0559, -0.0614],\n            [ 0.0205, -0.0413, -0.0164],\n            [-0.0263,  0.0695, -0.0307]],\n  \n           [[-0.0307, -0.0036,  0.0779],\n            [ 0.0348, -0.0336, -0.0329],\n            [ 0.0415,  0.0250, -0.0442]],\n  \n           ...,\n  \n           [[ 0.0043, -0.0138, -0.0417],\n            [-0.0288, -0.0027,  0.0415],\n            [ 0.0207,  0.0294, -0.0172]],\n  \n           [[ 0.0451,  0.0013, -0.0203],\n            [-0.0031, -0.0159,  0.0105],\n            [-0.0259,  0.0246, -0.0105]],\n  \n           [[-0.0049,  0.0738,  0.0085],\n            [ 0.0742, -0.0125,  0.0289],\n            [-0.0108,  0.0175,  0.0288]]]], requires_grad=True)),\n ('layer3.0.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.0.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.0.conv3.weight',\n  Parameter containing:\n  tensor([[[[-0.0686]],\n  \n           [[-0.0283]],\n  \n           [[ 0.0097]],\n  \n           ...,\n  \n           [[ 0.0343]],\n  \n           [[-0.0661]],\n  \n           [[-0.0526]]],\n  \n  \n          [[[-0.0218]],\n  \n           [[-0.0132]],\n  \n           [[-0.0259]],\n  \n           ...,\n  \n           [[ 0.0197]],\n  \n           [[ 0.0984]],\n  \n           [[-0.0743]]],\n  \n  \n          [[[-0.0377]],\n  \n           [[-0.0789]],\n  \n           [[ 0.0198]],\n  \n           ...,\n  \n           [[ 0.0414]],\n  \n           [[-0.0558]],\n  \n           [[-0.0468]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0251]],\n  \n           [[-0.0536]],\n  \n           [[ 0.0254]],\n  \n           ...,\n  \n           [[ 0.0134]],\n  \n           [[-0.0759]],\n  \n           [[-0.0472]]],\n  \n  \n          [[[ 0.1082]],\n  \n           [[-0.0331]],\n  \n           [[ 0.0160]],\n  \n           ...,\n  \n           [[ 0.0087]],\n  \n           [[-0.0622]],\n  \n           [[-0.0590]]],\n  \n  \n          [[[ 0.0456]],\n  \n           [[ 0.0003]],\n  \n           [[ 0.0144]],\n  \n           ...,\n  \n           [[-0.0242]],\n  \n           [[-0.0136]],\n  \n           [[ 0.0080]]]], requires_grad=True)),\n ('layer3.0.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.0.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.0.downsample.0.weight',\n  Parameter containing:\n  tensor([[[[-0.0418]],\n  \n           [[-0.0208]],\n  \n           [[-0.0206]],\n  \n           ...,\n  \n           [[ 0.0169]],\n  \n           [[ 0.0099]],\n  \n           [[-0.0694]]],\n  \n  \n          [[[ 0.0275]],\n  \n           [[-0.0150]],\n  \n           [[ 0.0252]],\n  \n           ...,\n  \n           [[-0.0361]],\n  \n           [[ 0.0399]],\n  \n           [[ 0.0184]]],\n  \n  \n          [[[-0.0298]],\n  \n           [[-0.0082]],\n  \n           [[-0.0231]],\n  \n           ...,\n  \n           [[-0.0983]],\n  \n           [[-0.0073]],\n  \n           [[-0.0450]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0242]],\n  \n           [[-0.0659]],\n  \n           [[ 0.0390]],\n  \n           ...,\n  \n           [[-0.0169]],\n  \n           [[ 0.0065]],\n  \n           [[-0.0043]]],\n  \n  \n          [[[-0.0112]],\n  \n           [[ 0.0470]],\n  \n           [[-0.1091]],\n  \n           ...,\n  \n           [[-0.0556]],\n  \n           [[ 0.0504]],\n  \n           [[ 0.0536]]],\n  \n  \n          [[[ 0.0255]],\n  \n           [[-0.0391]],\n  \n           [[-0.0058]],\n  \n           ...,\n  \n           [[ 0.0461]],\n  \n           [[ 0.0224]],\n  \n           [[-0.0092]]]], requires_grad=True)),\n ('layer3.0.downsample.1.weight',\n  Parameter containing:\n  tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)),\n ('layer3.0.downsample.1.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.1.conv1.weight',\n  Parameter containing:\n  tensor([[[[ 0.0826]],\n  \n           [[ 0.0453]],\n  \n           [[-0.0682]],\n  \n           ...,\n  \n           [[-0.0298]],\n  \n           [[-0.1112]],\n  \n           [[ 0.0268]]],\n  \n  \n          [[[-0.1231]],\n  \n           [[ 0.0271]],\n  \n           [[ 0.0273]],\n  \n           ...,\n  \n           [[-0.0921]],\n  \n           [[-0.0240]],\n  \n           [[-0.1141]]],\n  \n  \n          [[[ 0.0228]],\n  \n           [[-0.0395]],\n  \n           [[-0.0694]],\n  \n           ...,\n  \n           [[ 0.0231]],\n  \n           [[ 0.0933]],\n  \n           [[-0.1104]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.1635]],\n  \n           [[-0.0216]],\n  \n           [[ 0.0367]],\n  \n           ...,\n  \n           [[-0.0508]],\n  \n           [[-0.0599]],\n  \n           [[-0.1261]]],\n  \n  \n          [[[-0.0671]],\n  \n           [[-0.0695]],\n  \n           [[ 0.0251]],\n  \n           ...,\n  \n           [[ 0.0852]],\n  \n           [[-0.0611]],\n  \n           [[ 0.1764]]],\n  \n  \n          [[[ 0.0020]],\n  \n           [[ 0.0262]],\n  \n           [[-0.0006]],\n  \n           ...,\n  \n           [[ 0.1464]],\n  \n           [[ 0.1554]],\n  \n           [[-0.0512]]]], requires_grad=True)),\n ('layer3.1.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.1.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.1.conv2.weight',\n  Parameter containing:\n  tensor([[[[-1.5791e-02, -3.6361e-02,  2.9642e-02],\n            [ 4.9116e-02, -2.8563e-03,  1.4321e-02],\n            [-3.7315e-02, -6.8900e-03, -2.3771e-02]],\n  \n           [[ 1.1476e-02,  7.3049e-02, -7.8586e-03],\n            [-6.6519e-03,  2.4986e-03,  1.2868e-02],\n            [-1.7880e-02,  3.4451e-02,  4.1237e-04]],\n  \n           [[-4.0590e-02,  4.2409e-03, -4.9967e-02],\n            [-3.6613e-02,  3.5440e-02, -3.6613e-03],\n            [ 9.1528e-03, -2.2696e-02, -3.6998e-02]],\n  \n           ...,\n  \n           [[ 3.0958e-02, -3.3448e-02, -4.8575e-02],\n            [ 7.4950e-03, -3.9495e-02,  1.4678e-02],\n            [ 2.3532e-02, -2.0511e-02, -8.3222e-05]],\n  \n           [[-3.5708e-02,  2.7490e-02, -2.7328e-03],\n            [ 2.3998e-02, -4.3742e-02, -6.2410e-02],\n            [-4.2915e-02,  2.9370e-02, -1.4159e-02]],\n  \n           [[-4.6807e-02, -1.4428e-02,  6.3704e-04],\n            [ 3.2704e-03, -4.0851e-02,  5.5261e-03],\n            [ 1.1192e-03, -5.5855e-02,  2.0870e-02]]],\n  \n  \n          [[[-2.5382e-02,  1.8127e-02, -1.8301e-02],\n            [ 1.4928e-02, -4.1463e-02, -5.0125e-02],\n            [-3.9927e-02, -2.2737e-02,  3.5403e-02]],\n  \n           [[ 3.9119e-02,  3.0162e-02,  9.6497e-03],\n            [ 5.0243e-03, -1.9404e-02, -4.6817e-02],\n            [-1.9546e-02,  1.5879e-02,  4.2258e-02]],\n  \n           [[ 1.5648e-02, -2.4402e-02,  2.8837e-02],\n            [ 6.7391e-02, -1.7979e-03,  4.9645e-02],\n            [-2.4509e-02,  2.5692e-02, -1.3416e-02]],\n  \n           ...,\n  \n           [[-4.5968e-02, -3.2238e-02,  2.5851e-02],\n            [-2.7755e-02, -2.5351e-03,  1.6848e-02],\n            [-1.1841e-02, -2.2025e-02, -1.0079e-02]],\n  \n           [[ 2.9096e-02, -5.6588e-03, -3.7430e-02],\n            [ 2.3139e-02, -6.1421e-03,  5.3547e-03],\n            [-3.5642e-02, -1.6272e-02,  1.2680e-03]],\n  \n           [[-4.5059e-02,  3.0672e-02,  4.1950e-02],\n            [-3.3552e-03, -2.1627e-02, -1.8304e-02],\n            [ 9.7815e-03, -4.3939e-02, -2.8075e-02]]],\n  \n  \n          [[[-1.4995e-02, -2.7266e-03, -7.6974e-04],\n            [ 1.1558e-02,  3.4109e-02,  3.4308e-02],\n            [-2.4849e-03,  3.8366e-02,  2.5413e-02]],\n  \n           [[ 2.0227e-02,  2.9320e-02, -4.8138e-02],\n            [-3.1481e-02, -2.1374e-03,  6.6066e-03],\n            [ 6.3341e-03, -2.5838e-02, -4.0143e-02]],\n  \n           [[ 3.6880e-02, -1.2170e-02, -2.0159e-02],\n            [-1.0906e-02, -4.5360e-02,  2.1248e-02],\n            [-2.9780e-02,  4.0236e-02, -2.9158e-02]],\n  \n           ...,\n  \n           [[ 2.1072e-02,  6.1683e-02, -3.5969e-02],\n            [ 2.0307e-02, -5.4780e-05, -3.4308e-02],\n            [ 5.5149e-03,  4.9749e-02, -3.7749e-02]],\n  \n           [[-2.1034e-02,  3.4420e-02,  7.1253e-03],\n            [ 2.7329e-02,  1.1216e-02, -2.1896e-03],\n            [ 3.3569e-02,  2.1730e-02, -3.6669e-03]],\n  \n           [[ 1.6961e-03,  3.6973e-02, -7.8023e-03],\n            [-5.6584e-02,  3.8111e-02, -2.6280e-02],\n            [ 6.2532e-02,  3.7533e-02,  3.0196e-03]]],\n  \n  \n          ...,\n  \n  \n          [[[ 3.9798e-02, -2.5715e-02, -6.7929e-03],\n            [-2.5687e-02, -2.9894e-02, -2.9725e-02],\n            [-3.3797e-02,  1.5095e-02, -8.6130e-05]],\n  \n           [[-2.6542e-02,  7.6514e-02, -1.1748e-02],\n            [ 1.8552e-02, -2.5801e-02, -3.7192e-02],\n            [ 5.4452e-03, -2.2453e-02, -7.8372e-02]],\n  \n           [[ 6.4529e-02, -2.0579e-02,  4.7808e-02],\n            [ 6.1085e-02,  1.8428e-02, -1.3527e-02],\n            [-7.4832e-03, -2.5188e-02, -2.2736e-02]],\n  \n           ...,\n  \n           [[ 2.4293e-02, -1.3172e-02,  1.1246e-02],\n            [-4.3928e-02, -7.9775e-04,  1.1975e-02],\n            [-2.3470e-03,  1.9584e-02,  3.1799e-02]],\n  \n           [[-2.1593e-02,  3.2263e-02,  4.5072e-02],\n            [ 3.0098e-02, -4.5587e-02, -2.7600e-02],\n            [-1.0876e-02, -5.4578e-02,  2.8274e-02]],\n  \n           [[-3.1220e-02, -1.4101e-03,  5.1790e-03],\n            [-5.8895e-02,  1.5052e-02,  6.7957e-04],\n            [-4.6669e-02,  4.0789e-03,  8.4386e-03]]],\n  \n  \n          [[[ 2.6818e-02, -3.3355e-02,  7.3886e-04],\n            [ 6.5702e-02, -2.5676e-02,  1.6238e-02],\n            [-5.5273e-02, -6.3716e-03, -1.3914e-02]],\n  \n           [[ 4.4987e-02, -2.3497e-03, -2.8143e-02],\n            [ 2.5779e-02,  1.8882e-02, -6.3830e-04],\n            [-1.8685e-03,  3.3612e-02,  9.3307e-04]],\n  \n           [[-2.6406e-02,  3.5219e-03,  3.4773e-02],\n            [ 6.8522e-02,  8.3221e-03,  9.4333e-03],\n            [-4.6144e-02, -2.4965e-02, -4.1377e-03]],\n  \n           ...,\n  \n           [[-7.3474e-02, -5.5011e-02,  1.3724e-02],\n            [ 1.0802e-01,  2.9984e-02,  1.5933e-02],\n            [ 3.7104e-02, -1.3888e-02, -4.6466e-02]],\n  \n           [[ 6.0294e-03,  2.2019e-02, -4.4663e-04],\n            [ 1.8679e-02,  4.0528e-02, -1.1986e-02],\n            [ 6.0598e-02, -1.6437e-02, -3.0676e-03]],\n  \n           [[-5.7882e-02, -3.4872e-02, -2.6283e-02],\n            [-7.6126e-03,  2.7011e-03, -8.8197e-02],\n            [-4.4718e-02, -1.0295e-02, -2.9779e-02]]],\n  \n  \n          [[[-5.5724e-03,  2.0327e-02, -1.6305e-02],\n            [ 4.9112e-02,  2.2103e-02,  1.1756e-02],\n            [-1.9243e-03,  1.9718e-02,  2.5022e-02]],\n  \n           [[-4.6871e-03, -4.9651e-03, -4.6574e-02],\n            [-1.7458e-03, -2.3545e-02,  2.3278e-02],\n            [-1.1229e-02,  1.1368e-02,  4.2316e-02]],\n  \n           [[-2.7840e-02,  2.2139e-02, -1.5531e-02],\n            [-1.1877e-02,  3.0877e-02, -1.0118e-02],\n            [-1.2845e-02, -3.0826e-02,  1.8834e-02]],\n  \n           ...,\n  \n           [[ 1.8303e-02,  9.5048e-03, -3.5328e-03],\n            [-1.1956e-03,  1.1421e-03, -3.7856e-02],\n            [-1.9882e-02,  3.0404e-02, -3.0646e-02]],\n  \n           [[ 1.1251e-02,  4.9189e-02, -2.7675e-02],\n            [ 1.1346e-02,  3.6158e-02, -3.9763e-02],\n            [ 5.1055e-03,  3.1012e-03, -2.1895e-03]],\n  \n           [[ 1.7851e-03, -3.0441e-03, -4.1181e-02],\n            [-2.7162e-02,  1.8100e-02, -3.1112e-02],\n            [ 3.1842e-02,  3.9658e-02,  5.6591e-03]]]], requires_grad=True)),\n ('layer3.1.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.1.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.1.conv3.weight',\n  Parameter containing:\n  tensor([[[[-0.0280]],\n  \n           [[-0.0096]],\n  \n           [[-0.0810]],\n  \n           ...,\n  \n           [[-0.0509]],\n  \n           [[-0.0823]],\n  \n           [[-0.0097]]],\n  \n  \n          [[[-0.0252]],\n  \n           [[ 0.0282]],\n  \n           [[ 0.0197]],\n  \n           ...,\n  \n           [[ 0.0006]],\n  \n           [[-0.0064]],\n  \n           [[-0.0242]]],\n  \n  \n          [[[ 0.0306]],\n  \n           [[-0.0208]],\n  \n           [[ 0.0482]],\n  \n           ...,\n  \n           [[-0.0822]],\n  \n           [[ 0.0428]],\n  \n           [[-0.1039]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0301]],\n  \n           [[-0.0662]],\n  \n           [[ 0.0129]],\n  \n           ...,\n  \n           [[-0.0420]],\n  \n           [[-0.0155]],\n  \n           [[-0.0484]]],\n  \n  \n          [[[ 0.0248]],\n  \n           [[-0.0137]],\n  \n           [[ 0.0189]],\n  \n           ...,\n  \n           [[ 0.0184]],\n  \n           [[-0.0066]],\n  \n           [[ 0.0409]]],\n  \n  \n          [[[ 0.0030]],\n  \n           [[ 0.0016]],\n  \n           [[ 0.0436]],\n  \n           ...,\n  \n           [[-0.0076]],\n  \n           [[ 0.0018]],\n  \n           [[ 0.0217]]]], requires_grad=True)),\n ('layer3.1.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.1.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.2.conv1.weight',\n  Parameter containing:\n  tensor([[[[-0.0200]],\n  \n           [[-0.0951]],\n  \n           [[ 0.0186]],\n  \n           ...,\n  \n           [[ 0.0646]],\n  \n           [[-0.0765]],\n  \n           [[ 0.0731]]],\n  \n  \n          [[[ 0.0588]],\n  \n           [[ 0.0336]],\n  \n           [[ 0.0448]],\n  \n           ...,\n  \n           [[ 0.0682]],\n  \n           [[ 0.0963]],\n  \n           [[-0.0980]]],\n  \n  \n          [[[ 0.0687]],\n  \n           [[-0.0813]],\n  \n           [[ 0.0295]],\n  \n           ...,\n  \n           [[ 0.0353]],\n  \n           [[-0.0588]],\n  \n           [[ 0.0658]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0363]],\n  \n           [[-0.1020]],\n  \n           [[-0.0139]],\n  \n           ...,\n  \n           [[-0.0637]],\n  \n           [[ 0.0578]],\n  \n           [[ 0.0608]]],\n  \n  \n          [[[-0.0602]],\n  \n           [[-0.0510]],\n  \n           [[ 0.0497]],\n  \n           ...,\n  \n           [[-0.0003]],\n  \n           [[ 0.1302]],\n  \n           [[-0.0945]]],\n  \n  \n          [[[-0.1196]],\n  \n           [[-0.0980]],\n  \n           [[ 0.0038]],\n  \n           ...,\n  \n           [[-0.0067]],\n  \n           [[-0.0103]],\n  \n           [[-0.1775]]]], requires_grad=True)),\n ('layer3.2.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.2.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.2.conv2.weight',\n  Parameter containing:\n  tensor([[[[ 8.2847e-03,  1.6677e-02,  1.4527e-02],\n            [ 8.5915e-03,  3.2973e-02,  3.9604e-02],\n            [-2.2169e-02,  2.9133e-02, -1.6774e-02]],\n  \n           [[ 6.0294e-02, -7.1674e-04,  5.5939e-02],\n            [-1.6964e-02,  3.9841e-02,  3.5852e-02],\n            [-5.3230e-02,  1.4253e-03, -1.3850e-02]],\n  \n           [[ 1.2236e-02,  2.6060e-02,  1.7525e-02],\n            [-1.4547e-02,  3.9906e-02,  4.2274e-02],\n            [ 6.9792e-03,  1.1519e-02,  2.4149e-02]],\n  \n           ...,\n  \n           [[-3.5614e-02,  3.8915e-02,  4.9838e-02],\n            [-2.1771e-03,  1.3593e-03,  1.4572e-02],\n            [ 7.1951e-03, -2.3047e-03, -3.0261e-02]],\n  \n           [[-9.3531e-02,  1.9502e-02,  2.6046e-03],\n            [-2.3474e-02, -1.7514e-02,  5.2145e-02],\n            [-9.6907e-03,  8.5393e-03,  3.5762e-03]],\n  \n           [[-5.7824e-02,  2.0054e-02,  7.4115e-03],\n            [ 2.0059e-02, -9.7816e-03,  2.3132e-02],\n            [ 8.6665e-03, -3.3068e-02,  3.7214e-02]]],\n  \n  \n          [[[-3.9028e-03, -3.0342e-02, -1.8745e-02],\n            [-4.9580e-02,  2.6396e-02, -4.0269e-02],\n            [ 6.8253e-03,  1.0314e-02,  1.0629e-02]],\n  \n           [[ 4.6180e-03,  8.9664e-03,  4.6218e-02],\n            [-1.8667e-02, -2.8731e-03, -2.9544e-02],\n            [ 2.3982e-03,  1.7261e-02,  2.0615e-02]],\n  \n           [[ 1.8091e-02, -5.8626e-02,  4.2836e-02],\n            [ 2.5931e-02, -2.0920e-02, -1.0538e-02],\n            [ 6.1750e-02,  1.1744e-02,  1.6510e-02]],\n  \n           ...,\n  \n           [[ 5.6912e-02, -3.3634e-02,  1.4816e-02],\n            [-1.0799e-02,  2.3500e-02, -6.9046e-03],\n            [ 1.4270e-02,  1.3251e-02, -3.0396e-02]],\n  \n           [[-1.5597e-02, -3.4988e-02, -9.9426e-03],\n            [ 1.0084e-02,  8.3798e-03,  3.8586e-02],\n            [-1.0141e-02,  3.6370e-02, -2.6960e-02]],\n  \n           [[ 3.6252e-02, -6.7878e-03, -9.2819e-03],\n            [-1.2435e-02, -1.6956e-02, -2.3464e-02],\n            [-2.8971e-02, -1.4484e-05,  3.7100e-02]]],\n  \n  \n          [[[-2.5472e-02, -2.5646e-02, -2.1588e-02],\n            [-1.3794e-02,  1.1192e-02,  6.3307e-02],\n            [-5.5175e-02,  2.9825e-02,  2.8346e-02]],\n  \n           [[ 4.7922e-03,  3.2381e-02, -1.5673e-03],\n            [ 4.4095e-02,  3.4130e-02, -1.7086e-02],\n            [-2.2049e-02, -5.9746e-02,  3.4687e-03]],\n  \n           [[ 6.0871e-02, -6.0833e-04, -2.8201e-02],\n            [ 4.7908e-03, -5.3538e-03,  1.3398e-02],\n            [ 3.1056e-02,  2.8855e-02, -3.5163e-02]],\n  \n           ...,\n  \n           [[ 1.0334e-02, -3.4671e-03, -1.7009e-03],\n            [-2.0201e-02, -1.5973e-02,  2.5389e-02],\n            [-1.7292e-02,  2.6464e-02, -2.5394e-02]],\n  \n           [[ 2.3953e-02,  6.2547e-03, -1.8064e-02],\n            [-3.7268e-02, -5.1822e-03, -2.7933e-02],\n            [ 1.8027e-02, -7.2723e-02,  4.3008e-02]],\n  \n           [[-4.5355e-02, -2.9464e-02, -8.1841e-02],\n            [ 6.5791e-02, -2.8992e-04,  3.5616e-03],\n            [ 9.5372e-03, -1.0692e-02,  1.9527e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 7.9685e-02,  2.1810e-02, -1.2611e-02],\n            [ 1.5779e-02, -1.1464e-02,  1.2943e-02],\n            [-1.7995e-02,  6.5461e-04,  1.9755e-02]],\n  \n           [[-9.7413e-03, -3.7014e-02, -2.2684e-02],\n            [-8.1871e-04,  2.7409e-02, -5.0674e-02],\n            [-4.7077e-02,  2.6810e-02,  3.0313e-02]],\n  \n           [[ 7.2511e-04,  5.1719e-02,  1.1566e-02],\n            [ 1.9188e-02,  1.9353e-02,  2.7462e-02],\n            [-3.1201e-02, -5.0842e-02, -1.8247e-02]],\n  \n           ...,\n  \n           [[-1.1175e-02, -9.0107e-03, -5.1530e-03],\n            [-2.3717e-03,  1.6803e-02, -6.9798e-03],\n            [-6.2794e-02, -2.5733e-03,  8.0564e-03]],\n  \n           [[ 1.8738e-02, -1.4322e-02, -3.4469e-03],\n            [-1.6180e-02, -2.6044e-02, -1.2323e-03],\n            [-2.8622e-02, -7.4329e-03,  3.1876e-02]],\n  \n           [[ 5.2834e-03, -5.2158e-02, -3.3926e-02],\n            [-4.6468e-02, -1.6522e-02, -3.6937e-02],\n            [ 2.2631e-02,  1.9870e-02,  4.0593e-02]]],\n  \n  \n          [[[ 1.1668e-02, -2.8983e-02,  2.4513e-02],\n            [ 2.0327e-02, -7.9667e-03, -8.2108e-04],\n            [-3.1045e-02, -5.6449e-02,  3.2075e-02]],\n  \n           [[ 1.0411e-02,  3.6846e-02,  7.3951e-03],\n            [ 2.1491e-02,  6.4708e-02, -2.9158e-02],\n            [-6.7655e-02, -2.1640e-02,  5.0303e-02]],\n  \n           [[ 4.8983e-02, -1.9326e-02,  1.9924e-02],\n            [-6.5550e-03, -1.5009e-03, -4.5588e-02],\n            [ 2.6072e-02,  6.3135e-03, -5.3702e-02]],\n  \n           ...,\n  \n           [[-3.1809e-02,  1.3491e-02, -3.8330e-03],\n            [-1.1270e-02, -1.9763e-03, -2.2435e-02],\n            [-2.6880e-02,  2.4187e-03,  5.1126e-02]],\n  \n           [[ 9.4548e-03, -3.6026e-02,  1.1095e-02],\n            [-1.4757e-02, -4.6296e-02,  2.3856e-02],\n            [ 4.1071e-02,  1.4884e-03, -1.1488e-02]],\n  \n           [[ 9.8374e-03,  3.3745e-02,  1.5055e-02],\n            [ 1.5424e-03,  2.4750e-02, -2.0943e-03],\n            [-2.7401e-02,  2.8452e-02, -6.6456e-03]]],\n  \n  \n          [[[-2.9554e-02,  3.1573e-02, -2.3323e-02],\n            [ 3.2860e-02, -1.2010e-02,  1.3053e-02],\n            [-1.4563e-02,  9.4927e-02,  8.4784e-03]],\n  \n           [[-3.6402e-02, -3.7391e-03, -3.7304e-03],\n            [ 2.9706e-02, -2.2957e-02, -4.3798e-02],\n            [-2.5277e-02, -1.9728e-02, -2.7168e-02]],\n  \n           [[-4.5600e-02, -2.5123e-02, -8.5362e-03],\n            [-1.6403e-02,  3.8167e-02, -2.2246e-02],\n            [-3.2903e-02,  1.8067e-02, -1.4901e-02]],\n  \n           ...,\n  \n           [[ 2.8575e-02, -4.0251e-02,  5.1598e-02],\n            [ 1.3479e-02, -4.8410e-03, -2.1953e-02],\n            [-2.7338e-03,  2.0802e-02, -5.1434e-03]],\n  \n           [[ 5.2211e-02,  4.5468e-03, -9.3347e-03],\n            [-2.8731e-02,  2.1231e-02, -8.6606e-03],\n            [-1.4195e-03, -6.0878e-03, -6.8584e-03]],\n  \n           [[-3.1122e-02, -2.3617e-03,  8.3305e-03],\n            [ 3.9178e-02,  9.4839e-03,  4.7496e-02],\n            [ 1.4776e-02, -5.1577e-02,  1.2084e-02]]]], requires_grad=True)),\n ('layer3.2.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.2.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.2.conv3.weight',\n  Parameter containing:\n  tensor([[[[-0.0144]],\n  \n           [[-0.0766]],\n  \n           [[-0.0167]],\n  \n           ...,\n  \n           [[ 0.0017]],\n  \n           [[ 0.0279]],\n  \n           [[-0.0447]]],\n  \n  \n          [[[ 0.0170]],\n  \n           [[ 0.0075]],\n  \n           [[-0.0095]],\n  \n           ...,\n  \n           [[-0.0002]],\n  \n           [[ 0.0174]],\n  \n           [[ 0.0018]]],\n  \n  \n          [[[-0.0367]],\n  \n           [[-0.0542]],\n  \n           [[ 0.0189]],\n  \n           ...,\n  \n           [[ 0.0402]],\n  \n           [[ 0.0316]],\n  \n           [[ 0.0284]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0132]],\n  \n           [[ 0.0141]],\n  \n           [[ 0.0675]],\n  \n           ...,\n  \n           [[-0.0287]],\n  \n           [[ 0.0613]],\n  \n           [[ 0.0924]]],\n  \n  \n          [[[-0.1017]],\n  \n           [[ 0.0018]],\n  \n           [[-0.0132]],\n  \n           ...,\n  \n           [[ 0.0532]],\n  \n           [[-0.1004]],\n  \n           [[ 0.0086]]],\n  \n  \n          [[[ 0.0274]],\n  \n           [[ 0.0686]],\n  \n           [[ 0.0411]],\n  \n           ...,\n  \n           [[-0.0363]],\n  \n           [[-0.0396]],\n  \n           [[-0.0070]]]], requires_grad=True)),\n ('layer3.2.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.2.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.3.conv1.weight',\n  Parameter containing:\n  tensor([[[[ 0.0488]],\n  \n           [[-0.0243]],\n  \n           [[-0.0953]],\n  \n           ...,\n  \n           [[-0.0322]],\n  \n           [[-0.0006]],\n  \n           [[-0.0957]]],\n  \n  \n          [[[-0.0568]],\n  \n           [[ 0.0433]],\n  \n           [[-0.0657]],\n  \n           ...,\n  \n           [[-0.0415]],\n  \n           [[-0.0010]],\n  \n           [[ 0.0810]]],\n  \n  \n          [[[ 0.1400]],\n  \n           [[ 0.0629]],\n  \n           [[ 0.1938]],\n  \n           ...,\n  \n           [[ 0.1159]],\n  \n           [[ 0.0375]],\n  \n           [[ 0.0372]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.1325]],\n  \n           [[ 0.0417]],\n  \n           [[-0.0262]],\n  \n           ...,\n  \n           [[-0.0167]],\n  \n           [[ 0.0747]],\n  \n           [[ 0.1331]]],\n  \n  \n          [[[-0.0194]],\n  \n           [[-0.0361]],\n  \n           [[-0.0088]],\n  \n           ...,\n  \n           [[ 0.0381]],\n  \n           [[ 0.0449]],\n  \n           [[ 0.0604]]],\n  \n  \n          [[[-0.0150]],\n  \n           [[-0.1971]],\n  \n           [[-0.0012]],\n  \n           ...,\n  \n           [[-0.1198]],\n  \n           [[-0.1358]],\n  \n           [[-0.0544]]]], requires_grad=True)),\n ('layer3.3.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.3.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.3.conv2.weight',\n  Parameter containing:\n  tensor([[[[-1.7035e-02,  7.1942e-03, -2.3327e-02],\n            [ 3.0909e-02, -1.5759e-02, -2.3324e-02],\n            [-4.2068e-02,  2.5518e-02,  1.0073e-03]],\n  \n           [[-3.2041e-02, -3.4009e-02,  2.2773e-03],\n            [-1.1814e-03,  2.7857e-02, -2.1206e-03],\n            [ 1.4320e-02, -1.8606e-02,  1.5336e-02]],\n  \n           [[-3.9873e-02, -9.1911e-02,  1.8274e-02],\n            [ 1.7043e-02, -1.7755e-02,  1.6855e-02],\n            [-2.9968e-02,  2.0778e-03, -1.6478e-02]],\n  \n           ...,\n  \n           [[ 6.2308e-03,  4.4974e-03,  1.6540e-03],\n            [-3.1207e-04, -2.8205e-02, -1.2350e-02],\n            [-4.7715e-02,  4.8100e-02,  2.9551e-02]],\n  \n           [[ 4.1786e-02,  2.6159e-02,  3.3757e-05],\n            [-1.9367e-03, -5.1768e-02, -2.2192e-02],\n            [-4.4027e-02,  4.4060e-03,  5.0508e-03]],\n  \n           [[ 8.0665e-03,  8.1101e-03, -2.4044e-02],\n            [ 9.0603e-02,  2.6511e-02,  1.3938e-02],\n            [ 6.0320e-02,  3.4512e-02,  1.7849e-02]]],\n  \n  \n          [[[-4.5262e-03, -2.4455e-02, -4.6461e-03],\n            [ 8.6930e-03, -3.7523e-02, -5.4648e-02],\n            [-1.1002e-02,  1.0499e-02,  5.3224e-03]],\n  \n           [[ 2.7643e-02,  6.7480e-02,  2.0328e-02],\n            [-2.5531e-02,  5.3490e-03, -1.1315e-03],\n            [ 3.4711e-02,  5.3466e-03,  4.2625e-02]],\n  \n           [[-5.5408e-02,  3.7122e-02,  3.8803e-03],\n            [-7.6885e-03,  1.8007e-02,  6.5030e-02],\n            [ 2.5908e-02,  1.4660e-02,  1.3991e-02]],\n  \n           ...,\n  \n           [[ 4.5448e-02, -3.2369e-02,  1.1668e-02],\n            [ 3.6233e-02,  1.4623e-02,  7.5676e-03],\n            [-2.7893e-02, -1.9312e-02,  3.4560e-02]],\n  \n           [[ 1.9561e-02,  8.9970e-03,  3.8577e-02],\n            [ 1.3902e-02,  8.7593e-03,  2.5725e-02],\n            [-5.0444e-02, -1.2465e-02, -1.6791e-02]],\n  \n           [[-3.4285e-02, -3.7859e-02, -7.1149e-03],\n            [-2.8030e-02, -6.5988e-03,  1.1789e-02],\n            [ 1.9973e-02,  1.3128e-02,  3.1544e-03]]],\n  \n  \n          [[[ 2.5615e-02,  1.0154e-02, -2.3357e-02],\n            [ 1.2082e-02, -1.0405e-02,  9.1418e-02],\n            [ 4.5012e-02,  3.9455e-03,  4.8355e-03]],\n  \n           [[ 6.4426e-03,  3.4305e-02, -5.1241e-02],\n            [-1.9072e-02,  5.6626e-02, -9.7465e-03],\n            [-1.6924e-02, -1.7575e-03, -5.0027e-02]],\n  \n           [[ 3.5558e-02, -9.8706e-02, -2.6702e-03],\n            [-3.5161e-02,  4.7312e-02,  6.4126e-04],\n            [-1.0879e-02, -2.1216e-03, -4.9678e-02]],\n  \n           ...,\n  \n           [[-8.8431e-03,  2.0539e-02,  4.1600e-02],\n            [-3.6894e-02,  1.4730e-02,  7.6153e-02],\n            [-1.3843e-02, -3.8007e-02, -5.1604e-03]],\n  \n           [[ 5.7732e-02,  1.3151e-02,  1.9315e-02],\n            [ 4.6730e-02, -4.4377e-04, -1.0353e-03],\n            [-2.1472e-02,  1.0628e-02, -9.6998e-03]],\n  \n           [[-3.5218e-02, -1.4437e-02, -3.3259e-03],\n            [-3.1088e-02,  3.3845e-02,  3.7010e-02],\n            [ 1.1174e-02,  2.2558e-02,  2.0674e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 2.5338e-02, -2.6722e-02, -8.1347e-03],\n            [ 1.2231e-03,  2.1782e-02, -4.9276e-02],\n            [ 1.1718e-02, -3.4569e-02, -2.3134e-02]],\n  \n           [[ 6.3636e-03, -4.6785e-02, -1.6028e-03],\n            [-2.7843e-02, -6.6190e-02, -3.0660e-02],\n            [ 5.2333e-02,  4.4412e-02,  5.4673e-02]],\n  \n           [[ 4.3816e-02,  1.5387e-02, -1.3446e-02],\n            [-2.1582e-04, -6.0563e-03,  2.0600e-02],\n            [ 2.2070e-02,  1.6775e-02, -6.0009e-03]],\n  \n           ...,\n  \n           [[-7.2384e-03,  1.2649e-03, -6.6799e-04],\n            [-4.5331e-02, -1.3192e-03, -2.7107e-02],\n            [ 3.7801e-02, -3.7307e-02, -3.4440e-02]],\n  \n           [[-6.8024e-03,  5.5347e-03, -1.6108e-02],\n            [-1.6065e-02,  1.2956e-02,  3.1415e-02],\n            [-3.7788e-02, -3.1853e-03,  8.8218e-03]],\n  \n           [[ 3.5222e-02, -1.8286e-02, -5.8837e-02],\n            [ 3.2023e-03,  4.7753e-02, -1.7863e-02],\n            [-2.6978e-03, -2.7018e-02,  1.1348e-02]]],\n  \n  \n          [[[ 9.4646e-03, -4.9642e-02, -1.4083e-02],\n            [-1.0439e-02,  3.7155e-03, -1.7208e-02],\n            [ 4.4904e-02,  1.2062e-02, -1.2025e-03]],\n  \n           [[-2.1972e-04, -1.9514e-02,  1.8675e-03],\n            [ 3.9539e-02, -1.5473e-02, -2.4104e-02],\n            [ 3.5305e-02, -2.2602e-02, -6.8348e-02]],\n  \n           [[ 1.4678e-02, -3.6212e-02, -4.3677e-02],\n            [ 2.9750e-02,  1.2638e-02,  2.7216e-02],\n            [ 5.3793e-03,  1.8831e-02, -3.7095e-02]],\n  \n           ...,\n  \n           [[ 5.4385e-02,  3.5083e-02, -1.3416e-02],\n            [-3.0418e-02, -2.1716e-02, -2.4747e-02],\n            [-7.4251e-03,  5.8676e-03,  9.7599e-03]],\n  \n           [[-3.6018e-02,  2.9550e-02, -4.3119e-02],\n            [ 8.5634e-03, -9.1419e-03, -7.5138e-03],\n            [ 1.6382e-02,  1.7247e-02, -3.1261e-02]],\n  \n           [[ 3.4712e-02, -3.8684e-03, -9.7017e-03],\n            [ 5.6648e-03,  1.0028e-02,  5.2526e-03],\n            [-5.8685e-02, -8.2147e-03,  1.2674e-02]]],\n  \n  \n          [[[-5.3166e-02, -4.8768e-02, -1.3591e-02],\n            [-2.5855e-02,  8.6810e-04, -2.0275e-02],\n            [ 1.8711e-02,  1.5382e-02, -5.1489e-02]],\n  \n           [[-1.1853e-02,  3.2730e-03, -1.4702e-02],\n            [-7.4826e-03, -7.2251e-03, -1.0603e-02],\n            [ 3.5157e-02, -4.1526e-02,  1.2648e-02]],\n  \n           [[-2.7885e-03, -4.7841e-04,  1.1337e-02],\n            [-1.6938e-03,  1.7197e-02,  5.8655e-03],\n            [-5.3049e-02,  4.3308e-02, -1.4599e-02]],\n  \n           ...,\n  \n           [[ 5.3726e-02, -4.4607e-02,  2.9851e-03],\n            [-4.1750e-03,  2.4736e-02, -4.1434e-02],\n            [-1.0675e-02, -3.2776e-02,  1.7287e-02]],\n  \n           [[-1.0569e-02,  1.8271e-02, -4.7659e-02],\n            [ 2.5124e-02, -7.7333e-03,  1.0467e-02],\n            [ 3.2057e-02,  1.7390e-02, -4.6390e-02]],\n  \n           [[-1.0176e-02, -2.4395e-03,  1.9234e-02],\n            [ 3.9036e-02, -3.5639e-02, -7.3514e-02],\n            [-1.5681e-03, -6.2660e-02, -1.1289e-02]]]], requires_grad=True)),\n ('layer3.3.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.3.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.3.conv3.weight',\n  Parameter containing:\n  tensor([[[[-0.0740]],\n  \n           [[ 0.0006]],\n  \n           [[-0.0194]],\n  \n           ...,\n  \n           [[-0.0347]],\n  \n           [[ 0.0174]],\n  \n           [[-0.0092]]],\n  \n  \n          [[[-0.0247]],\n  \n           [[-0.0448]],\n  \n           [[ 0.0328]],\n  \n           ...,\n  \n           [[-0.0049]],\n  \n           [[ 0.0437]],\n  \n           [[-0.0303]]],\n  \n  \n          [[[ 0.0682]],\n  \n           [[ 0.0395]],\n  \n           [[ 0.0420]],\n  \n           ...,\n  \n           [[-0.1273]],\n  \n           [[-0.0295]],\n  \n           [[-0.0332]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0200]],\n  \n           [[ 0.0546]],\n  \n           [[-0.0362]],\n  \n           ...,\n  \n           [[ 0.0198]],\n  \n           [[-0.0488]],\n  \n           [[-0.0178]]],\n  \n  \n          [[[-0.0435]],\n  \n           [[ 0.0261]],\n  \n           [[ 0.0481]],\n  \n           ...,\n  \n           [[-0.0055]],\n  \n           [[-0.0137]],\n  \n           [[ 0.0312]]],\n  \n  \n          [[[ 0.0020]],\n  \n           [[ 0.0494]],\n  \n           [[ 0.0359]],\n  \n           ...,\n  \n           [[ 0.0071]],\n  \n           [[ 0.0292]],\n  \n           [[-0.0646]]]], requires_grad=True)),\n ('layer3.3.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.3.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.4.conv1.weight',\n  Parameter containing:\n  tensor([[[[ 0.0251]],\n  \n           [[-0.0959]],\n  \n           [[-0.0428]],\n  \n           ...,\n  \n           [[ 0.0545]],\n  \n           [[ 0.0474]],\n  \n           [[-0.0231]]],\n  \n  \n          [[[ 0.0412]],\n  \n           [[-0.1001]],\n  \n           [[ 0.0165]],\n  \n           ...,\n  \n           [[ 0.1040]],\n  \n           [[-0.0598]],\n  \n           [[-0.0018]]],\n  \n  \n          [[[-0.0410]],\n  \n           [[ 0.0863]],\n  \n           [[-0.0144]],\n  \n           ...,\n  \n           [[ 0.0507]],\n  \n           [[-0.0472]],\n  \n           [[ 0.0432]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0558]],\n  \n           [[-0.0153]],\n  \n           [[ 0.0689]],\n  \n           ...,\n  \n           [[-0.0202]],\n  \n           [[ 0.0411]],\n  \n           [[-0.0481]]],\n  \n  \n          [[[-0.1581]],\n  \n           [[-0.0546]],\n  \n           [[ 0.0222]],\n  \n           ...,\n  \n           [[ 0.1405]],\n  \n           [[ 0.0790]],\n  \n           [[ 0.1383]]],\n  \n  \n          [[[ 0.0294]],\n  \n           [[-0.0496]],\n  \n           [[-0.0059]],\n  \n           ...,\n  \n           [[ 0.0113]],\n  \n           [[-0.1224]],\n  \n           [[ 0.0432]]]], requires_grad=True)),\n ('layer3.4.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.4.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.4.conv2.weight',\n  Parameter containing:\n  tensor([[[[-1.6009e-03,  1.0269e-02, -5.2161e-02],\n            [ 4.0043e-02,  5.0708e-02,  3.5235e-02],\n            [-4.4591e-03,  5.7668e-02,  1.2324e-02]],\n  \n           [[ 1.7627e-02, -5.0328e-03,  3.8007e-02],\n            [ 5.7972e-03,  2.1237e-02,  1.4645e-02],\n            [-4.7255e-03, -3.0667e-02, -2.1673e-02]],\n  \n           [[ 4.2580e-02, -3.6451e-02, -2.9569e-02],\n            [ 3.6061e-02, -2.0741e-02, -1.8165e-02],\n            [ 1.2687e-02,  4.9119e-02,  1.5015e-02]],\n  \n           ...,\n  \n           [[-2.4219e-02, -9.8479e-03, -7.3977e-04],\n            [-5.5825e-03,  3.7482e-02,  9.6175e-02],\n            [-2.9165e-03, -1.0624e-02,  1.8222e-03]],\n  \n           [[-2.1274e-02,  2.4935e-02,  2.8950e-02],\n            [ 1.9929e-02, -1.7737e-03, -1.8487e-02],\n            [-4.3933e-02,  2.1108e-02, -4.5799e-02]],\n  \n           [[ 1.1021e-02,  1.3842e-02, -1.2138e-02],\n            [-1.2067e-02, -3.7539e-02,  1.2205e-02],\n            [-3.8640e-03,  1.6865e-02, -1.5869e-02]]],\n  \n  \n          [[[-1.8228e-02,  8.5102e-03, -4.2277e-02],\n            [ 2.7811e-02, -9.1877e-03, -9.5644e-03],\n            [ 4.9165e-02, -2.2988e-02, -2.8005e-02]],\n  \n           [[-2.4615e-02,  1.2243e-02, -1.7397e-02],\n            [-9.8779e-03, -1.8880e-02, -3.1017e-02],\n            [-5.1766e-03,  9.8683e-04, -2.7243e-02]],\n  \n           [[-2.0895e-02,  1.9797e-02, -2.2411e-02],\n            [ 4.9098e-02,  2.3675e-02,  3.6758e-02],\n            [ 1.2734e-02,  3.8140e-02,  1.9758e-02]],\n  \n           ...,\n  \n           [[ 1.6472e-02,  3.2317e-02,  2.5021e-02],\n            [-2.8711e-03, -4.5026e-03,  1.7441e-02],\n            [-1.1909e-02,  3.7979e-02,  1.5970e-02]],\n  \n           [[-6.0905e-02,  5.1312e-03, -9.7965e-03],\n            [ 8.7254e-03, -3.2012e-03, -2.3571e-02],\n            [ 7.9624e-03, -2.2770e-02, -6.2757e-02]],\n  \n           [[ 4.7039e-02, -2.2488e-02, -6.6792e-03],\n            [ 6.7790e-03,  4.0642e-02, -1.3390e-02],\n            [-3.8539e-02,  1.9785e-02, -2.8096e-02]]],\n  \n  \n          [[[ 5.2986e-03, -2.1664e-03,  3.4332e-03],\n            [-2.6189e-03,  1.5890e-02,  1.0145e-03],\n            [ 5.5392e-02,  2.1408e-02, -1.4123e-02]],\n  \n           [[-7.2157e-02, -3.6886e-02, -2.3648e-02],\n            [ 8.3747e-03,  2.3851e-02,  5.2880e-02],\n            [-6.5505e-03,  4.4400e-02, -1.0688e-02]],\n  \n           [[ 1.8727e-03, -1.2892e-02, -3.4563e-02],\n            [ 2.2497e-02,  3.0416e-02,  7.0525e-02],\n            [ 4.2641e-02, -3.7020e-02, -3.0178e-02]],\n  \n           ...,\n  \n           [[ 5.7191e-02,  3.1901e-02,  4.9941e-03],\n            [-2.8775e-02,  2.8710e-02,  4.5303e-03],\n            [-1.4885e-02, -1.9145e-02, -2.7643e-02]],\n  \n           [[ 4.5434e-02,  7.0119e-02,  2.6029e-02],\n            [-1.6961e-02, -7.6769e-03,  9.0082e-03],\n            [ 2.6719e-02,  2.9827e-02, -2.4014e-02]],\n  \n           [[ 3.0025e-02, -3.5443e-02, -1.6958e-02],\n            [ 3.8376e-02,  8.3631e-04,  9.9465e-06],\n            [-2.8835e-02, -2.2292e-02, -2.1455e-03]]],\n  \n  \n          ...,\n  \n  \n          [[[-2.4478e-03,  5.2327e-02, -1.5298e-02],\n            [ 3.0101e-02, -2.9548e-03, -1.0679e-02],\n            [-7.3549e-03, -7.6644e-03, -1.7683e-02]],\n  \n           [[-1.2216e-02,  1.4038e-02, -2.3639e-02],\n            [ 3.0317e-02, -4.1579e-02,  1.3884e-02],\n            [-6.9711e-03,  1.9289e-02,  8.7853e-03]],\n  \n           [[-2.6694e-02, -1.8092e-02, -4.2012e-02],\n            [-5.6319e-02, -1.3250e-03, -2.4169e-02],\n            [-2.7920e-03, -4.9812e-02,  6.0940e-03]],\n  \n           ...,\n  \n           [[-5.1009e-02,  1.3664e-03,  1.2903e-02],\n            [ 6.9633e-02, -2.9692e-02, -3.7569e-02],\n            [ 3.9885e-02,  9.6930e-03,  6.6237e-02]],\n  \n           [[ 2.0155e-02,  1.4155e-02, -7.7627e-03],\n            [-1.8187e-02, -4.0251e-02, -5.8297e-03],\n            [ 1.9117e-02, -3.6646e-02, -3.6978e-02]],\n  \n           [[ 2.1382e-03, -1.4589e-02, -4.1422e-03],\n            [ 1.9308e-02,  3.1649e-02, -8.0743e-02],\n            [ 4.4503e-03,  2.0663e-02, -7.0815e-03]]],\n  \n  \n          [[[-2.5216e-02, -2.2953e-02, -1.3140e-02],\n            [-1.6166e-02, -1.2077e-02, -1.1348e-02],\n            [ 1.8164e-02, -1.7068e-02, -5.5312e-03]],\n  \n           [[ 3.0465e-02,  9.5054e-03,  3.3812e-02],\n            [ 5.7187e-03, -6.4852e-02,  3.7219e-02],\n            [-6.3665e-02,  1.0682e-03,  7.9524e-03]],\n  \n           [[-1.7826e-02,  1.2793e-02, -4.0524e-02],\n            [-2.8849e-02, -3.3858e-02, -1.7214e-02],\n            [-9.2936e-03, -9.0694e-02, -3.7321e-02]],\n  \n           ...,\n  \n           [[ 9.3909e-03,  1.5924e-02,  1.3192e-02],\n            [ 1.8454e-02, -2.0455e-03, -5.9107e-03],\n            [ 4.5423e-03, -1.0603e-02, -4.8922e-02]],\n  \n           [[-3.7442e-02,  5.2821e-03,  4.7385e-02],\n            [ 4.8794e-02, -4.3371e-02, -1.0199e-02],\n            [ 2.6104e-02, -1.3963e-03, -8.6061e-03]],\n  \n           [[ 3.2018e-03, -5.4097e-02,  1.0782e-02],\n            [-3.4256e-02, -1.1275e-02,  4.7361e-02],\n            [ 1.7762e-02,  2.0964e-02,  1.8498e-02]]],\n  \n  \n          [[[-3.8928e-03, -2.7141e-03,  8.0282e-02],\n            [ 1.6840e-02,  2.1849e-02, -4.4947e-03],\n            [-2.5872e-03,  1.0124e-02,  2.5470e-02]],\n  \n           [[-4.5082e-02,  1.4184e-02,  6.2454e-02],\n            [ 1.8772e-02, -2.7898e-02,  3.7316e-02],\n            [ 1.5929e-02,  4.5841e-02,  1.2856e-03]],\n  \n           [[ 4.1949e-05, -4.5213e-03, -4.3752e-02],\n            [ 2.5881e-02,  3.6509e-02, -3.6787e-02],\n            [-7.6144e-04,  2.6140e-02, -7.2909e-03]],\n  \n           ...,\n  \n           [[ 2.7576e-02,  2.2081e-04,  4.4722e-02],\n            [ 1.4542e-02, -1.9347e-02, -8.7500e-03],\n            [-3.3666e-02, -8.7216e-03,  6.1050e-02]],\n  \n           [[ 4.8506e-02,  8.1524e-03, -3.0751e-02],\n            [-9.4473e-03,  4.5495e-02, -2.2727e-02],\n            [-1.4064e-02,  3.7462e-02, -3.3589e-02]],\n  \n           [[ 3.8455e-02,  2.5547e-02, -7.7607e-02],\n            [-2.1535e-02,  3.7851e-02, -1.8051e-02],\n            [-1.2138e-02, -1.1708e-02,  7.7128e-02]]]], requires_grad=True)),\n ('layer3.4.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.4.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.4.conv3.weight',\n  Parameter containing:\n  tensor([[[[ 0.0045]],\n  \n           [[ 0.0044]],\n  \n           [[-0.0911]],\n  \n           ...,\n  \n           [[-0.0347]],\n  \n           [[-0.0436]],\n  \n           [[ 0.0638]]],\n  \n  \n          [[[-0.0112]],\n  \n           [[ 0.0490]],\n  \n           [[ 0.0046]],\n  \n           ...,\n  \n           [[ 0.0429]],\n  \n           [[ 0.0371]],\n  \n           [[-0.0258]]],\n  \n  \n          [[[-0.0271]],\n  \n           [[-0.0352]],\n  \n           [[-0.0645]],\n  \n           ...,\n  \n           [[ 0.0979]],\n  \n           [[-0.0059]],\n  \n           [[-0.0179]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0250]],\n  \n           [[-0.0537]],\n  \n           [[-0.0432]],\n  \n           ...,\n  \n           [[-0.0911]],\n  \n           [[-0.0098]],\n  \n           [[-0.0308]]],\n  \n  \n          [[[-0.0220]],\n  \n           [[-0.0425]],\n  \n           [[ 0.0297]],\n  \n           ...,\n  \n           [[ 0.0403]],\n  \n           [[ 0.0201]],\n  \n           [[ 0.0108]]],\n  \n  \n          [[[ 0.0364]],\n  \n           [[-0.0380]],\n  \n           [[-0.0156]],\n  \n           ...,\n  \n           [[-0.0154]],\n  \n           [[-0.0092]],\n  \n           [[ 0.0012]]]], requires_grad=True)),\n ('layer3.4.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.4.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.5.conv1.weight',\n  Parameter containing:\n  tensor([[[[-0.0043]],\n  \n           [[ 0.0350]],\n  \n           [[ 0.0014]],\n  \n           ...,\n  \n           [[ 0.0885]],\n  \n           [[ 0.0438]],\n  \n           [[ 0.0069]]],\n  \n  \n          [[[ 0.0312]],\n  \n           [[ 0.0504]],\n  \n           [[-0.0351]],\n  \n           ...,\n  \n           [[-0.0128]],\n  \n           [[ 0.1635]],\n  \n           [[ 0.0118]]],\n  \n  \n          [[[-0.2543]],\n  \n           [[ 0.1250]],\n  \n           [[ 0.1887]],\n  \n           ...,\n  \n           [[ 0.0169]],\n  \n           [[ 0.0846]],\n  \n           [[-0.0082]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0547]],\n  \n           [[ 0.0112]],\n  \n           [[-0.0782]],\n  \n           ...,\n  \n           [[ 0.1330]],\n  \n           [[ 0.0309]],\n  \n           [[ 0.0876]]],\n  \n  \n          [[[-0.0116]],\n  \n           [[ 0.0339]],\n  \n           [[-0.1773]],\n  \n           ...,\n  \n           [[ 0.0444]],\n  \n           [[ 0.0096]],\n  \n           [[ 0.1220]]],\n  \n  \n          [[[ 0.1451]],\n  \n           [[-0.0023]],\n  \n           [[ 0.2502]],\n  \n           ...,\n  \n           [[ 0.0243]],\n  \n           [[ 0.0253]],\n  \n           [[-0.0036]]]], requires_grad=True)),\n ('layer3.5.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.5.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.5.conv2.weight',\n  Parameter containing:\n  tensor([[[[ 2.6434e-02, -2.1836e-02, -1.1234e-02],\n            [-9.2392e-03, -2.2462e-02, -2.0102e-02],\n            [ 2.2414e-02,  5.0330e-03,  6.6964e-03]],\n  \n           [[ 1.4137e-02, -1.0181e-02, -1.3095e-02],\n            [ 5.3328e-02,  6.4189e-03,  5.4357e-02],\n            [-5.3178e-03, -1.9453e-04,  1.8373e-02]],\n  \n           [[ 4.0453e-02,  3.8967e-02, -1.7597e-02],\n            [ 3.0030e-03,  2.7744e-02,  3.4714e-02],\n            [ 2.8372e-02,  2.6899e-02, -4.4800e-03]],\n  \n           ...,\n  \n           [[-4.0310e-03,  2.1234e-02, -3.4657e-02],\n            [ 5.2988e-03,  1.5898e-02,  4.7074e-02],\n            [-1.9815e-02,  2.8434e-03,  2.2006e-02]],\n  \n           [[ 1.0875e-02, -1.6967e-02, -1.9532e-02],\n            [-4.0627e-02,  5.3548e-02, -4.6134e-02],\n            [-4.3328e-02, -3.8583e-03, -5.8635e-03]],\n  \n           [[ 1.4780e-02, -3.6662e-02, -3.4554e-02],\n            [ 6.2814e-02, -4.1860e-03,  6.5860e-02],\n            [ 4.0634e-02,  2.6404e-02,  1.5064e-02]]],\n  \n  \n          [[[-2.0236e-02,  9.2149e-03, -4.0874e-02],\n            [-3.3153e-02, -8.2559e-03,  4.0984e-02],\n            [-9.6066e-03,  1.7085e-02,  2.8705e-03]],\n  \n           [[ 1.9451e-02,  3.7758e-02, -1.4316e-02],\n            [-4.3537e-02, -2.0956e-02,  3.6915e-02],\n            [-2.2385e-02,  3.3081e-02, -4.8114e-02]],\n  \n           [[ 8.5790e-03, -1.5657e-02,  6.7200e-04],\n            [-4.1046e-02, -6.8384e-02, -7.5918e-02],\n            [-6.0541e-03,  7.8350e-02, -2.0818e-02]],\n  \n           ...,\n  \n           [[ 3.8658e-02,  5.2896e-03,  4.3779e-03],\n            [-2.7270e-04,  1.8343e-02, -3.5445e-02],\n            [ 2.0636e-02,  2.6610e-02,  2.4861e-02]],\n  \n           [[-5.4595e-02,  9.0047e-03, -3.9797e-02],\n            [ 1.2457e-02,  3.4872e-02, -6.6128e-05],\n            [-3.3244e-03, -2.1983e-02,  1.8678e-02]],\n  \n           [[-1.7539e-02,  1.9651e-02,  1.3256e-02],\n            [-1.0192e-02,  1.4562e-02, -1.1938e-02],\n            [-9.4854e-03,  3.0962e-02,  3.0916e-02]]],\n  \n  \n          [[[-1.0523e-02, -1.3421e-02, -1.8469e-02],\n            [ 1.8604e-02, -1.6417e-02, -7.9428e-03],\n            [ 2.4185e-02, -1.2071e-02, -1.8940e-02]],\n  \n           [[-1.4024e-02,  9.8398e-04,  2.4521e-02],\n            [ 1.6701e-02,  4.2594e-03,  2.7685e-03],\n            [ 1.5224e-02,  7.2711e-02,  7.4280e-03]],\n  \n           [[ 2.2856e-02,  1.0961e-02,  2.5747e-02],\n            [-5.4889e-02, -3.6458e-02, -3.4843e-02],\n            [-1.7421e-02, -4.4129e-04, -1.1286e-02]],\n  \n           ...,\n  \n           [[ 1.6951e-02,  3.0037e-02,  6.9573e-03],\n            [ 2.0069e-02, -4.4338e-02,  7.7577e-02],\n            [ 4.6790e-02, -2.1949e-02, -3.5002e-03]],\n  \n           [[-1.6753e-02,  2.6515e-02,  1.1000e-02],\n            [ 9.3577e-05,  4.5227e-02,  2.1505e-03],\n            [ 2.4821e-02,  3.4910e-02,  4.2795e-02]],\n  \n           [[ 2.3439e-02,  7.7976e-04, -7.4422e-03],\n            [ 1.3590e-02, -5.2847e-03, -7.2529e-02],\n            [-7.6659e-03, -2.2971e-03, -3.8417e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[-1.2381e-02, -8.4840e-03,  4.2581e-02],\n            [ 2.9181e-02,  1.8611e-02, -4.6999e-02],\n            [-1.2886e-02,  8.5311e-03,  8.8962e-03]],\n  \n           [[-4.0055e-03,  1.9318e-03, -2.8455e-03],\n            [ 7.4364e-03,  1.3394e-02,  3.2150e-02],\n            [-6.1572e-03,  6.0423e-03,  3.2564e-02]],\n  \n           [[-2.4173e-02,  2.2617e-02, -5.6635e-03],\n            [-7.6475e-02, -7.8061e-03, -3.7392e-02],\n            [ 6.2541e-04, -1.5019e-02,  4.4601e-02]],\n  \n           ...,\n  \n           [[ 1.8854e-02,  4.6594e-03,  4.2575e-02],\n            [-3.2127e-02,  3.9457e-02,  8.4578e-03],\n            [ 1.3317e-02,  1.1770e-03,  1.5734e-02]],\n  \n           [[-1.2903e-02, -2.4881e-02,  2.8543e-02],\n            [ 9.3455e-03,  2.5847e-02,  7.2611e-03],\n            [ 1.4907e-02, -3.8168e-02, -1.2744e-02]],\n  \n           [[-1.4446e-02,  2.7135e-02,  2.6066e-03],\n            [ 1.4309e-02, -1.3428e-02,  1.0472e-02],\n            [-3.1630e-02,  2.6150e-02,  1.5644e-02]]],\n  \n  \n          [[[ 1.9963e-03, -9.8211e-03,  5.2271e-02],\n            [ 9.1818e-03,  3.4334e-02,  1.0121e-02],\n            [ 2.7377e-02,  1.0128e-02, -3.7383e-02]],\n  \n           [[-4.0218e-03, -1.2025e-02, -2.3390e-02],\n            [ 9.1903e-03,  1.9955e-02, -1.5659e-02],\n            [ 1.2404e-02, -1.3599e-03,  2.1699e-02]],\n  \n           [[ 2.7179e-02,  7.3579e-03,  1.6134e-02],\n            [ 4.0107e-02,  4.0452e-02,  1.3144e-02],\n            [-4.6361e-02, -1.8972e-02, -1.0145e-02]],\n  \n           ...,\n  \n           [[ 1.9948e-02,  2.5877e-02, -2.9555e-02],\n            [-2.8426e-02, -3.3010e-02,  3.1103e-03],\n            [ 6.9286e-03,  5.1598e-02,  4.4639e-02]],\n  \n           [[ 2.7256e-02,  2.2442e-02,  6.6776e-02],\n            [ 2.8108e-02,  3.2509e-02, -3.1326e-02],\n            [ 5.3454e-02,  7.9991e-04,  3.6430e-02]],\n  \n           [[ 1.9641e-02, -5.8673e-03,  2.1618e-02],\n            [ 1.3525e-03,  2.2246e-02,  2.6140e-02],\n            [ 3.5949e-02,  3.2162e-02,  1.0413e-02]]],\n  \n  \n          [[[ 2.9142e-02, -6.8248e-03,  3.1984e-02],\n            [ 2.3306e-02, -1.9535e-02, -2.3457e-02],\n            [ 9.2808e-03, -2.5361e-02, -1.4000e-03]],\n  \n           [[ 2.4940e-02,  4.6259e-03, -1.6168e-02],\n            [ 3.3114e-02,  5.0440e-02, -3.0993e-02],\n            [ 5.6596e-02,  7.1922e-02,  8.3012e-04]],\n  \n           [[ 3.7307e-02,  5.3811e-02,  1.0921e-02],\n            [-4.1824e-03, -2.0072e-04, -4.2779e-02],\n            [-9.4249e-03, -2.5234e-03, -8.4463e-02]],\n  \n           ...,\n  \n           [[ 3.8850e-02, -2.5920e-02, -8.6630e-03],\n            [-2.9257e-02, -1.6580e-02, -1.1036e-02],\n            [-3.2739e-03,  4.8092e-02, -2.6990e-03]],\n  \n           [[-2.1638e-02, -2.6557e-02, -6.0524e-02],\n            [-5.2343e-02,  5.4182e-03, -2.1419e-02],\n            [-3.0394e-02, -3.8983e-02, -1.6648e-02]],\n  \n           [[-4.8174e-03, -1.9662e-02, -4.5180e-02],\n            [-4.0188e-02,  1.3924e-02, -2.0476e-02],\n            [ 3.9147e-02,  7.4245e-03,  4.7037e-02]]]], requires_grad=True)),\n ('layer3.5.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1.], requires_grad=True)),\n ('layer3.5.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n         requires_grad=True)),\n ('layer3.5.conv3.weight',\n  Parameter containing:\n  tensor([[[[-0.0615]],\n  \n           [[ 0.0263]],\n  \n           [[ 0.0758]],\n  \n           ...,\n  \n           [[-0.0147]],\n  \n           [[ 0.0666]],\n  \n           [[ 0.0276]]],\n  \n  \n          [[[ 0.0117]],\n  \n           [[-0.0110]],\n  \n           [[-0.0161]],\n  \n           ...,\n  \n           [[ 0.0337]],\n  \n           [[-0.0308]],\n  \n           [[-0.0739]]],\n  \n  \n          [[[-0.0050]],\n  \n           [[ 0.0043]],\n  \n           [[-0.0038]],\n  \n           ...,\n  \n           [[-0.0171]],\n  \n           [[-0.0593]],\n  \n           [[-0.0623]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0822]],\n  \n           [[ 0.0019]],\n  \n           [[-0.0674]],\n  \n           ...,\n  \n           [[-0.0398]],\n  \n           [[-0.0516]],\n  \n           [[-0.0331]]],\n  \n  \n          [[[-0.0094]],\n  \n           [[-0.0401]],\n  \n           [[ 0.0197]],\n  \n           ...,\n  \n           [[-0.0362]],\n  \n           [[ 0.0016]],\n  \n           [[-0.0070]]],\n  \n  \n          [[[ 0.0430]],\n  \n           [[ 0.0288]],\n  \n           [[ 0.0055]],\n  \n           ...,\n  \n           [[-0.0367]],\n  \n           [[-0.0204]],\n  \n           [[-0.0331]]]], requires_grad=True)),\n ('layer3.5.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer3.5.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer4.0.conv1.weight',\n  Parameter containing:\n  tensor([[[[-5.9745e-02]],\n  \n           [[-7.4324e-02]],\n  \n           [[-4.4100e-02]],\n  \n           ...,\n  \n           [[ 1.5265e-03]],\n  \n           [[ 6.9681e-02]],\n  \n           [[ 6.4814e-02]]],\n  \n  \n          [[[ 9.4036e-02]],\n  \n           [[-5.6925e-02]],\n  \n           [[ 4.6335e-02]],\n  \n           ...,\n  \n           [[ 5.5958e-02]],\n  \n           [[-7.8256e-02]],\n  \n           [[-5.1269e-02]]],\n  \n  \n          [[[-2.4896e-02]],\n  \n           [[ 7.1292e-02]],\n  \n           [[ 8.5281e-02]],\n  \n           ...,\n  \n           [[ 9.0233e-02]],\n  \n           [[-5.7452e-02]],\n  \n           [[ 6.2677e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 9.4506e-02]],\n  \n           [[ 1.0230e-01]],\n  \n           [[ 1.0362e-01]],\n  \n           ...,\n  \n           [[-1.6433e-02]],\n  \n           [[ 1.5824e-02]],\n  \n           [[ 2.1363e-02]]],\n  \n  \n          [[[ 4.4766e-02]],\n  \n           [[ 8.4423e-06]],\n  \n           [[ 2.3952e-02]],\n  \n           ...,\n  \n           [[-8.0248e-02]],\n  \n           [[ 2.3833e-03]],\n  \n           [[-1.2434e-01]]],\n  \n  \n          [[[ 1.0772e-01]],\n  \n           [[-9.4413e-02]],\n  \n           [[ 4.4412e-02]],\n  \n           ...,\n  \n           [[-2.1047e-02]],\n  \n           [[ 4.6554e-03]],\n  \n           [[-4.9911e-02]]]], requires_grad=True)),\n ('layer4.0.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer4.0.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer4.0.conv2.weight',\n  Parameter containing:\n  tensor([[[[-3.8304e-03,  2.0046e-03, -2.7439e-02],\n            [ 2.7892e-02, -2.0464e-02, -4.3362e-03],\n            [-2.5856e-02, -5.1440e-03,  8.1045e-03]],\n  \n           [[-2.8010e-02, -9.9120e-03,  6.1674e-03],\n            [-1.8060e-02,  2.3548e-02, -1.8937e-03],\n            [ 3.3644e-03,  4.2877e-03,  1.9253e-03]],\n  \n           [[-1.0331e-02, -3.2050e-02, -1.2061e-02],\n            [-6.5353e-03,  2.3804e-03, -2.7365e-03],\n            [ 2.2381e-02, -1.9510e-02,  2.5892e-02]],\n  \n           ...,\n  \n           [[-1.1943e-02,  2.3960e-02,  5.1885e-02],\n            [-5.3831e-03, -1.0590e-03, -1.4592e-02],\n            [ 2.0945e-03,  4.8015e-02, -1.5303e-02]],\n  \n           [[-1.7354e-03,  1.1883e-02,  1.1044e-02],\n            [ 1.5336e-03, -2.9820e-03,  2.1448e-02],\n            [ 1.9597e-02, -1.4044e-02, -1.7430e-02]],\n  \n           [[ 1.1387e-02,  9.9918e-03, -4.7555e-03],\n            [-1.8673e-02, -1.7513e-02, -1.8200e-03],\n            [ 8.1278e-03, -3.9202e-03,  2.1658e-02]]],\n  \n  \n          [[[ 2.8501e-02,  3.0649e-03, -1.6597e-02],\n            [ 4.4866e-02, -4.0526e-03, -4.5529e-04],\n            [-2.2344e-02,  3.5199e-02, -5.6911e-03]],\n  \n           [[ 2.5598e-03, -2.3868e-02, -2.4421e-03],\n            [-2.8201e-02, -3.8168e-02, -1.9877e-02],\n            [ 8.1070e-03,  2.6183e-02,  1.4152e-02]],\n  \n           [[ 3.0290e-02, -5.1524e-02,  3.9259e-02],\n            [-2.3437e-02, -2.1343e-02,  5.6372e-03],\n            [-6.6533e-03, -4.1233e-02, -1.4518e-02]],\n  \n           ...,\n  \n           [[-2.4665e-02, -3.3341e-02, -1.4408e-02],\n            [ 1.8418e-03, -1.1030e-03, -2.3373e-02],\n            [ 4.1626e-02,  2.6753e-02,  1.1980e-02]],\n  \n           [[ 2.3877e-02, -1.9031e-02, -2.6479e-02],\n            [-1.0980e-03,  1.3448e-02, -6.7551e-03],\n            [-9.5208e-03, -1.1656e-02,  1.0237e-02]],\n  \n           [[-1.0321e-02,  1.7482e-02,  4.3014e-02],\n            [-4.0213e-03, -3.8233e-02, -1.4208e-02],\n            [-4.1915e-03, -6.1718e-03, -6.7788e-03]]],\n  \n  \n          [[[-1.9105e-02,  3.1632e-02,  7.9245e-03],\n            [ 3.2457e-03, -5.9955e-03, -2.9531e-02],\n            [ 3.0654e-03, -3.3552e-02, -1.2069e-02]],\n  \n           [[-7.8934e-03, -3.6186e-03, -1.0570e-02],\n            [-1.8258e-02,  6.7459e-03,  1.1433e-02],\n            [-2.4231e-02,  4.5846e-03,  2.6002e-02]],\n  \n           [[ 7.8882e-03,  5.1516e-03,  2.4427e-02],\n            [ 1.9511e-02,  4.8677e-03,  2.5931e-02],\n            [ 7.1875e-03,  1.0486e-02, -8.4402e-03]],\n  \n           ...,\n  \n           [[-2.1804e-03, -2.2392e-02, -1.4152e-02],\n            [ 1.7224e-02, -3.8805e-02, -4.0450e-02],\n            [-3.3390e-02, -3.5384e-04, -2.2877e-02]],\n  \n           [[ 2.3938e-02,  4.4311e-05, -4.7773e-02],\n            [ 2.8038e-02,  1.3672e-02,  1.8736e-02],\n            [-1.4882e-02,  1.4046e-02, -1.2547e-02]],\n  \n           [[-4.0174e-02, -1.7367e-02, -2.2264e-02],\n            [-2.5122e-03, -2.5608e-02, -9.6666e-03],\n            [-2.7492e-02, -2.4944e-02, -1.7466e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 1.5336e-03,  1.0210e-02,  2.9918e-02],\n            [ 3.6739e-02, -1.6524e-02, -1.1422e-02],\n            [ 2.4231e-02,  1.1863e-04,  2.0878e-02]],\n  \n           [[-2.1000e-02,  5.3396e-03,  2.7090e-02],\n            [-1.4960e-02,  2.7985e-02,  2.7828e-03],\n            [ 3.9340e-03, -4.5147e-03, -4.6418e-02]],\n  \n           [[-1.6133e-03, -2.2086e-02, -2.9943e-02],\n            [ 2.8666e-02,  4.4507e-02, -7.9303e-03],\n            [-1.2702e-02,  2.7493e-03,  5.3313e-02]],\n  \n           ...,\n  \n           [[ 4.7325e-03, -3.9084e-03, -5.6850e-03],\n            [ 1.5464e-02, -5.0315e-02, -1.5982e-02],\n            [ 2.8746e-02, -4.8295e-02,  1.4566e-03]],\n  \n           [[ 1.0323e-02, -3.1141e-02, -2.1491e-02],\n            [-6.4892e-03, -6.0506e-03,  3.8508e-02],\n            [-2.7322e-02,  1.5664e-02,  3.0606e-02]],\n  \n           [[ 1.7488e-02,  1.1108e-02, -8.0003e-03],\n            [-2.5927e-02,  4.9017e-02, -4.0043e-02],\n            [ 2.6868e-02, -3.2086e-02, -2.8714e-03]]],\n  \n  \n          [[[ 3.6620e-03, -2.9163e-02, -1.8104e-02],\n            [ 8.2524e-03,  1.1828e-02,  3.4014e-02],\n            [ 2.2276e-02,  1.8918e-02,  2.0629e-02]],\n  \n           [[ 8.2328e-03,  1.8445e-02,  2.2128e-02],\n            [-2.3662e-02, -4.4555e-03, -2.7564e-02],\n            [-1.8956e-03, -2.8180e-02, -1.0048e-02]],\n  \n           [[ 1.6084e-02, -2.4208e-02,  2.2892e-02],\n            [ 5.5978e-03, -5.1475e-02, -1.7993e-02],\n            [-5.7298e-03, -9.5657e-03, -6.6069e-03]],\n  \n           ...,\n  \n           [[-1.8728e-02, -3.3384e-02,  3.2787e-02],\n            [ 7.3618e-03,  7.6472e-03,  6.7767e-03],\n            [-3.5113e-03,  3.9306e-03,  1.6094e-03]],\n  \n           [[ 5.9951e-03,  5.4963e-03, -2.2865e-02],\n            [-6.9230e-03, -6.2176e-03, -1.6691e-02],\n            [ 1.5921e-02, -7.2493e-03,  3.4725e-02]],\n  \n           [[-1.9864e-02,  7.7958e-03,  1.8141e-02],\n            [ 2.3738e-02,  9.3161e-03,  3.5635e-03],\n            [-3.9675e-03,  4.7543e-03,  7.5388e-03]]],\n  \n  \n          [[[ 1.0158e-02, -8.2707e-03, -1.8857e-02],\n            [-4.6163e-03,  6.1191e-03,  2.7733e-03],\n            [ 3.3697e-02,  3.5186e-02,  8.1448e-03]],\n  \n           [[ 1.4247e-03,  1.9141e-03,  1.6456e-02],\n            [ 2.4840e-02, -3.0980e-02,  1.6093e-02],\n            [-1.1054e-02,  6.7501e-03,  3.0667e-02]],\n  \n           [[ 7.5286e-03,  6.5059e-03,  6.3152e-04],\n            [ 1.4788e-02, -6.2009e-03, -4.0890e-02],\n            [-1.0598e-02,  5.4706e-04, -1.0918e-02]],\n  \n           ...,\n  \n           [[ 2.1269e-02, -1.7388e-02, -4.3288e-02],\n            [-3.0102e-03, -6.4838e-03,  1.1306e-02],\n            [ 2.9644e-02,  1.6322e-02,  1.9997e-02]],\n  \n           [[-3.4962e-02,  1.4516e-02,  1.0152e-03],\n            [-4.5765e-02, -1.6689e-02,  3.4970e-02],\n            [ 2.0386e-02,  2.8021e-03, -3.2868e-02]],\n  \n           [[ 3.2117e-03, -3.6542e-02, -3.9330e-03],\n            [-1.0898e-02,  5.8171e-03,  9.4409e-03],\n            [ 1.2874e-02, -8.5553e-04, -2.1420e-02]]]], requires_grad=True)),\n ('layer4.0.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer4.0.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer4.0.conv3.weight',\n  Parameter containing:\n  tensor([[[[ 0.0046]],\n  \n           [[-0.0008]],\n  \n           [[-0.0233]],\n  \n           ...,\n  \n           [[-0.0107]],\n  \n           [[-0.0437]],\n  \n           [[-0.0185]]],\n  \n  \n          [[[ 0.0070]],\n  \n           [[ 0.0212]],\n  \n           [[ 0.0024]],\n  \n           ...,\n  \n           [[ 0.0335]],\n  \n           [[-0.0535]],\n  \n           [[-0.0604]]],\n  \n  \n          [[[-0.0353]],\n  \n           [[-0.0117]],\n  \n           [[-0.0006]],\n  \n           ...,\n  \n           [[ 0.0195]],\n  \n           [[ 0.0140]],\n  \n           [[-0.0113]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0137]],\n  \n           [[ 0.0049]],\n  \n           [[-0.0327]],\n  \n           ...,\n  \n           [[-0.0119]],\n  \n           [[ 0.0281]],\n  \n           [[ 0.0484]]],\n  \n  \n          [[[-0.0099]],\n  \n           [[-0.0086]],\n  \n           [[-0.0018]],\n  \n           ...,\n  \n           [[-0.0190]],\n  \n           [[-0.0030]],\n  \n           [[-0.0048]]],\n  \n  \n          [[[-0.0155]],\n  \n           [[ 0.0632]],\n  \n           [[-0.0235]],\n  \n           ...,\n  \n           [[-0.0011]],\n  \n           [[-0.0269]],\n  \n           [[ 0.0105]]]], requires_grad=True)),\n ('layer4.0.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer4.0.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer4.0.downsample.0.weight',\n  Parameter containing:\n  tensor([[[[ 0.0083]],\n  \n           [[-0.0228]],\n  \n           [[-0.0358]],\n  \n           ...,\n  \n           [[ 0.0407]],\n  \n           [[ 0.0470]],\n  \n           [[ 0.0972]]],\n  \n  \n          [[[-0.0314]],\n  \n           [[-0.0021]],\n  \n           [[-0.0051]],\n  \n           ...,\n  \n           [[ 0.0472]],\n  \n           [[-0.0119]],\n  \n           [[ 0.0029]]],\n  \n  \n          [[[-0.0372]],\n  \n           [[-0.0418]],\n  \n           [[-0.0275]],\n  \n           ...,\n  \n           [[-0.0457]],\n  \n           [[ 0.0187]],\n  \n           [[ 0.0087]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0391]],\n  \n           [[ 0.0145]],\n  \n           [[ 0.0012]],\n  \n           ...,\n  \n           [[ 0.0104]],\n  \n           [[-0.0364]],\n  \n           [[ 0.0500]]],\n  \n  \n          [[[ 0.0189]],\n  \n           [[ 0.0402]],\n  \n           [[-0.0187]],\n  \n           ...,\n  \n           [[-0.0638]],\n  \n           [[ 0.0278]],\n  \n           [[-0.0170]]],\n  \n  \n          [[[-0.0285]],\n  \n           [[-0.0459]],\n  \n           [[ 0.0076]],\n  \n           ...,\n  \n           [[-0.0510]],\n  \n           [[-0.0109]],\n  \n           [[-0.0272]]]], requires_grad=True)),\n ('layer4.0.downsample.1.weight',\n  Parameter containing:\n  tensor([1., 1., 1.,  ..., 1., 1., 1.], requires_grad=True)),\n ('layer4.0.downsample.1.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer4.1.conv1.weight',\n  Parameter containing:\n  tensor([[[[-0.1204]],\n  \n           [[ 0.0467]],\n  \n           [[ 0.0948]],\n  \n           ...,\n  \n           [[-0.0150]],\n  \n           [[ 0.0605]],\n  \n           [[-0.0105]]],\n  \n  \n          [[[-0.0534]],\n  \n           [[-0.0625]],\n  \n           [[-0.1042]],\n  \n           ...,\n  \n           [[ 0.1123]],\n  \n           [[-0.0423]],\n  \n           [[ 0.0004]]],\n  \n  \n          [[[ 0.0787]],\n  \n           [[ 0.0911]],\n  \n           [[-0.0282]],\n  \n           ...,\n  \n           [[ 0.0347]],\n  \n           [[ 0.0323]],\n  \n           [[-0.0826]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0602]],\n  \n           [[ 0.0721]],\n  \n           [[ 0.0858]],\n  \n           ...,\n  \n           [[-0.0578]],\n  \n           [[-0.0190]],\n  \n           [[ 0.0481]]],\n  \n  \n          [[[ 0.1233]],\n  \n           [[ 0.0088]],\n  \n           [[ 0.0011]],\n  \n           ...,\n  \n           [[-0.0653]],\n  \n           [[ 0.0105]],\n  \n           [[-0.0348]]],\n  \n  \n          [[[-0.0663]],\n  \n           [[ 0.0027]],\n  \n           [[ 0.1578]],\n  \n           ...,\n  \n           [[ 0.0338]],\n  \n           [[-0.0268]],\n  \n           [[ 0.0397]]]], requires_grad=True)),\n ('layer4.1.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer4.1.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer4.1.conv2.weight',\n  Parameter containing:\n  tensor([[[[-1.4875e-02,  3.2817e-02,  2.0309e-03],\n            [ 2.7361e-02,  1.7980e-02,  4.7771e-03],\n            [ 8.3609e-03,  1.9908e-03, -2.1515e-02]],\n  \n           [[ 1.5472e-02, -1.8721e-02, -1.5944e-02],\n            [ 1.0105e-02, -1.6259e-02,  4.2593e-03],\n            [-1.4311e-02, -2.1756e-02, -3.5767e-02]],\n  \n           [[-2.0048e-03,  2.5163e-02,  2.1553e-03],\n            [ 7.1713e-03, -1.5705e-02,  6.5028e-03],\n            [-2.8759e-04,  1.9339e-02,  1.1133e-02]],\n  \n           ...,\n  \n           [[ 1.9712e-02, -1.4370e-02,  1.4445e-02],\n            [ 1.0048e-03,  7.7705e-03,  3.4548e-03],\n            [ 4.1120e-02, -2.0746e-02,  1.8221e-02]],\n  \n           [[-3.8585e-02,  1.9015e-05,  2.3851e-02],\n            [ 2.2705e-02, -1.2875e-02, -4.3378e-02],\n            [-1.0224e-03, -1.1943e-02, -5.5919e-02]],\n  \n           [[ 2.7346e-03,  5.4173e-03,  1.4899e-02],\n            [-2.9461e-02, -6.3389e-03, -2.5316e-02],\n            [-8.9263e-04, -2.2314e-02, -1.3692e-03]]],\n  \n  \n          [[[-2.3572e-02,  1.3061e-02,  8.1310e-03],\n            [-3.3054e-02,  3.7524e-03, -8.4084e-03],\n            [ 2.0152e-02,  9.5777e-03, -3.0059e-02]],\n  \n           [[ 2.0882e-02, -3.3932e-04, -2.1867e-02],\n            [-5.7642e-03, -2.5067e-02, -3.2238e-02],\n            [-3.9565e-03,  9.7498e-03, -7.4497e-03]],\n  \n           [[-4.2966e-03,  1.1454e-02,  9.1774e-03],\n            [-4.3429e-03, -9.5762e-04, -2.8221e-04],\n            [-2.4145e-03,  5.8480e-03, -6.3342e-03]],\n  \n           ...,\n  \n           [[ 1.5185e-02,  1.7225e-02, -1.3493e-02],\n            [ 1.8122e-02, -2.5433e-02, -8.0547e-03],\n            [-3.2391e-03,  3.1090e-03,  2.7796e-02]],\n  \n           [[ 2.5820e-02, -2.0805e-03,  3.1768e-02],\n            [-4.5366e-03,  2.5713e-03, -5.1541e-02],\n            [ 2.7077e-02,  2.4704e-02,  2.3425e-02]],\n  \n           [[ 1.0992e-02, -4.3989e-03, -2.9784e-02],\n            [-3.8059e-02, -3.4452e-03,  1.6183e-02],\n            [-2.3624e-02,  3.6482e-03, -3.8072e-03]]],\n  \n  \n          [[[-2.5082e-02, -2.1973e-02,  2.4752e-02],\n            [ 8.6584e-03, -4.8067e-03,  3.2572e-02],\n            [ 3.1512e-02, -1.4320e-02,  4.2325e-03]],\n  \n           [[-2.4489e-03,  7.2891e-03, -4.3926e-02],\n            [-3.4502e-03, -3.2062e-03,  2.8278e-02],\n            [ 2.1744e-03, -1.1845e-04,  3.5486e-02]],\n  \n           [[-1.5193e-02,  3.4429e-02,  1.2551e-02],\n            [-1.3719e-02,  1.1010e-02, -1.6960e-02],\n            [-4.3950e-04, -7.7311e-03,  1.3521e-02]],\n  \n           ...,\n  \n           [[ 4.4235e-04, -1.6710e-02,  2.8953e-02],\n            [ 2.0065e-02,  9.1982e-03, -3.5463e-02],\n            [ 1.7301e-02, -1.9391e-02, -2.4868e-02]],\n  \n           [[ 1.7814e-02, -5.5200e-03, -2.2634e-02],\n            [-3.7579e-03,  1.1860e-02, -5.0001e-04],\n            [-5.1746e-03, -1.8023e-02, -5.7977e-03]],\n  \n           [[ 2.3632e-02,  2.4088e-02, -1.4406e-02],\n            [ 4.4347e-03, -1.2388e-02, -3.5370e-02],\n            [ 3.0446e-02,  5.7332e-03,  2.2244e-04]]],\n  \n  \n          ...,\n  \n  \n          [[[-2.5161e-03, -1.5265e-02, -2.7160e-02],\n            [ 1.5742e-02,  1.2813e-02, -4.9171e-03],\n            [ 3.2447e-02,  3.7092e-02,  2.4212e-03]],\n  \n           [[ 1.2892e-02, -5.6578e-03,  4.0294e-03],\n            [-1.8055e-02, -2.5514e-02,  4.1187e-03],\n            [-4.0521e-02, -1.6409e-02,  2.2845e-02]],\n  \n           [[-1.1588e-02, -6.1483e-03,  2.0049e-02],\n            [-3.7533e-03,  2.4134e-02,  5.8985e-03],\n            [ 1.6091e-02,  4.1375e-02, -1.1914e-03]],\n  \n           ...,\n  \n           [[-1.8447e-02,  2.1346e-02, -2.0074e-02],\n            [ 4.4945e-02,  8.4388e-03, -4.4124e-03],\n            [-3.5401e-02,  1.3727e-02, -3.0955e-02]],\n  \n           [[-8.4029e-03, -6.6551e-03, -1.2266e-02],\n            [ 2.0679e-02,  4.7484e-02, -3.3798e-04],\n            [ 1.6623e-02, -2.4894e-02, -3.1041e-02]],\n  \n           [[-9.0618e-03, -2.8014e-03, -2.0485e-03],\n            [ 1.0627e-02, -5.7895e-03, -1.2222e-02],\n            [-3.6892e-03,  1.8226e-02,  3.0287e-03]]],\n  \n  \n          [[[-1.4413e-02, -2.5413e-02,  1.1292e-02],\n            [-1.8975e-02,  4.7378e-04,  2.7890e-02],\n            [-5.3665e-03,  6.1852e-03,  1.8250e-02]],\n  \n           [[-2.0131e-02, -4.0751e-02, -7.6659e-03],\n            [-2.9683e-02, -1.3461e-02,  4.4304e-03],\n            [-4.3538e-02, -2.5943e-02,  3.9320e-02]],\n  \n           [[ 1.4356e-02, -3.6358e-04,  1.3382e-03],\n            [-1.8499e-02,  4.6870e-04,  1.2708e-06],\n            [-1.0194e-02,  2.3945e-02, -2.1767e-02]],\n  \n           ...,\n  \n           [[-1.2497e-02, -2.6169e-03, -2.8204e-02],\n            [-1.1539e-02,  9.4762e-03,  2.0115e-02],\n            [ 3.3677e-02, -1.7105e-02,  1.4507e-02]],\n  \n           [[-1.3053e-02, -2.8408e-02, -4.0868e-02],\n            [-2.8236e-02,  6.2103e-03, -4.6452e-03],\n            [ 4.4288e-03,  1.4584e-02, -3.8451e-03]],\n  \n           [[ 2.6254e-02,  1.8147e-02,  3.6901e-02],\n            [ 1.7411e-02, -3.1215e-02,  5.8578e-03],\n            [-4.6043e-02,  7.0331e-02, -2.2305e-02]]],\n  \n  \n          [[[ 6.3843e-03, -2.6831e-02, -1.0774e-02],\n            [ 1.4541e-02,  9.5206e-03, -6.6421e-03],\n            [ 2.6314e-02,  1.0686e-02,  1.6661e-03]],\n  \n           [[ 1.4916e-02, -1.0201e-02,  1.5558e-02],\n            [ 1.7815e-03,  1.8692e-02, -2.5806e-02],\n            [ 1.0424e-02,  3.0308e-02,  1.9223e-02]],\n  \n           [[ 3.0190e-02,  1.5153e-02, -7.5506e-03],\n            [ 1.0196e-02,  9.0764e-03,  1.5230e-02],\n            [ 2.2625e-02,  1.7294e-02,  1.0638e-02]],\n  \n           ...,\n  \n           [[ 1.1707e-02,  1.8566e-02, -1.4157e-02],\n            [-8.9678e-03,  2.0868e-02,  1.1559e-02],\n            [ 7.8745e-04,  3.0270e-02,  1.4833e-02]],\n  \n           [[-5.7109e-03, -1.2130e-02, -2.7492e-02],\n            [-4.6356e-03,  3.6730e-02,  1.3568e-02],\n            [ 4.1427e-02, -7.8176e-03, -7.0532e-03]],\n  \n           [[ 4.5137e-03,  1.3893e-02, -2.5502e-02],\n            [ 3.4937e-02, -9.3189e-03,  1.8159e-02],\n            [-1.0250e-02,  4.7569e-04, -1.2051e-02]]]], requires_grad=True)),\n ('layer4.1.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer4.1.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer4.1.conv3.weight',\n  Parameter containing:\n  tensor([[[[ 0.0697]],\n  \n           [[ 0.0180]],\n  \n           [[-0.0302]],\n  \n           ...,\n  \n           [[ 0.0276]],\n  \n           [[ 0.0211]],\n  \n           [[-0.0052]]],\n  \n  \n          [[[ 0.0008]],\n  \n           [[-0.0202]],\n  \n           [[ 0.0040]],\n  \n           ...,\n  \n           [[-0.0055]],\n  \n           [[-0.0018]],\n  \n           [[ 0.0169]]],\n  \n  \n          [[[ 0.0036]],\n  \n           [[-0.0117]],\n  \n           [[-0.0001]],\n  \n           ...,\n  \n           [[-0.0088]],\n  \n           [[ 0.0150]],\n  \n           [[-0.0107]]],\n  \n  \n          ...,\n  \n  \n          [[[-0.0468]],\n  \n           [[ 0.0299]],\n  \n           [[-0.0465]],\n  \n           ...,\n  \n           [[ 0.0008]],\n  \n           [[ 0.0666]],\n  \n           [[-0.0275]]],\n  \n  \n          [[[-0.0167]],\n  \n           [[-0.0299]],\n  \n           [[-0.0457]],\n  \n           ...,\n  \n           [[ 0.0287]],\n  \n           [[-0.1034]],\n  \n           [[ 0.0131]]],\n  \n  \n          [[[ 0.0016]],\n  \n           [[-0.0029]],\n  \n           [[ 0.0188]],\n  \n           ...,\n  \n           [[ 0.0329]],\n  \n           [[-0.0163]],\n  \n           [[-0.0215]]]], requires_grad=True)),\n ('layer4.1.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer4.1.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer4.2.conv1.weight',\n  Parameter containing:\n  tensor([[[[ 0.0869]],\n  \n           [[ 0.0008]],\n  \n           [[-0.0862]],\n  \n           ...,\n  \n           [[-0.0899]],\n  \n           [[ 0.0431]],\n  \n           [[ 0.0210]]],\n  \n  \n          [[[-0.0638]],\n  \n           [[ 0.0383]],\n  \n           [[-0.0071]],\n  \n           ...,\n  \n           [[ 0.0776]],\n  \n           [[ 0.0079]],\n  \n           [[-0.0025]]],\n  \n  \n          [[[-0.0752]],\n  \n           [[ 0.0471]],\n  \n           [[ 0.0177]],\n  \n           ...,\n  \n           [[-0.0676]],\n  \n           [[ 0.0094]],\n  \n           [[-0.0518]]],\n  \n  \n          ...,\n  \n  \n          [[[ 0.0063]],\n  \n           [[-0.0471]],\n  \n           [[-0.0583]],\n  \n           ...,\n  \n           [[ 0.0199]],\n  \n           [[-0.0632]],\n  \n           [[-0.0098]]],\n  \n  \n          [[[ 0.0009]],\n  \n           [[-0.0451]],\n  \n           [[-0.0351]],\n  \n           ...,\n  \n           [[ 0.0193]],\n  \n           [[ 0.0237]],\n  \n           [[-0.0408]]],\n  \n  \n          [[[ 0.1019]],\n  \n           [[ 0.0358]],\n  \n           [[-0.0502]],\n  \n           ...,\n  \n           [[-0.0375]],\n  \n           [[ 0.0198]],\n  \n           [[ 0.0104]]]], requires_grad=True)),\n ('layer4.2.bn1.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer4.2.bn1.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer4.2.conv2.weight',\n  Parameter containing:\n  tensor([[[[ 4.0372e-03,  9.6009e-03,  5.7336e-03],\n            [ 3.6475e-03,  1.3929e-03, -4.3581e-02],\n            [ 1.8282e-02,  2.6522e-02, -4.4680e-03]],\n  \n           [[ 1.8466e-03, -2.1687e-02,  3.2666e-02],\n            [-1.1404e-03,  4.9626e-03,  2.6408e-02],\n            [ 7.1162e-03,  1.9486e-02, -2.4696e-02]],\n  \n           [[-1.1102e-02, -3.2891e-03,  1.2114e-02],\n            [-3.5552e-02,  1.5232e-02,  3.2585e-02],\n            [-2.1484e-02,  3.1279e-02,  1.4156e-02]],\n  \n           ...,\n  \n           [[-2.5637e-03,  2.4398e-03, -3.0658e-03],\n            [-1.1979e-02,  1.6302e-02,  2.3396e-03],\n            [ 8.2219e-03,  6.6287e-02, -7.6532e-03]],\n  \n           [[-2.9873e-03,  3.1535e-03, -3.0700e-03],\n            [ 3.1522e-02, -3.0844e-02, -2.4698e-02],\n            [ 2.0255e-02, -5.4907e-03,  7.8847e-03]],\n  \n           [[-4.5566e-02, -3.2117e-04, -2.9282e-02],\n            [ 2.1581e-02,  3.1376e-02, -6.7348e-03],\n            [ 3.3176e-02, -1.7408e-02, -1.2875e-03]]],\n  \n  \n          [[[ 9.0611e-03, -7.6816e-04, -1.9904e-02],\n            [-7.1802e-03,  1.6015e-02, -1.7035e-02],\n            [ 8.4524e-03,  2.4376e-02, -1.3405e-02]],\n  \n           [[ 1.3314e-03, -3.2731e-02, -2.7464e-02],\n            [ 3.4712e-02, -1.2054e-02, -1.3446e-02],\n            [ 1.7766e-03,  6.2005e-02, -3.8113e-02]],\n  \n           [[ 4.3504e-02,  8.5865e-04,  7.6798e-04],\n            [-1.7990e-02,  1.2751e-02, -7.5429e-03],\n            [ 1.0408e-02,  3.8950e-02, -1.0803e-02]],\n  \n           ...,\n  \n           [[-4.2192e-02,  3.7597e-02,  1.2419e-03],\n            [-1.1473e-02, -2.0376e-02,  1.3651e-02],\n            [ 1.8838e-02, -1.9380e-02, -1.4051e-02]],\n  \n           [[-3.3401e-03, -1.8234e-03,  1.1112e-02],\n            [ 2.3838e-02, -7.7340e-03,  1.4265e-03],\n            [-3.9946e-03, -4.3768e-02, -1.4484e-03]],\n  \n           [[-1.8119e-02,  2.5660e-02,  3.8637e-03],\n            [ 5.3056e-04, -1.4192e-02, -1.6952e-02],\n            [ 1.3493e-02, -1.7834e-02,  2.4105e-02]]],\n  \n  \n          [[[ 1.4504e-02,  1.2735e-02,  2.6928e-03],\n            [ 4.2135e-03,  4.0163e-03,  3.3633e-02],\n            [ 1.4109e-02,  2.5378e-02,  3.2268e-03]],\n  \n           [[-4.0881e-02,  2.9987e-02, -6.7412e-03],\n            [-1.3335e-03,  9.1620e-03, -3.5123e-04],\n            [-2.5886e-02,  1.2616e-02,  1.3708e-02]],\n  \n           [[-2.6507e-03, -2.2557e-02,  3.2115e-02],\n            [-2.4336e-02, -3.8428e-03, -2.8200e-02],\n            [-1.9067e-02,  2.4719e-02, -5.3098e-02]],\n  \n           ...,\n  \n           [[ 1.3047e-02, -4.6418e-02,  1.8212e-02],\n            [ 1.8339e-02,  2.4303e-03,  1.7478e-02],\n            [ 1.5208e-02, -8.4832e-03,  3.6281e-02]],\n  \n           [[-1.3485e-02,  6.7969e-03,  2.2060e-02],\n            [ 3.4260e-02,  3.5324e-03, -2.3624e-02],\n            [-1.1581e-02, -1.8611e-02, -1.7838e-02]],\n  \n           [[-2.5433e-03,  2.6012e-02,  1.2603e-02],\n            [ 2.5943e-02,  1.4686e-02, -2.8406e-03],\n            [-1.1629e-02,  1.8200e-02,  2.5403e-03]]],\n  \n  \n          ...,\n  \n  \n          [[[ 2.7300e-02,  1.0148e-02,  3.8089e-02],\n            [-3.7646e-02, -2.8380e-02,  6.6293e-03],\n            [-1.4086e-02, -2.3508e-02,  3.8443e-02]],\n  \n           [[-3.1648e-02,  2.7956e-04, -4.5027e-03],\n            [-3.5491e-02,  4.1126e-02, -3.2401e-02],\n            [ 2.2612e-02, -1.0945e-02, -4.6242e-03]],\n  \n           [[-2.6057e-02,  5.9616e-03, -2.8074e-02],\n            [ 1.2896e-02,  1.8758e-02, -1.3279e-02],\n            [ 9.2041e-03, -5.8497e-03, -1.0998e-02]],\n  \n           ...,\n  \n           [[-9.1234e-03,  6.9578e-03, -2.1609e-02],\n            [ 1.9468e-03,  1.8515e-02, -1.8667e-04],\n            [ 3.8241e-02, -1.6975e-02,  2.9781e-02]],\n  \n           [[ 3.0355e-02, -8.8759e-04, -2.8915e-02],\n            [ 1.3347e-02,  3.3460e-03,  7.2086e-03],\n            [-2.0003e-02,  1.0483e-02,  1.0230e-02]],\n  \n           [[ 1.1430e-03, -2.5290e-02, -7.6073e-03],\n            [ 1.6937e-02,  8.2370e-03,  2.9507e-02],\n            [ 1.3583e-02, -1.5253e-02, -2.7242e-02]]],\n  \n  \n          [[[-7.6098e-04,  3.9051e-03,  1.8709e-02],\n            [-1.1853e-02, -3.9333e-03, -2.3217e-02],\n            [ 1.3281e-02,  6.7374e-03, -1.1190e-02]],\n  \n           [[-1.4907e-02, -2.2079e-02, -5.1073e-03],\n            [ 8.6738e-03,  2.5048e-02,  3.1550e-02],\n            [ 1.2738e-02,  1.1874e-02,  7.4871e-03]],\n  \n           [[-1.3418e-02, -1.6440e-02,  2.0048e-03],\n            [ 3.3758e-02,  1.7985e-02, -7.6593e-03],\n            [ 1.0501e-02, -1.1576e-02,  1.1482e-02]],\n  \n           ...,\n  \n           [[-3.9476e-02,  4.3286e-02, -1.4864e-02],\n            [ 1.0187e-02,  3.5476e-03, -2.0036e-03],\n            [ 2.8613e-02,  5.0859e-03,  1.9285e-02]],\n  \n           [[ 2.6005e-02, -9.6460e-03,  1.4698e-02],\n            [ 1.5942e-02, -6.0562e-03,  5.4321e-03],\n            [ 1.8563e-02,  2.8981e-02, -4.2562e-02]],\n  \n           [[ 6.3422e-03, -8.8243e-03,  1.2056e-02],\n            [-8.7140e-03, -7.0939e-03,  4.0304e-02],\n            [-1.3844e-02, -1.5328e-02,  1.6865e-02]]],\n  \n  \n          [[[ 9.2547e-03, -2.0169e-02, -2.6993e-02],\n            [ 2.2630e-02, -9.1510e-03,  2.9532e-03],\n            [ 1.5936e-02, -1.2428e-02, -1.1674e-02]],\n  \n           [[-6.4904e-03, -2.4222e-02,  3.2656e-04],\n            [-7.8919e-03, -2.1352e-02,  1.6693e-02],\n            [ 1.6575e-02,  3.7735e-03, -3.0243e-02]],\n  \n           [[ 7.2092e-03,  3.3445e-02,  3.1266e-02],\n            [ 3.0373e-03, -4.3604e-02,  1.2334e-02],\n            [ 1.8124e-02, -1.6813e-02, -3.5622e-02]],\n  \n           ...,\n  \n           [[-5.3451e-04, -7.1725e-04,  3.3224e-02],\n            [ 3.2404e-02,  2.0535e-02,  3.7013e-02],\n            [ 1.8017e-02, -4.6330e-03, -7.2671e-05]],\n  \n           [[-2.1402e-02, -1.5603e-02, -2.9944e-02],\n            [ 2.2587e-02, -2.3721e-02,  1.6249e-02],\n            [ 2.3000e-02,  1.2225e-02, -2.4762e-02]],\n  \n           [[-7.1571e-03,  5.2951e-03, -6.9071e-03],\n            [ 1.3710e-02, -1.1639e-02, -2.1029e-02],\n            [-2.8345e-02, -9.9142e-03,  1.6903e-02]]]], requires_grad=True)),\n ('layer4.2.bn2.weight',\n  Parameter containing:\n  tensor([1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n          1., 1., 1., 1., 1., 1., 1., 1.], requires_grad=True)),\n ('layer4.2.bn2.bias',\n  Parameter containing:\n  tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n          0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)),\n ('layer4.2.conv3.weight',\n  Parameter containing:\n  tensor([[[[ 1.2131e-02]],\n  \n           [[ 5.8428e-02]],\n  \n           [[ 2.0046e-02]],\n  \n           ...,\n  \n           [[ 1.2015e-03]],\n  \n           [[-5.0296e-04]],\n  \n           [[ 2.0999e-03]]],\n  \n  \n          [[[-3.1696e-03]],\n  \n           [[ 1.6836e-02]],\n  \n           [[-4.9158e-02]],\n  \n           ...,\n  \n           [[ 2.1881e-02]],\n  \n           [[ 1.6721e-02]],\n  \n           [[-1.4479e-02]]],\n  \n  \n          [[[ 1.4656e-02]],\n  \n           [[-1.9131e-02]],\n  \n           [[-1.2144e-02]],\n  \n           ...,\n  \n           [[ 2.0264e-02]],\n  \n           [[-6.3847e-02]],\n  \n           [[-1.8942e-02]]],\n  \n  \n          ...,\n  \n  \n          [[[ 5.3712e-05]],\n  \n           [[ 3.9394e-02]],\n  \n           [[ 4.2408e-03]],\n  \n           ...,\n  \n           [[ 1.0187e-02]],\n  \n           [[-3.4083e-02]],\n  \n           [[ 3.9114e-02]]],\n  \n  \n          [[[ 1.5032e-02]],\n  \n           [[-9.2758e-03]],\n  \n           [[-1.4523e-02]],\n  \n           ...,\n  \n           [[-6.3346e-04]],\n  \n           [[ 8.7917e-03]],\n  \n           [[-3.3098e-02]]],\n  \n  \n          [[[-5.7945e-03]],\n  \n           [[ 2.6874e-02]],\n  \n           [[-3.3141e-02]],\n  \n           ...,\n  \n           [[ 1.7235e-02]],\n  \n           [[-9.5212e-03]],\n  \n           [[-2.5897e-02]]]], requires_grad=True)),\n ('layer4.2.bn3.weight',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('layer4.2.bn3.bias',\n  Parameter containing:\n  tensor([0., 0., 0.,  ..., 0., 0., 0.], requires_grad=True)),\n ('fc.weight',\n  Parameter containing:\n  tensor([[ 0.0066,  0.0131, -0.0071,  ..., -0.0081, -0.0121, -0.0037],\n          [ 0.0060,  0.0119,  0.0071,  ..., -0.0145,  0.0127, -0.0213],\n          [-0.0149, -0.0186, -0.0024,  ..., -0.0134, -0.0213,  0.0144],\n          ...,\n          [ 0.0178,  0.0139,  0.0033,  ..., -0.0196, -0.0128, -0.0092],\n          [ 0.0086, -0.0037, -0.0164,  ...,  0.0169,  0.0027, -0.0115],\n          [ 0.0034,  0.0171,  0.0171,  ...,  0.0028,  0.0152,  0.0161]],\n         requires_grad=True)),\n ('fc.bias',\n  Parameter containing:\n  tensor([-4.9517e-03, -2.5835e-03,  2.0803e-02,  9.4650e-03,  3.3053e-03,\n          -5.6519e-03,  1.9817e-02,  2.0498e-02, -1.6693e-02,  2.1255e-02,\n          -9.7826e-04, -9.4811e-03,  1.1158e-02,  5.8161e-04,  1.3158e-02,\n           2.0398e-02, -1.5821e-02,  1.4266e-02,  7.0510e-03, -3.6685e-03,\n          -1.7214e-02, -1.4209e-02, -3.0199e-03,  1.0627e-02,  6.5157e-03,\n           9.4924e-04,  1.0380e-02,  3.8509e-03, -5.7467e-03,  1.5026e-02,\n          -5.9394e-03, -8.6701e-03,  1.6169e-02,  4.5708e-03, -2.0653e-02,\n          -1.5963e-03,  6.8233e-03,  1.7060e-02, -1.3947e-02, -3.4503e-03,\n          -1.1365e-02, -1.2223e-02,  1.2113e-02, -2.4736e-04,  1.3549e-02,\n          -2.0833e-02, -1.6784e-02, -1.9889e-02, -4.5695e-03,  7.6254e-03,\n          -4.5849e-03,  1.6212e-02, -1.9736e-03,  1.7916e-02, -1.5456e-03,\n          -1.7383e-02, -6.6034e-03,  4.0540e-03, -1.6151e-02,  2.4600e-03,\n          -7.5390e-03, -1.7564e-02,  8.3464e-03, -5.7819e-03,  9.3517e-03,\n          -1.0946e-02, -1.9944e-02, -9.4418e-03,  2.0330e-02, -8.0082e-03,\n          -2.0544e-02,  1.6829e-02,  1.4149e-02, -1.4097e-03,  1.5277e-02,\n           2.0283e-02, -6.3989e-03, -5.1823e-03, -3.7406e-05,  1.3873e-02,\n          -1.6744e-02, -2.1233e-02, -3.0119e-03,  1.9213e-02,  1.0302e-02,\n          -5.7545e-03, -5.6215e-03,  6.3183e-03, -1.5332e-02, -1.8230e-02,\n           5.7956e-03, -2.2010e-02, -2.3797e-03,  1.1589e-02, -2.0739e-02,\n          -4.9881e-03,  6.2564e-03, -2.0244e-02,  8.1927e-03,  2.0681e-02,\n          -7.8505e-03, -1.7218e-02,  1.0768e-02, -1.2801e-02, -1.3541e-02,\n          -1.2043e-02, -2.1172e-02, -1.4716e-02, -5.4089e-03,  2.0620e-02,\n           2.0371e-02, -2.1446e-02, -2.1623e-02,  4.9458e-03, -2.0032e-02,\n           1.3742e-02,  2.7540e-03,  1.0424e-02, -9.8711e-04,  2.9598e-03,\n          -1.5282e-02, -2.5758e-03, -1.8975e-02, -1.8392e-02, -2.3569e-03,\n          -1.9854e-02, -1.6204e-02,  1.5703e-02,  1.4556e-02, -5.1735e-03,\n           1.5252e-02,  1.8951e-02, -8.9992e-03,  1.9383e-02, -3.2948e-03,\n          -1.0476e-02, -7.5083e-03,  3.5340e-03,  9.8979e-03, -1.5054e-02,\n          -2.0320e-02, -6.3631e-03,  2.0043e-02,  1.9830e-02,  1.3094e-02,\n          -1.5390e-02, -7.6599e-03,  1.3593e-02,  1.0336e-02,  1.0929e-02,\n           2.0372e-04,  1.1107e-04,  1.0213e-02,  3.6813e-03, -1.6412e-02,\n           1.1802e-03,  8.4597e-03, -9.2707e-03, -5.4170e-03,  1.9675e-02,\n          -8.5243e-03, -1.1558e-02,  1.6129e-02, -1.3405e-02,  4.6223e-04,\n          -1.8774e-02, -2.9299e-03,  7.3921e-03,  2.8056e-03, -1.2501e-03,\n           7.3736e-03, -1.4411e-03, -1.7926e-02,  1.0428e-03,  2.0704e-02,\n           1.0374e-02, -1.8622e-02, -1.9445e-04,  2.8512e-03, -9.0790e-03,\n          -2.2082e-02,  4.1009e-03,  1.2553e-02, -2.1904e-02,  1.1606e-02,\n          -5.5057e-03,  1.7030e-02, -1.4541e-02, -1.8824e-03,  3.1945e-03,\n          -2.0524e-02, -8.5445e-03,  1.2024e-02,  1.8271e-02, -1.3442e-02,\n           9.6984e-03, -1.7566e-02, -1.5033e-02,  8.0769e-03, -1.9867e-02,\n          -2.1866e-03,  5.9113e-03,  9.7809e-03, -2.0488e-02, -1.3959e-03,\n          -9.7055e-03, -9.8444e-03,  7.6046e-03, -1.1191e-02,  9.0376e-03,\n          -5.9319e-03,  1.2340e-03, -1.5603e-02, -1.1446e-02, -1.2420e-02,\n          -1.6465e-02, -2.1580e-02,  1.1416e-02, -1.8703e-02, -5.5200e-03,\n           1.4327e-02, -9.4115e-03, -6.8531e-03, -7.7144e-03, -1.8033e-02,\n           1.2368e-02,  1.2903e-02,  2.1552e-02,  1.7600e-02, -2.2058e-02,\n          -1.4971e-02,  9.7099e-03, -1.6501e-02,  3.2777e-03, -1.6091e-02,\n          -5.1584e-03, -1.9415e-02,  7.5102e-03, -1.9682e-04,  2.1740e-02,\n           1.1847e-02, -7.9145e-03, -1.2820e-02,  4.5116e-03, -1.3666e-02,\n          -6.1617e-03,  6.9369e-03, -1.5905e-02, -2.1222e-02, -2.0794e-02,\n           6.3799e-03, -8.7952e-03,  3.8081e-03,  4.8792e-03, -1.3953e-02,\n           1.5906e-02, -1.2821e-02, -1.5098e-02,  1.1480e-02, -1.5811e-02,\n          -9.0970e-03,  1.3753e-02, -1.7108e-02, -8.2053e-03,  8.0987e-03,\n          -2.1263e-02, -1.6520e-03,  5.0387e-03, -1.2997e-02,  1.8601e-02,\n          -1.2021e-02,  3.5198e-03,  1.0363e-02, -1.8617e-02,  1.9091e-02,\n           4.8841e-03, -3.8488e-03,  1.0711e-02,  1.1413e-02,  1.8473e-02,\n           1.3143e-02, -1.2351e-02,  1.7619e-02,  2.1178e-02, -7.2509e-03,\n           1.7539e-03,  8.9103e-03,  6.8085e-03, -1.8319e-02, -6.9452e-03,\n          -9.7860e-03,  1.2727e-02,  6.6076e-03,  1.7720e-02,  1.4674e-02,\n          -1.2926e-02, -1.6900e-02, -1.1140e-02,  1.4983e-02, -1.1368e-02,\n           9.4286e-03,  1.4835e-02, -1.5095e-02,  1.0321e-02,  1.3902e-02,\n          -1.7476e-02, -1.6859e-02, -1.0545e-02, -1.1084e-02, -1.1835e-02,\n          -1.1957e-02, -8.5929e-03, -1.3127e-02, -1.3736e-02,  1.3488e-02,\n          -2.0739e-02,  2.0337e-02,  5.7911e-03, -6.8449e-03,  1.3518e-02,\n          -8.1384e-04, -1.8692e-02,  9.2392e-03,  1.1783e-02, -1.8160e-02,\n          -1.9098e-02, -1.1255e-02, -1.0805e-02,  8.0050e-03, -1.8007e-02,\n          -1.5128e-02, -2.1591e-02, -2.1274e-02, -1.1624e-02,  1.9031e-02,\n          -4.1611e-03,  1.0402e-02, -6.3048e-03, -2.2016e-03,  1.0273e-02,\n          -9.4744e-03, -1.1790e-03,  1.9104e-02,  2.1342e-02, -3.6236e-03,\n          -1.0277e-03,  1.5656e-02, -1.3345e-02, -1.1322e-03, -1.7586e-02,\n           4.8012e-03,  1.5300e-02,  4.5882e-03,  2.1683e-02, -2.1033e-02,\n          -1.5027e-02, -6.3138e-03,  2.1747e-02, -2.1445e-02,  1.0298e-02,\n          -2.2789e-03, -6.4715e-03,  1.9718e-02, -4.3231e-03, -1.1430e-02,\n          -1.5865e-04, -1.4925e-02,  1.5791e-02,  1.3199e-02, -1.4581e-05,\n          -1.4061e-02,  5.8062e-03, -3.1722e-03, -1.6711e-02, -4.6809e-04,\n          -4.7077e-03,  1.9770e-03,  2.1560e-02,  5.8819e-03,  8.6811e-03,\n          -1.3900e-02,  1.2796e-02,  1.1072e-02, -1.9871e-02, -2.1805e-02,\n          -6.9476e-03,  4.2047e-03,  1.1786e-02, -1.2602e-02,  2.0011e-02,\n          -2.0960e-02, -2.0632e-02,  1.9798e-02, -1.8500e-02,  1.4066e-02,\n           9.7893e-03, -1.4190e-02, -1.2838e-02, -8.0358e-03,  9.5744e-03,\n          -2.1932e-02,  1.7855e-02,  8.4145e-03,  1.7016e-02,  1.1687e-02,\n          -3.4172e-03,  1.6287e-02,  5.0397e-03,  1.7872e-02,  1.1479e-02,\n           2.1969e-02,  1.0821e-02, -7.9417e-03,  7.1654e-03,  2.1060e-02,\n          -8.3020e-03, -1.8638e-02, -9.3095e-04, -7.7715e-03, -8.2957e-03,\n          -1.3282e-02, -1.9109e-02, -1.2374e-02,  9.7353e-04, -8.1837e-03,\n          -1.4622e-02,  1.1454e-02,  1.0515e-02, -1.9166e-02, -6.8525e-03,\n          -1.8575e-02,  5.7142e-03,  1.4394e-03,  8.9292e-03,  5.1929e-03,\n          -1.2240e-02, -2.2635e-03,  1.3791e-02, -2.4685e-03, -7.2995e-03,\n          -1.4123e-02,  1.6297e-02, -3.3858e-03, -4.4183e-03,  1.8000e-02,\n          -1.4335e-02,  4.5018e-03,  1.5483e-02,  2.5999e-04, -1.1335e-03,\n           8.6351e-03,  7.0153e-03,  1.8897e-02, -1.5894e-02,  4.6480e-03,\n          -1.9323e-02,  1.5131e-02,  1.7955e-02,  4.6175e-03, -1.0162e-02,\n          -2.1903e-02, -1.3536e-02,  2.1272e-02,  1.2172e-02,  1.5229e-02,\n           1.5017e-02, -6.5936e-04,  9.1162e-03,  1.1969e-02,  3.7553e-03,\n           2.5052e-03, -7.1500e-03, -1.2421e-02, -7.1459e-03, -4.2322e-03,\n           1.3768e-02,  1.4648e-02,  1.5209e-02,  1.9892e-02,  7.9093e-03,\n           4.3179e-03, -1.3151e-02, -2.3440e-04,  3.3739e-03,  1.0237e-04,\n           1.8864e-02,  1.2870e-02, -9.9585e-04, -2.1991e-02, -1.4360e-02,\n           3.7434e-03, -7.6701e-03, -1.1384e-02,  1.6661e-02,  1.9187e-02,\n          -2.0348e-02, -1.4835e-02, -1.5876e-02, -1.6566e-02, -1.7361e-02,\n          -8.1709e-03, -1.3951e-02, -7.7663e-03,  1.7139e-02, -1.2510e-03,\n           2.1167e-02,  1.2276e-02,  1.5110e-02,  1.6990e-02, -1.9665e-02,\n           1.3673e-02, -9.1360e-03,  8.1295e-03,  1.7129e-02,  1.0140e-02,\n          -5.9856e-03, -7.5962e-03,  1.6340e-02, -1.9074e-02,  1.3153e-02,\n           7.5182e-03, -5.6618e-03,  1.2192e-02,  2.4147e-03,  2.1047e-02,\n          -1.1521e-02, -1.0900e-02, -2.1034e-02,  1.3398e-02, -1.0785e-02,\n           1.8369e-03,  1.5977e-02,  1.2432e-02, -7.9101e-05,  1.1572e-02,\n           1.2245e-02, -6.3671e-03, -1.4696e-02, -1.6340e-02,  1.5202e-02,\n          -1.2289e-02, -2.7069e-03, -7.0397e-03,  2.1084e-02, -8.9272e-03,\n          -1.2174e-02, -3.4679e-03,  1.3586e-02,  1.8469e-02,  1.1128e-02,\n          -1.4652e-02, -1.4174e-03, -2.5736e-03, -1.5123e-02,  8.1012e-03,\n          -2.0349e-02,  1.0656e-02, -2.0199e-02, -1.1416e-02, -9.7104e-03,\n           2.0190e-02,  1.2100e-02, -3.8040e-03,  1.1001e-02,  5.2912e-03,\n          -4.6104e-03,  1.0243e-02,  1.3352e-02,  2.1053e-02, -6.2129e-03,\n          -2.0363e-02,  1.6866e-02,  1.1901e-02,  8.1996e-03,  1.5963e-03,\n          -1.7059e-02,  1.5045e-02, -1.8559e-02,  1.3757e-02, -1.8632e-02,\n          -2.2015e-02,  2.0504e-02,  1.5887e-02, -1.2357e-02, -1.8150e-02,\n           1.1014e-02, -1.2420e-02,  1.7391e-02,  1.2971e-02,  1.7690e-02,\n           2.1935e-02,  3.9169e-03,  1.3033e-02, -1.6934e-02,  1.5721e-02,\n           1.1105e-02, -1.5463e-02,  1.5717e-02,  1.6165e-02,  1.9766e-02,\n           8.9036e-03,  5.6378e-03, -8.1526e-03, -8.4512e-03, -2.0250e-02,\n           1.2077e-02,  7.5124e-03, -2.1096e-02,  1.3698e-03, -1.3093e-02,\n           1.3279e-02,  6.6738e-03,  1.8701e-02, -6.9794e-03,  1.0992e-02,\n           1.5188e-03, -4.6861e-03,  7.2525e-03, -1.2847e-02,  1.9655e-02,\n          -1.5097e-02, -2.0911e-03, -9.8533e-03, -7.6127e-03, -9.8279e-03,\n           8.1965e-03,  8.5138e-03, -1.6099e-02, -1.5560e-02, -6.0219e-03,\n          -2.1577e-02, -1.4375e-02,  9.0273e-03, -4.5386e-03,  2.0099e-02,\n           1.0280e-02,  4.0028e-04, -1.7355e-02, -3.2309e-03,  5.0577e-03,\n          -1.9788e-02, -1.6672e-02,  9.0755e-04,  3.9617e-03, -1.6378e-02,\n           1.4650e-02,  1.9184e-02,  2.2063e-02,  3.0684e-03,  3.6840e-03,\n           1.5183e-02,  6.2177e-03, -8.0125e-03,  2.0416e-02,  6.6802e-03,\n           1.9526e-02, -7.2869e-03, -1.6115e-02,  1.6575e-02, -2.1773e-02,\n           1.4647e-02, -1.1335e-03,  1.1754e-02,  7.4279e-03,  2.1790e-02,\n           9.7781e-03, -1.1670e-02,  1.8610e-02, -9.9980e-03,  2.0379e-02,\n           2.0725e-02,  3.3085e-03,  1.8963e-02,  6.0999e-03,  9.5618e-03,\n          -1.7002e-04,  6.4194e-03, -1.9245e-02, -1.2431e-02, -2.1172e-02,\n          -1.8921e-03, -1.3325e-02, -3.3971e-03, -1.1584e-02, -1.7310e-02,\n          -6.4643e-03,  3.3318e-03, -1.2277e-02,  1.5242e-02,  1.1760e-02,\n          -1.6007e-02,  2.0333e-03, -7.5641e-03,  8.6426e-03, -1.3459e-02,\n           7.0664e-03,  1.2533e-02, -1.9227e-02, -1.3475e-02, -1.4893e-02,\n           5.8377e-03, -1.1470e-02, -3.8911e-03,  2.1943e-02, -5.3640e-03,\n          -1.2865e-02,  3.3044e-03,  2.0893e-02, -1.3921e-05,  1.5204e-02,\n           3.6257e-03,  1.7901e-02,  1.0070e-02,  6.9632e-03, -1.9174e-03,\n           1.6392e-02, -9.9775e-03,  1.8044e-02, -1.5411e-02, -1.3273e-02,\n          -1.7880e-02, -7.8289e-03,  2.2550e-03,  6.3893e-03,  3.5462e-03,\n           6.9925e-03, -2.5911e-03, -1.0612e-02,  1.4826e-02,  1.3884e-02,\n          -1.4000e-03,  1.5987e-03, -4.6452e-03, -1.1618e-02, -2.0149e-02,\n          -1.3772e-02, -1.7160e-02, -8.4254e-03, -7.1950e-03, -4.1100e-03,\n          -1.2062e-02, -1.7413e-02,  1.5985e-02, -8.4590e-03, -1.4495e-02,\n          -5.3490e-03,  8.3093e-03, -4.2371e-03,  1.6793e-02, -1.3310e-02,\n           3.6993e-03,  1.3885e-02, -1.0712e-02,  2.4818e-03,  1.4337e-02,\n           2.0618e-02, -8.4925e-03, -1.8957e-02, -1.8900e-03,  1.9504e-02,\n           1.3275e-02,  1.1105e-02, -1.4725e-02,  3.2079e-03, -1.6973e-02,\n           3.6681e-03,  7.1125e-04,  6.5027e-03, -1.7414e-02,  8.2018e-03,\n          -1.3817e-02,  1.2712e-02, -1.5307e-02, -6.0227e-03,  1.5489e-02,\n           7.1236e-03,  2.3681e-03,  6.0732e-03, -4.2783e-03,  1.2268e-02,\n          -8.3375e-03,  2.0609e-02, -8.2188e-03,  1.4331e-02,  1.7850e-02,\n           1.5566e-02,  1.8160e-03, -5.8200e-03,  1.7657e-02,  1.9558e-02,\n          -1.7277e-02,  2.1292e-02, -1.2465e-02, -1.7991e-02, -2.0505e-02,\n          -1.3503e-02, -4.1113e-03, -1.1449e-02,  2.4823e-03, -1.1202e-02,\n          -6.9809e-04, -1.8982e-02, -5.3955e-04,  9.1083e-04, -1.5664e-02,\n          -1.8429e-02,  1.0945e-02, -1.2788e-02,  4.6081e-03, -5.5316e-03,\n          -1.2816e-02,  3.8990e-03,  1.6105e-02, -5.4946e-03,  5.6768e-03,\n           1.5971e-02,  2.6038e-03, -8.8425e-03, -1.7558e-03,  9.6863e-03,\n          -1.2111e-02, -5.9780e-03,  7.3771e-03,  1.1320e-02,  2.1157e-02,\n          -9.7849e-03, -1.3291e-02,  1.4207e-02,  3.2374e-03,  2.0961e-02,\n           7.2693e-03,  5.8949e-04, -9.8841e-03,  3.5098e-03,  1.6390e-02,\n          -1.0017e-02, -3.1970e-04, -5.9094e-03, -6.4119e-03,  5.3413e-03,\n           7.1157e-03,  6.4619e-03,  1.1393e-02, -7.1464e-03,  5.1133e-03,\n           9.0641e-03,  9.6438e-03,  9.0189e-05,  1.0883e-02, -3.7932e-03,\n           2.5017e-03, -1.7825e-02, -1.4203e-02, -1.7710e-03, -7.1340e-04,\n          -4.0021e-03, -9.6363e-03, -5.5243e-03, -7.0829e-03,  1.0999e-02,\n           1.1586e-02,  2.4833e-03,  1.5734e-02,  1.1409e-02,  8.6240e-04,\n          -1.7745e-02,  1.2150e-02,  1.3793e-02,  1.9770e-02, -3.3163e-03,\n           1.5077e-02,  1.6411e-02, -4.1076e-03,  8.8523e-03, -1.1404e-02,\n          -9.1464e-03,  4.6428e-03,  3.9873e-03, -3.1982e-03,  2.0111e-02,\n          -1.5999e-02, -1.7787e-03,  3.6124e-03,  4.0629e-03, -4.5407e-03,\n           8.6281e-03, -1.3185e-02,  1.2544e-03,  5.6449e-03,  2.0068e-03,\n          -6.1967e-03,  3.6230e-03, -1.5569e-02, -9.3001e-03,  2.0333e-03,\n          -1.3977e-02, -1.3309e-02, -1.3870e-02, -4.3964e-03, -7.7488e-03,\n          -1.3162e-02,  8.4607e-03, -1.7212e-02,  2.7149e-03,  1.3080e-02,\n          -1.4455e-02, -1.9062e-02,  1.3224e-02, -4.5129e-03,  3.6081e-03,\n           2.7577e-03,  4.0550e-03, -1.6445e-02, -1.1008e-02,  1.6174e-02,\n          -7.7051e-03,  6.3677e-03, -1.3266e-02, -7.3469e-03,  1.4390e-02,\n          -3.2118e-03, -8.6737e-03,  1.5162e-02,  1.0866e-02,  6.9447e-03,\n           1.0797e-02, -1.7845e-02,  6.7007e-03, -1.9287e-02, -2.1862e-02,\n           1.9741e-02, -1.9081e-02, -1.4026e-02,  1.1471e-02,  1.7169e-02,\n          -1.9041e-02,  7.0727e-03,  2.2238e-03, -9.9619e-03,  1.3220e-02,\n          -3.2950e-04,  1.2184e-02,  1.7468e-02,  2.2058e-02, -1.5459e-03,\n           6.7954e-03, -1.2202e-02, -2.0387e-03, -1.5792e-03,  6.3441e-03,\n          -1.1300e-02, -6.2397e-03, -1.8822e-03,  1.4816e-02,  6.6225e-03,\n           9.9826e-03, -8.7119e-03,  3.0391e-03, -2.8649e-03, -1.3948e-02,\n           2.0792e-02,  1.0017e-02, -3.1149e-03, -1.7141e-02,  1.1803e-02,\n          -4.2656e-03,  1.6982e-02,  5.9842e-04, -2.0265e-02,  4.7678e-03,\n           5.1757e-03,  1.6460e-02, -1.7625e-02,  9.2040e-03,  1.4852e-02,\n          -3.9193e-03,  1.7594e-02, -9.7115e-03,  5.8997e-03, -5.5003e-03,\n          -1.6391e-02,  2.0805e-02, -1.4473e-02, -1.5498e-02, -2.1715e-02,\n          -1.8653e-02, -1.0868e-02,  3.8331e-04,  1.8159e-02, -1.8092e-02,\n          -1.1388e-02,  5.8366e-03,  1.1091e-02, -2.1615e-02, -4.4408e-03,\n          -1.0165e-02, -8.3646e-03,  1.6342e-02, -1.0834e-02,  8.9725e-03],\n         requires_grad=True))]"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(resnet50.named_parameters())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:19:28.072850900Z",
     "start_time": "2023-12-28T15:19:27.593846300Z"
    }
   },
   "id": "8559cbb60513de6f"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bn = nn.BatchNorm2d(123)\n",
    "list(bn.named_children())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:25:54.756517500Z",
     "start_time": "2023-12-28T15:25:54.287998400Z"
    }
   },
   "id": "f86c8c6a79ff7280"
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "data": {
      "text/plain": "[[4], [4], [4]]"
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = [[]] * 3\n",
    "l[0].append(4)\n",
    "l"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:28:43.916532100Z",
     "start_time": "2023-12-28T15:28:43.500455600Z"
    }
   },
   "id": "c11963e7398038cc"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "def bn_filter(module_name, module, param_name, param):\n",
    "    return isinstance(module, (nn.InstanceNorm1d, nn.InstanceNorm2d, nn.InstanceNorm3d, nn.LazyInstanceNorm1d, nn.LazyInstanceNorm2d, nn.LazyInstanceNorm3d, nn.BatchNorm1d, nn.BatchNorm2d, nn.BatchNorm3d, nn.LazyBatchNorm1d, nn.LazyBatchNorm2d, nn.LazyBatchNorm3d))\n",
    "\n",
    "def bias_filter(module_name, module, param_name, param):\n",
    "    return param_name == \"bias\"\n",
    "\n",
    "def bn_or_bias_filter(module_name, module, param_name, param):\n",
    "    return bn_filter(module_name, module, param_name, param) or bias_filter(module_name, module, param_name, param)\n",
    "\n",
    "def pass_all_filter(module_name, module, param_name, param):\n",
    "    return True\n",
    "\n",
    "def split_params(model: nn.Module, filters, prefix=\"\"):\n",
    "    results = []\n",
    "    for i in range(len(filters)):\n",
    "        results.append([])\n",
    "    for module_name, module in model.named_children():\n",
    "        full_module_name = prefix + module_name\n",
    "        for param_name, param in module.named_parameters(recurse=False):\n",
    "            for i, f in enumerate(filters):\n",
    "                if f(full_module_name, module, param_name, param):\n",
    "                    results[i].append(param)\n",
    "                    break\n",
    "        module_results = split_params(module, filters, full_module_name + \".\")\n",
    "        for i in range(len(filters)):\n",
    "            results[i] += module_results[i]\n",
    "    return results\n",
    "\n",
    "def add_weight_decay(\n",
    "        model, \n",
    "        weight_decay=1e-5):\n",
    "    params = split_params(model, [bn_or_bias_filter, pass_all_filter])\n",
    "    return [\n",
    "        {'params': params[0], 'weight_decay': 0.},\n",
    "        {'params': params[1], 'weight_decay': weight_decay}]\n",
    "\n",
    "# result = split_params(resnet50, [bn_or_bias_filter, pass_all_filter])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:41:44.195483800Z",
     "start_time": "2023-12-28T15:41:43.867304400Z"
    }
   },
   "id": "ec23306805f75690"
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "AdamW (\nParameter Group 0\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 0.0\n\nParameter Group 1\n    amsgrad: False\n    betas: (0.9, 0.999)\n    capturable: False\n    differentiable: False\n    eps: 1e-08\n    foreach: None\n    fused: None\n    lr: 0.001\n    maximize: False\n    weight_decay: 1e-05\n)"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.optim import AdamW\n",
    "\n",
    "AdamW(add_weight_decay(resnet50), lr=0.001)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:47:09.046264400Z",
     "start_time": "2023-12-28T15:47:08.624713300Z"
    }
   },
   "id": "82548b81b1582efd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "21d97eda78138f48"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['conv1.weight',\n 'layer1.0.conv1.weight',\n 'layer1.0.conv2.weight',\n 'layer1.0.conv3.weight',\n 'layer1.0.downsample.0.weight',\n 'layer1.1.conv1.weight',\n 'layer1.1.conv2.weight',\n 'layer1.1.conv3.weight',\n 'layer1.2.conv1.weight',\n 'layer1.2.conv2.weight',\n 'layer1.2.conv3.weight',\n 'layer2.0.conv1.weight',\n 'layer2.0.conv2.weight',\n 'layer2.0.conv3.weight',\n 'layer2.0.downsample.0.weight',\n 'layer2.1.conv1.weight',\n 'layer2.1.conv2.weight',\n 'layer2.1.conv3.weight',\n 'layer2.2.conv1.weight',\n 'layer2.2.conv2.weight',\n 'layer2.2.conv3.weight',\n 'layer2.3.conv1.weight',\n 'layer2.3.conv2.weight',\n 'layer2.3.conv3.weight',\n 'layer3.0.conv1.weight',\n 'layer3.0.conv2.weight',\n 'layer3.0.conv3.weight',\n 'layer3.0.downsample.0.weight',\n 'layer3.1.conv1.weight',\n 'layer3.1.conv2.weight',\n 'layer3.1.conv3.weight',\n 'layer3.2.conv1.weight',\n 'layer3.2.conv2.weight',\n 'layer3.2.conv3.weight',\n 'layer3.3.conv1.weight',\n 'layer3.3.conv2.weight',\n 'layer3.3.conv3.weight',\n 'layer3.4.conv1.weight',\n 'layer3.4.conv2.weight',\n 'layer3.4.conv3.weight',\n 'layer3.5.conv1.weight',\n 'layer3.5.conv2.weight',\n 'layer3.5.conv3.weight',\n 'layer4.0.conv1.weight',\n 'layer4.0.conv2.weight',\n 'layer4.0.conv3.weight',\n 'layer4.0.downsample.0.weight',\n 'layer4.1.conv1.weight',\n 'layer4.1.conv2.weight',\n 'layer4.1.conv3.weight',\n 'layer4.2.conv1.weight',\n 'layer4.2.conv2.weight',\n 'layer4.2.conv3.weight',\n 'fc.weight']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[m[0] for m in result[1]]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:39:12.810111100Z",
     "start_time": "2023-12-28T15:39:12.276111400Z"
    }
   },
   "id": "35ade6e993f711cc"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(resnet50.named_parameters(recurse=False))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:30:12.117268200Z",
     "start_time": "2023-12-28T15:30:11.397982700Z"
    }
   },
   "id": "1b44c44915a148e9"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "data": {
      "text/plain": "['.conv1.weight',\n '.bn1.weight',\n '.bn1.bias',\n '.layer1.0.conv1.weight',\n '.layer1.0.bn1.weight',\n '.layer1.0.bn1.bias',\n '.layer1.0.conv2.weight',\n '.layer1.0.bn2.weight',\n '.layer1.0.bn2.bias',\n '.layer1.0.conv3.weight',\n '.layer1.0.bn3.weight',\n '.layer1.0.bn3.bias',\n '.layer1.0.downsample.0.weight',\n '.layer1.0.downsample.1.weight',\n '.layer1.0.downsample.1.bias',\n '.layer1.1.conv1.weight',\n '.layer1.1.bn1.weight',\n '.layer1.1.bn1.bias',\n '.layer1.1.conv2.weight',\n '.layer1.1.bn2.weight',\n '.layer1.1.bn2.bias',\n '.layer1.1.conv3.weight',\n '.layer1.1.bn3.weight',\n '.layer1.1.bn3.bias',\n '.layer1.2.conv1.weight',\n '.layer1.2.bn1.weight',\n '.layer1.2.bn1.bias',\n '.layer1.2.conv2.weight',\n '.layer1.2.bn2.weight',\n '.layer1.2.bn2.bias',\n '.layer1.2.conv3.weight',\n '.layer1.2.bn3.weight',\n '.layer1.2.bn3.bias',\n '.layer2.0.conv1.weight',\n '.layer2.0.bn1.weight',\n '.layer2.0.bn1.bias',\n '.layer2.0.conv2.weight',\n '.layer2.0.bn2.weight',\n '.layer2.0.bn2.bias',\n '.layer2.0.conv3.weight',\n '.layer2.0.bn3.weight',\n '.layer2.0.bn3.bias',\n '.layer2.0.downsample.0.weight',\n '.layer2.0.downsample.1.weight',\n '.layer2.0.downsample.1.bias',\n '.layer2.1.conv1.weight',\n '.layer2.1.bn1.weight',\n '.layer2.1.bn1.bias',\n '.layer2.1.conv2.weight',\n '.layer2.1.bn2.weight',\n '.layer2.1.bn2.bias',\n '.layer2.1.conv3.weight',\n '.layer2.1.bn3.weight',\n '.layer2.1.bn3.bias',\n '.layer2.2.conv1.weight',\n '.layer2.2.bn1.weight',\n '.layer2.2.bn1.bias',\n '.layer2.2.conv2.weight',\n '.layer2.2.bn2.weight',\n '.layer2.2.bn2.bias',\n '.layer2.2.conv3.weight',\n '.layer2.2.bn3.weight',\n '.layer2.2.bn3.bias',\n '.layer2.3.conv1.weight',\n '.layer2.3.bn1.weight',\n '.layer2.3.bn1.bias',\n '.layer2.3.conv2.weight',\n '.layer2.3.bn2.weight',\n '.layer2.3.bn2.bias',\n '.layer2.3.conv3.weight',\n '.layer2.3.bn3.weight',\n '.layer2.3.bn3.bias',\n '.layer3.0.conv1.weight',\n '.layer3.0.bn1.weight',\n '.layer3.0.bn1.bias',\n '.layer3.0.conv2.weight',\n '.layer3.0.bn2.weight',\n '.layer3.0.bn2.bias',\n '.layer3.0.conv3.weight',\n '.layer3.0.bn3.weight',\n '.layer3.0.bn3.bias',\n '.layer3.0.downsample.0.weight',\n '.layer3.0.downsample.1.weight',\n '.layer3.0.downsample.1.bias',\n '.layer3.1.conv1.weight',\n '.layer3.1.bn1.weight',\n '.layer3.1.bn1.bias',\n '.layer3.1.conv2.weight',\n '.layer3.1.bn2.weight',\n '.layer3.1.bn2.bias',\n '.layer3.1.conv3.weight',\n '.layer3.1.bn3.weight',\n '.layer3.1.bn3.bias',\n '.layer3.2.conv1.weight',\n '.layer3.2.bn1.weight',\n '.layer3.2.bn1.bias',\n '.layer3.2.conv2.weight',\n '.layer3.2.bn2.weight',\n '.layer3.2.bn2.bias',\n '.layer3.2.conv3.weight',\n '.layer3.2.bn3.weight',\n '.layer3.2.bn3.bias',\n '.layer3.3.conv1.weight',\n '.layer3.3.bn1.weight',\n '.layer3.3.bn1.bias',\n '.layer3.3.conv2.weight',\n '.layer3.3.bn2.weight',\n '.layer3.3.bn2.bias',\n '.layer3.3.conv3.weight',\n '.layer3.3.bn3.weight',\n '.layer3.3.bn3.bias',\n '.layer3.4.conv1.weight',\n '.layer3.4.bn1.weight',\n '.layer3.4.bn1.bias',\n '.layer3.4.conv2.weight',\n '.layer3.4.bn2.weight',\n '.layer3.4.bn2.bias',\n '.layer3.4.conv3.weight',\n '.layer3.4.bn3.weight',\n '.layer3.4.bn3.bias',\n '.layer3.5.conv1.weight',\n '.layer3.5.bn1.weight',\n '.layer3.5.bn1.bias',\n '.layer3.5.conv2.weight',\n '.layer3.5.bn2.weight',\n '.layer3.5.bn2.bias',\n '.layer3.5.conv3.weight',\n '.layer3.5.bn3.weight',\n '.layer3.5.bn3.bias',\n '.layer4.0.conv1.weight',\n '.layer4.0.bn1.weight',\n '.layer4.0.bn1.bias',\n '.layer4.0.conv2.weight',\n '.layer4.0.bn2.weight',\n '.layer4.0.bn2.bias',\n '.layer4.0.conv3.weight',\n '.layer4.0.bn3.weight',\n '.layer4.0.bn3.bias',\n '.layer4.0.downsample.0.weight',\n '.layer4.0.downsample.1.weight',\n '.layer4.0.downsample.1.bias',\n '.layer4.1.conv1.weight',\n '.layer4.1.bn1.weight',\n '.layer4.1.bn1.bias',\n '.layer4.1.conv2.weight',\n '.layer4.1.bn2.weight',\n '.layer4.1.bn2.bias',\n '.layer4.1.conv3.weight',\n '.layer4.1.bn3.weight',\n '.layer4.1.bn3.bias',\n '.layer4.2.conv1.weight',\n '.layer4.2.bn1.weight',\n '.layer4.2.bn1.bias',\n '.layer4.2.conv2.weight',\n '.layer4.2.bn2.weight',\n '.layer4.2.bn2.bias',\n '.layer4.2.conv3.weight',\n '.layer4.2.bn3.weight',\n '.layer4.2.bn3.bias',\n '.fc.weight',\n '.fc.bias',\n 'conv1.weight',\n 'layer1.0.conv1.weight',\n 'layer1.0.bn1.weight',\n 'layer1.0.bn1.bias',\n 'layer1.0.conv2.weight',\n 'layer1.0.bn2.weight',\n 'layer1.0.bn2.bias',\n 'layer1.0.conv3.weight',\n 'layer1.0.bn3.weight',\n 'layer1.0.bn3.bias',\n 'layer1.0.downsample.0.weight',\n 'layer1.0.downsample.1.weight',\n 'layer1.0.downsample.1.bias',\n 'layer1.1.conv1.weight',\n 'layer1.1.bn1.weight',\n 'layer1.1.bn1.bias',\n 'layer1.1.conv2.weight',\n 'layer1.1.bn2.weight',\n 'layer1.1.bn2.bias',\n 'layer1.1.conv3.weight',\n 'layer1.1.bn3.weight',\n 'layer1.1.bn3.bias',\n 'layer1.2.conv1.weight',\n 'layer1.2.bn1.weight',\n 'layer1.2.bn1.bias',\n 'layer1.2.conv2.weight',\n 'layer1.2.bn2.weight',\n 'layer1.2.bn2.bias',\n 'layer1.2.conv3.weight',\n 'layer1.2.bn3.weight',\n 'layer1.2.bn3.bias',\n 'layer1.0.conv1.weight',\n 'layer1.0.bn1.weight',\n 'layer1.0.bn1.bias',\n 'layer1.0.conv2.weight',\n 'layer1.0.bn2.weight',\n 'layer1.0.bn2.bias',\n 'layer1.0.conv3.weight',\n 'layer1.0.bn3.weight',\n 'layer1.0.bn3.bias',\n 'layer1.0.downsample.0.weight',\n 'layer1.0.downsample.1.weight',\n 'layer1.0.downsample.1.bias',\n 'layer1.0.conv1.weight',\n 'layer1.0.conv2.weight',\n 'layer1.0.conv3.weight',\n 'layer1.0.downsample.0.weight',\n 'layer1.0.downsample.1.weight',\n 'layer1.0.downsample.1.bias',\n 'layer1.0.downsample.0.weight',\n 'layer1.1.conv1.weight',\n 'layer1.1.bn1.weight',\n 'layer1.1.bn1.bias',\n 'layer1.1.conv2.weight',\n 'layer1.1.bn2.weight',\n 'layer1.1.bn2.bias',\n 'layer1.1.conv3.weight',\n 'layer1.1.bn3.weight',\n 'layer1.1.bn3.bias',\n 'layer1.1.conv1.weight',\n 'layer1.1.conv2.weight',\n 'layer1.1.conv3.weight',\n 'layer1.2.conv1.weight',\n 'layer1.2.bn1.weight',\n 'layer1.2.bn1.bias',\n 'layer1.2.conv2.weight',\n 'layer1.2.bn2.weight',\n 'layer1.2.bn2.bias',\n 'layer1.2.conv3.weight',\n 'layer1.2.bn3.weight',\n 'layer1.2.bn3.bias',\n 'layer1.2.conv1.weight',\n 'layer1.2.conv2.weight',\n 'layer1.2.conv3.weight',\n 'layer2.0.conv1.weight',\n 'layer2.0.bn1.weight',\n 'layer2.0.bn1.bias',\n 'layer2.0.conv2.weight',\n 'layer2.0.bn2.weight',\n 'layer2.0.bn2.bias',\n 'layer2.0.conv3.weight',\n 'layer2.0.bn3.weight',\n 'layer2.0.bn3.bias',\n 'layer2.0.downsample.0.weight',\n 'layer2.0.downsample.1.weight',\n 'layer2.0.downsample.1.bias',\n 'layer2.1.conv1.weight',\n 'layer2.1.bn1.weight',\n 'layer2.1.bn1.bias',\n 'layer2.1.conv2.weight',\n 'layer2.1.bn2.weight',\n 'layer2.1.bn2.bias',\n 'layer2.1.conv3.weight',\n 'layer2.1.bn3.weight',\n 'layer2.1.bn3.bias',\n 'layer2.2.conv1.weight',\n 'layer2.2.bn1.weight',\n 'layer2.2.bn1.bias',\n 'layer2.2.conv2.weight',\n 'layer2.2.bn2.weight',\n 'layer2.2.bn2.bias',\n 'layer2.2.conv3.weight',\n 'layer2.2.bn3.weight',\n 'layer2.2.bn3.bias',\n 'layer2.3.conv1.weight',\n 'layer2.3.bn1.weight',\n 'layer2.3.bn1.bias',\n 'layer2.3.conv2.weight',\n 'layer2.3.bn2.weight',\n 'layer2.3.bn2.bias',\n 'layer2.3.conv3.weight',\n 'layer2.3.bn3.weight',\n 'layer2.3.bn3.bias',\n 'layer2.0.conv1.weight',\n 'layer2.0.bn1.weight',\n 'layer2.0.bn1.bias',\n 'layer2.0.conv2.weight',\n 'layer2.0.bn2.weight',\n 'layer2.0.bn2.bias',\n 'layer2.0.conv3.weight',\n 'layer2.0.bn3.weight',\n 'layer2.0.bn3.bias',\n 'layer2.0.downsample.0.weight',\n 'layer2.0.downsample.1.weight',\n 'layer2.0.downsample.1.bias',\n 'layer2.0.conv1.weight',\n 'layer2.0.conv2.weight',\n 'layer2.0.conv3.weight',\n 'layer2.0.downsample.0.weight',\n 'layer2.0.downsample.1.weight',\n 'layer2.0.downsample.1.bias',\n 'layer2.0.downsample.0.weight',\n 'layer2.1.conv1.weight',\n 'layer2.1.bn1.weight',\n 'layer2.1.bn1.bias',\n 'layer2.1.conv2.weight',\n 'layer2.1.bn2.weight',\n 'layer2.1.bn2.bias',\n 'layer2.1.conv3.weight',\n 'layer2.1.bn3.weight',\n 'layer2.1.bn3.bias',\n 'layer2.1.conv1.weight',\n 'layer2.1.conv2.weight',\n 'layer2.1.conv3.weight',\n 'layer2.2.conv1.weight',\n 'layer2.2.bn1.weight',\n 'layer2.2.bn1.bias',\n 'layer2.2.conv2.weight',\n 'layer2.2.bn2.weight',\n 'layer2.2.bn2.bias',\n 'layer2.2.conv3.weight',\n 'layer2.2.bn3.weight',\n 'layer2.2.bn3.bias',\n 'layer2.2.conv1.weight',\n 'layer2.2.conv2.weight',\n 'layer2.2.conv3.weight',\n 'layer2.3.conv1.weight',\n 'layer2.3.bn1.weight',\n 'layer2.3.bn1.bias',\n 'layer2.3.conv2.weight',\n 'layer2.3.bn2.weight',\n 'layer2.3.bn2.bias',\n 'layer2.3.conv3.weight',\n 'layer2.3.bn3.weight',\n 'layer2.3.bn3.bias',\n 'layer2.3.conv1.weight',\n 'layer2.3.conv2.weight',\n 'layer2.3.conv3.weight',\n 'layer3.0.conv1.weight',\n 'layer3.0.bn1.weight',\n 'layer3.0.bn1.bias',\n 'layer3.0.conv2.weight',\n 'layer3.0.bn2.weight',\n 'layer3.0.bn2.bias',\n 'layer3.0.conv3.weight',\n 'layer3.0.bn3.weight',\n 'layer3.0.bn3.bias',\n 'layer3.0.downsample.0.weight',\n 'layer3.0.downsample.1.weight',\n 'layer3.0.downsample.1.bias',\n 'layer3.1.conv1.weight',\n 'layer3.1.bn1.weight',\n 'layer3.1.bn1.bias',\n 'layer3.1.conv2.weight',\n 'layer3.1.bn2.weight',\n 'layer3.1.bn2.bias',\n 'layer3.1.conv3.weight',\n 'layer3.1.bn3.weight',\n 'layer3.1.bn3.bias',\n 'layer3.2.conv1.weight',\n 'layer3.2.bn1.weight',\n 'layer3.2.bn1.bias',\n 'layer3.2.conv2.weight',\n 'layer3.2.bn2.weight',\n 'layer3.2.bn2.bias',\n 'layer3.2.conv3.weight',\n 'layer3.2.bn3.weight',\n 'layer3.2.bn3.bias',\n 'layer3.3.conv1.weight',\n 'layer3.3.bn1.weight',\n 'layer3.3.bn1.bias',\n 'layer3.3.conv2.weight',\n 'layer3.3.bn2.weight',\n 'layer3.3.bn2.bias',\n 'layer3.3.conv3.weight',\n 'layer3.3.bn3.weight',\n 'layer3.3.bn3.bias',\n 'layer3.4.conv1.weight',\n 'layer3.4.bn1.weight',\n 'layer3.4.bn1.bias',\n 'layer3.4.conv2.weight',\n 'layer3.4.bn2.weight',\n 'layer3.4.bn2.bias',\n 'layer3.4.conv3.weight',\n 'layer3.4.bn3.weight',\n 'layer3.4.bn3.bias',\n 'layer3.5.conv1.weight',\n 'layer3.5.bn1.weight',\n 'layer3.5.bn1.bias',\n 'layer3.5.conv2.weight',\n 'layer3.5.bn2.weight',\n 'layer3.5.bn2.bias',\n 'layer3.5.conv3.weight',\n 'layer3.5.bn3.weight',\n 'layer3.5.bn3.bias',\n 'layer3.0.conv1.weight',\n 'layer3.0.bn1.weight',\n 'layer3.0.bn1.bias',\n 'layer3.0.conv2.weight',\n 'layer3.0.bn2.weight',\n 'layer3.0.bn2.bias',\n 'layer3.0.conv3.weight',\n 'layer3.0.bn3.weight',\n 'layer3.0.bn3.bias',\n 'layer3.0.downsample.0.weight',\n 'layer3.0.downsample.1.weight',\n 'layer3.0.downsample.1.bias',\n 'layer3.0.conv1.weight',\n 'layer3.0.conv2.weight',\n 'layer3.0.conv3.weight',\n 'layer3.0.downsample.0.weight',\n 'layer3.0.downsample.1.weight',\n 'layer3.0.downsample.1.bias',\n 'layer3.0.downsample.0.weight',\n 'layer3.1.conv1.weight',\n 'layer3.1.bn1.weight',\n 'layer3.1.bn1.bias',\n 'layer3.1.conv2.weight',\n 'layer3.1.bn2.weight',\n 'layer3.1.bn2.bias',\n 'layer3.1.conv3.weight',\n 'layer3.1.bn3.weight',\n 'layer3.1.bn3.bias',\n 'layer3.1.conv1.weight',\n 'layer3.1.conv2.weight',\n 'layer3.1.conv3.weight',\n 'layer3.2.conv1.weight',\n 'layer3.2.bn1.weight',\n 'layer3.2.bn1.bias',\n 'layer3.2.conv2.weight',\n 'layer3.2.bn2.weight',\n 'layer3.2.bn2.bias',\n 'layer3.2.conv3.weight',\n 'layer3.2.bn3.weight',\n 'layer3.2.bn3.bias',\n 'layer3.2.conv1.weight',\n 'layer3.2.conv2.weight',\n 'layer3.2.conv3.weight',\n 'layer3.3.conv1.weight',\n 'layer3.3.bn1.weight',\n 'layer3.3.bn1.bias',\n 'layer3.3.conv2.weight',\n 'layer3.3.bn2.weight',\n 'layer3.3.bn2.bias',\n 'layer3.3.conv3.weight',\n 'layer3.3.bn3.weight',\n 'layer3.3.bn3.bias',\n 'layer3.3.conv1.weight',\n 'layer3.3.conv2.weight',\n 'layer3.3.conv3.weight',\n 'layer3.4.conv1.weight',\n 'layer3.4.bn1.weight',\n 'layer3.4.bn1.bias',\n 'layer3.4.conv2.weight',\n 'layer3.4.bn2.weight',\n 'layer3.4.bn2.bias',\n 'layer3.4.conv3.weight',\n 'layer3.4.bn3.weight',\n 'layer3.4.bn3.bias',\n 'layer3.4.conv1.weight',\n 'layer3.4.conv2.weight',\n 'layer3.4.conv3.weight',\n 'layer3.5.conv1.weight',\n 'layer3.5.bn1.weight',\n 'layer3.5.bn1.bias',\n 'layer3.5.conv2.weight',\n 'layer3.5.bn2.weight',\n 'layer3.5.bn2.bias',\n 'layer3.5.conv3.weight',\n 'layer3.5.bn3.weight',\n 'layer3.5.bn3.bias',\n 'layer3.5.conv1.weight',\n 'layer3.5.conv2.weight',\n 'layer3.5.conv3.weight',\n 'layer4.0.conv1.weight',\n 'layer4.0.bn1.weight',\n 'layer4.0.bn1.bias',\n 'layer4.0.conv2.weight',\n 'layer4.0.bn2.weight',\n 'layer4.0.bn2.bias',\n 'layer4.0.conv3.weight',\n 'layer4.0.bn3.weight',\n 'layer4.0.bn3.bias',\n 'layer4.0.downsample.0.weight',\n 'layer4.0.downsample.1.weight',\n 'layer4.0.downsample.1.bias',\n 'layer4.1.conv1.weight',\n 'layer4.1.bn1.weight',\n 'layer4.1.bn1.bias',\n 'layer4.1.conv2.weight',\n 'layer4.1.bn2.weight',\n 'layer4.1.bn2.bias',\n 'layer4.1.conv3.weight',\n 'layer4.1.bn3.weight',\n 'layer4.1.bn3.bias',\n 'layer4.2.conv1.weight',\n 'layer4.2.bn1.weight',\n 'layer4.2.bn1.bias',\n 'layer4.2.conv2.weight',\n 'layer4.2.bn2.weight',\n 'layer4.2.bn2.bias',\n 'layer4.2.conv3.weight',\n 'layer4.2.bn3.weight',\n 'layer4.2.bn3.bias',\n 'layer4.0.conv1.weight',\n 'layer4.0.bn1.weight',\n 'layer4.0.bn1.bias',\n 'layer4.0.conv2.weight',\n 'layer4.0.bn2.weight',\n 'layer4.0.bn2.bias',\n 'layer4.0.conv3.weight',\n 'layer4.0.bn3.weight',\n 'layer4.0.bn3.bias',\n 'layer4.0.downsample.0.weight',\n 'layer4.0.downsample.1.weight',\n 'layer4.0.downsample.1.bias',\n 'layer4.0.conv1.weight',\n 'layer4.0.conv2.weight',\n 'layer4.0.conv3.weight',\n 'layer4.0.downsample.0.weight',\n 'layer4.0.downsample.1.weight',\n 'layer4.0.downsample.1.bias',\n 'layer4.0.downsample.0.weight',\n 'layer4.1.conv1.weight',\n 'layer4.1.bn1.weight',\n 'layer4.1.bn1.bias',\n 'layer4.1.conv2.weight',\n 'layer4.1.bn2.weight',\n 'layer4.1.bn2.bias',\n 'layer4.1.conv3.weight',\n 'layer4.1.bn3.weight',\n 'layer4.1.bn3.bias',\n 'layer4.1.conv1.weight',\n 'layer4.1.conv2.weight',\n 'layer4.1.conv3.weight',\n 'layer4.2.conv1.weight',\n 'layer4.2.bn1.weight',\n 'layer4.2.bn1.bias',\n 'layer4.2.conv2.weight',\n 'layer4.2.bn2.weight',\n 'layer4.2.bn2.bias',\n 'layer4.2.conv3.weight',\n 'layer4.2.bn3.weight',\n 'layer4.2.bn3.bias',\n 'layer4.2.conv1.weight',\n 'layer4.2.conv2.weight',\n 'layer4.2.conv3.weight',\n 'fc.weight',\n 'fc.bias']"
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result[1][\"param_names\"]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T15:25:01.278893700Z",
     "start_time": "2023-12-28T15:25:01.237707400Z"
    }
   },
   "id": "fe9f389eac666f"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor(-2.)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(12, 100)\n",
    "y = x * 10\n",
    "x_norm = torch.linalg.norm(x, ord=2, dim=1)\n",
    "y_norm = torch.linalg.norm(y, ord=2, dim=1)\n",
    "-2 * torch.mean(torch.sum(x * y, dim=1) / (x_norm * y_norm))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T19:03:58.838708200Z",
     "start_time": "2023-12-28T19:03:57.001062300Z"
    }
   },
   "id": "15dd58c60e3c864d"
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train step 0142/2356 - loss 4.558164:   0%|          | 0/10 [00:25<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[31], line 150\u001B[0m\n\u001B[0;32m    147\u001B[0m resnet50 \u001B[38;5;241m=\u001B[39m timm\u001B[38;5;241m.\u001B[39mcreate_model(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresnet50\u001B[39m\u001B[38;5;124m\"\u001B[39m, pretrained\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    148\u001B[0m backbone \u001B[38;5;241m=\u001B[39m _Backbone(resnet50)\n\u001B[1;32m--> 150\u001B[0m \u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mbackbone\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[31], line 66\u001B[0m, in \u001B[0;36mClassificationCallback.eval_model\u001B[1;34m(self, backbone, backbone_output_size)\u001B[0m\n\u001B[0;32m     64\u001B[0m opt\u001B[38;5;241m.\u001B[39mzero_grad()\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m---> 66\u001B[0m     features \u001B[38;5;241m=\u001B[39m \u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mimage\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     67\u001B[0m logits \u001B[38;5;241m=\u001B[39m fc(features)\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtclip:\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "Cell \u001B[1;32mIn[31], line 143\u001B[0m, in \u001B[0;36m_Backbone.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    142\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[1;32m--> 143\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbackbone\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mforward_features\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    144\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbackbone\u001B[38;5;241m.\u001B[39mglobal_pool(x)\n\u001B[0;32m    145\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\timm\\models\\resnet.py:568\u001B[0m, in \u001B[0;36mResNet.forward_features\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    566\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer2(x)\n\u001B[0;32m    567\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mlayer3(x)\n\u001B[1;32m--> 568\u001B[0m     x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlayer4\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    569\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m x\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001B[0m, in \u001B[0;36mSequential.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    213\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m):\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m:\n\u001B[1;32m--> 215\u001B[0m         \u001B[38;5;28minput\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[43mmodule\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28minput\u001B[39m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\timm\\models\\resnet.py:227\u001B[0m, in \u001B[0;36mBottleneck.forward\u001B[1;34m(self, x)\u001B[0m\n\u001B[0;32m    224\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mact2(x)\n\u001B[0;32m    225\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maa(x)\n\u001B[1;32m--> 227\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv3\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    228\u001B[0m x \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbn3(x)\n\u001B[0;32m    230\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mse \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\conv.py:460\u001B[0m, in \u001B[0;36mConv2d.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    459\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 460\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_conv_forward\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001B[0m, in \u001B[0;36mConv2d._conv_forward\u001B[1;34m(self, input, weight, bias)\u001B[0m\n\u001B[0;32m    452\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mzeros\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    453\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m F\u001B[38;5;241m.\u001B[39mconv2d(F\u001B[38;5;241m.\u001B[39mpad(\u001B[38;5;28minput\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_reversed_padding_repeated_twice, mode\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpadding_mode),\n\u001B[0;32m    454\u001B[0m                     weight, bias, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mstride,\n\u001B[0;32m    455\u001B[0m                     _pair(\u001B[38;5;241m0\u001B[39m), \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdilation, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgroups)\n\u001B[1;32m--> 456\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mconv2d\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbias\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstride\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    457\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpadding\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdilation\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgroups\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from typing import List\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim\n",
    "import torchmetrics\n",
    "from tqdm import tqdm\n",
    "\n",
    "import models\n",
    "import utils\n",
    "from datamodules import Task1Datamodule\n",
    "\n",
    "\n",
    "class ClassificationCallback(pl.Callback):\n",
    "    def __init__(\n",
    "        self,\n",
    "        datamodule: Task1Datamodule,\n",
    "        num_epochs: int = 80,\n",
    "        lr: List[float] = None,\n",
    "        tclip: bool = True,\n",
    "        tclip_alpha: float = 10.0,\n",
    "        weight_decay: float = 1e-2,\n",
    "        early_stopping: int = 10,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        if lr is None:\n",
    "            lr = [1e-3]\n",
    "\n",
    "        self.datamodule = datamodule\n",
    "        self.num_epochs = num_epochs\n",
    "        self.lr = lr\n",
    "        self.tclip = tclip\n",
    "        self.tclip_alpha = tclip_alpha\n",
    "        self.weight_decay = weight_decay\n",
    "        self.early_stopping = early_stopping\n",
    "\n",
    "    def eval_model(self, backbone: nn.Module, backbone_output_size: int):\n",
    "        acc = torchmetrics.Accuracy(\"multiclass\", num_classes=models.NUM_CLASSES)\n",
    "        ap = torchmetrics.AveragePrecision(\"multiclass\", num_classes=models.NUM_CLASSES)\n",
    "        best_loss = -1000000\n",
    "        best_lr = None\n",
    "        best_epoch = None\n",
    "        best_acc = None\n",
    "        best_ap = None\n",
    "        for lr in self.lr:\n",
    "            fc = nn.Linear(backbone_output_size, models.NUM_CLASSES)\n",
    "            opt = torch.optim.AdamW(\n",
    "                utils.add_weight_decay(fc, self.weight_decay), lr=lr\n",
    "            )\n",
    "            best_lr_loss = -1000000\n",
    "            best_lr_epoch = -1\n",
    "\n",
    "            train_dataloader = self.datamodule.train_dataloader()\n",
    "            val_dataloader = self.datamodule.val_dataloader()\n",
    "            if isinstance(val_dataloader, list):\n",
    "                assert len(val_dataloader) == 1\n",
    "                val_dataloader = val_dataloader[0]\n",
    "            tq = tqdm(range(self.num_epochs))\n",
    "            for epoch in tq:\n",
    "                num_batches = 0\n",
    "                for data, labels in train_dataloader:\n",
    "                    num_batches += 1\n",
    "                    opt.zero_grad()\n",
    "                    with torch.no_grad():\n",
    "                        features = backbone(data[\"image\"])\n",
    "                    logits = fc(features)\n",
    "                    if self.tclip:\n",
    "                        logits = self.tclip_alpha * torch.tanh(\n",
    "                            logits / self.tclip_alpha\n",
    "                        )\n",
    "                    loss = F.cross_entropy(logits, labels)\n",
    "                    loss.backward()\n",
    "                    opt.step()\n",
    "                    tq.set_description(\n",
    "                        f\"Train step {num_batches:04d}/{len(train_dataloader):04d} - loss {loss:.6f}\"\n",
    "                    )\n",
    "\n",
    "                loss = 0\n",
    "                num_batches = 0\n",
    "                for data, labels in val_dataloader:\n",
    "                    num_batches += 1\n",
    "                    with torch.no_grad():\n",
    "                        features = backbone(data[\"image\"])\n",
    "                        logits = fc(features)\n",
    "                        if self.tclip:\n",
    "                            logits = self.tclip_alpha * torch.tanh(\n",
    "                                logits / self.tclip_alpha\n",
    "                            )\n",
    "                        loss += F.cross_entropy(logits, labels)\n",
    "                        acc.update(logits, labels)\n",
    "                        ap.update(logits, labels)\n",
    "                    tq.set_description(\n",
    "                        f\"Eval step {num_batches:04d}/{len(val_dataloader):04d} - loss {loss:.6f}\"\n",
    "                    )\n",
    "                val_loss = loss / num_batches\n",
    "                val_acc = acc.compute()\n",
    "                val_ap = ap.compute()\n",
    "                acc.reset()\n",
    "                ap.reset()\n",
    "                if val_loss < best_loss:\n",
    "                    best_loss = val_loss\n",
    "                    best_lr = lr\n",
    "                    best_epoch = epoch\n",
    "                    best_acc = val_acc\n",
    "                    best_ap = val_ap\n",
    "                if val_loss < best_lr_loss:\n",
    "                    best_lr_loss = val_loss\n",
    "                    best_lr_epoch = epoch\n",
    "                elif epoch - best_lr_epoch > self.early_stopping:\n",
    "                    break\n",
    "        return {\n",
    "            \"loss\": best_loss,\n",
    "            \"lr\": best_lr,\n",
    "            \"epoch\": best_epoch,\n",
    "            \"acc\": best_acc,\n",
    "            \"ap\": best_ap,\n",
    "        }\n",
    "\n",
    "    def on_validation_epoch_end(\n",
    "        self, trainer: pl.Trainer, pl_module: pl.LightningModule\n",
    "    ):\n",
    "        backbone = nn.Sequential(pl_module.online_backbone, pl_module.global_pool)\n",
    "\n",
    "        logs = self.eval_model(backbone, pl_module.hparams.mlp_out_size)\n",
    "\n",
    "        pl_module.log_dict({f\"classification/{k}\": v for k, v in logs.items()})\n",
    "        print(f\"Classification {logs}\")\n",
    "\n",
    "import datamodules\n",
    "\n",
    "datamodule = datamodules.Task1Datamodule(\"C:/Data/AAIT/task1\", num_train_workers=0, num_val_workers=0, num_test_workers=0, batch_size=64, labeled=True, unlabeled=False, val_size=0.2, train_dataset_replicas=2)\n",
    "datamodule.setup(\"fit\")\n",
    "\n",
    "callback = ClassificationCallback(datamodule, num_epochs=10, lr=[0.001], tclip=True, tclip_alpha=10., weight_decay=1e-2, early_stopping=10)\n",
    "\n",
    "class _Backbone(nn.Module):\n",
    "    def __init__(self, model: nn.Module):\n",
    "        super().__init__()\n",
    "        self.backbone = model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone.forward_features(x)\n",
    "        x = self.backbone.global_pool(x)\n",
    "        return x\n",
    "\n",
    "resnet50 = timm.create_model(\"resnet50\", pretrained=True)\n",
    "backbone = _Backbone(resnet50)\n",
    "\n",
    "callback.eval_model(backbone, 2048)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T20:07:48.573508100Z",
     "start_time": "2023-12-28T20:07:19.413410700Z"
    }
   },
   "id": "61652c228373ab5e"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (16x1000 and 2048x100)",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[25], line 20\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m x\n\u001B[0;32m     18\u001B[0m backbone \u001B[38;5;241m=\u001B[39m _Backbone(resnet50)\n\u001B[1;32m---> 20\u001B[0m \u001B[43mcallback\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43meval_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresnet50\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m2048\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[24], line 67\u001B[0m, in \u001B[0;36mClassificationCallback.eval_model\u001B[1;34m(self, backbone, backbone_output_size)\u001B[0m\n\u001B[0;32m     65\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[0;32m     66\u001B[0m     features \u001B[38;5;241m=\u001B[39m backbone(data[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mimage\u001B[39m\u001B[38;5;124m\"\u001B[39m])\n\u001B[1;32m---> 67\u001B[0m logits \u001B[38;5;241m=\u001B[39m \u001B[43mfc\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfeatures\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     68\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtclip:\n\u001B[0;32m     69\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtclip_alpha \u001B[38;5;241m*\u001B[39m torch\u001B[38;5;241m.\u001B[39mtanh(\n\u001B[0;32m     70\u001B[0m         features \u001B[38;5;241m/\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mtclip_alpha\n\u001B[0;32m     71\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001B[0m, in \u001B[0;36mModule._wrapped_call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1516\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_compiled_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[0;32m   1517\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m-> 1518\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call_impl(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001B[0m, in \u001B[0;36mModule._call_impl\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1522\u001B[0m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[0;32m   1523\u001B[0m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[0;32m   1524\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_forward_pre_hooks\n\u001B[0;32m   1525\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[0;32m   1526\u001B[0m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[1;32m-> 1527\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m forward_call(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   1529\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m   1530\u001B[0m     result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001B[0m, in \u001B[0;36mLinear.forward\u001B[1;34m(self, input)\u001B[0m\n\u001B[0;32m    113\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, \u001B[38;5;28minput\u001B[39m: Tensor) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Tensor:\n\u001B[1;32m--> 114\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mF\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mlinear\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43minput\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mweight\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mbias\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[1;31mRuntimeError\u001B[0m: mat1 and mat2 shapes cannot be multiplied (16x1000 and 2048x100)"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-12-28T20:01:46.273059Z",
     "start_time": "2023-12-28T20:01:43.201449700Z"
    }
   },
   "id": "4ba638c352ebdf67"
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.neighbors import KNeighborsClassifier\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "n = 23555\n",
    "m = 2048\n",
    "X = np.random.randn(n, m) * 10 + np.random.randn(n, m) * 5 + 8\n",
    "y = np.random.randint(0, 100, (n,)).astype(int)\n",
    "# y_sampled = y\n",
    "# y_binarized = np.zeros((y.size, 100))\n",
    "# y_binarized[np.arange(y.size), y] = 1\n",
    "# y = y_binarized\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:37:20.354795400Z",
     "start_time": "2024-01-03T20:37:17.279227500Z"
    }
   },
   "id": "99ed0691fab4f91c"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "labels = np.arange(100)\n",
    "def train_test(algo, pca_n_comp=2048, knn_n_neighbors=20):\n",
    "    global X_train, X_test, y_train, y_test, labels\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_transformed = scaler.fit_transform(X_train)\n",
    "    X_test_transformed = scaler.transform(X_test)\n",
    "    \n",
    "    if pca_n_comp != X_train.shape[1]:\n",
    "        pca = PCA(n_components=pca_n_comp)\n",
    "        X_train_transformed = pca.fit_transform(X_train_transformed)\n",
    "        X_test_transformed = pca.transform(X_test_transformed)\n",
    "    \n",
    "    if algo == \"knn\":\n",
    "        knn = KNeighborsClassifier(n_neighbors=knn_n_neighbors)\n",
    "        knn.fit(X_train_transformed, y_train)\n",
    "        y_proba = knn.predict_proba(X_test_transformed)\n",
    "    elif algo == \"randomforest\":\n",
    "        model = RandomForestClassifier(class_weight=\"balanced_subsample\")\n",
    "        model.fit(X_train_transformed, y_train)\n",
    "        y_proba = model.predict_proba(X_test_transformed)\n",
    "    elif algo == \"xgb\":\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception(\"unknown algo\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T18:10:04.526256500Z",
     "start_time": "2024-01-03T18:10:04.523278200Z"
    }
   },
   "id": "558c25ea3947cb25"
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.2 s ± 609 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit train_test(algo=\"knn\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:49:15.420583700Z",
     "start_time": "2024-01-03T17:48:42.059890600Z"
    }
   },
   "id": "a7e39bb57430e422"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%timeit train_test(algo=\"randomforest\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-01-03T17:49:55.511538400Z"
    }
   },
   "id": "3c5b0fd5692135c2"
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(class_weight=\"balanced_subsample\", n_jobs=12)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T18:10:11.248599900Z",
     "start_time": "2024-01-03T18:10:11.231520800Z"
    }
   },
   "id": "7cc4dfa0b18e9bdb"
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "RandomForestClassifier(class_weight='balanced_subsample', n_jobs=12)",
      "text/html": "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(class_weight=&#x27;balanced_subsample&#x27;, n_jobs=12)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(class_weight=&#x27;balanced_subsample&#x27;, n_jobs=12)</pre></div></div></div></div></div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_train_transformed, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T18:13:27.909838700Z",
     "start_time": "2024-01-03T18:10:12.548518300Z"
    }
   },
   "id": "bf21f4a76010466b"
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "y_proba = model.predict_proba(X_test_transformed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T18:13:28.195837300Z",
     "start_time": "2024-01-03T18:13:27.913838100Z"
    }
   },
   "id": "f6c00d7d1d06d5e8"
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "from xgboost.sklearn import XGBClassifier\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_transformed = scaler.fit_transform(X_train)\n",
    "X_test_transformed = scaler.transform(X_test)\n",
    "\n",
    "pca = PCA(n_components=256)\n",
    "X_train_transformed = pca.fit_transform(X_train_transformed)\n",
    "X_test_transformed = pca.transform(X_test_transformed)\n",
    "\n",
    "sample_weights = compute_sample_weight(class_weight='balanced', y=y_train)\n",
    "xgb = XGBClassifier(early_stopping_rounds=10, tree_method=\"hist\", max_depth=4, n_estimators=100, device=\"cuda\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:37:37.301640900Z",
     "start_time": "2024-01-03T20:37:30.170418100Z"
    }
   },
   "id": "657de4bff7b623b3"
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mlogloss:4.61183\n",
      "[1]\tvalidation_0-mlogloss:4.61640\n",
      "[2]\tvalidation_0-mlogloss:4.62094\n",
      "[3]\tvalidation_0-mlogloss:4.62685\n",
      "[4]\tvalidation_0-mlogloss:4.63390\n",
      "[5]\tvalidation_0-mlogloss:4.63917\n",
      "[6]\tvalidation_0-mlogloss:4.64556\n",
      "[7]\tvalidation_0-mlogloss:4.65126\n",
      "[8]\tvalidation_0-mlogloss:4.65507\n",
      "[9]\tvalidation_0-mlogloss:4.66272\n"
     ]
    },
    {
     "data": {
      "text/plain": "XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device='cuda', early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=4, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, objective='multi:softprob', ...)",
      "text/html": "<style>#sk-container-id-8 {color: black;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=&#x27;cuda&#x27;, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=4, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n              colsample_bylevel=None, colsample_bynode=None,\n              colsample_bytree=None, device=&#x27;cuda&#x27;, early_stopping_rounds=10,\n              enable_categorical=False, eval_metric=None, feature_types=None,\n              gamma=None, grow_policy=None, importance_type=None,\n              interaction_constraints=None, learning_rate=None, max_bin=None,\n              max_cat_threshold=None, max_cat_to_onehot=None,\n              max_delta_step=None, max_depth=4, max_leaves=None,\n              min_child_weight=None, missing=nan, monotone_constraints=None,\n              multi_strategy=None, n_estimators=100, n_jobs=None,\n              num_parallel_tree=None, objective=&#x27;multi:softprob&#x27;, ...)</pre></div></div></div></div></div>"
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.fit(X_train_transformed, y_train, eval_set=[(X_test_transformed, y_test)], verbose=True, sample_weight=sample_weights)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:37:48.219437200Z",
     "start_time": "2024-01-03T20:37:37.304644400Z"
    }
   },
   "id": "24a50c88d1f55549"
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "(4711, 100)"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba = xgb.predict_proba(X_test_transformed)\n",
    "y_proba.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:44:17.644611Z",
     "start_time": "2024-01-03T20:44:16.762541200Z"
    }
   },
   "id": "1c0d02caf4a7417e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xgboost import DMatrix\n",
    "\n",
    "DMatrix()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "48a3f4574bbefe42"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "['T_destination',\n '__annotations__',\n '__call__',\n '__class__',\n '__delattr__',\n '__dict__',\n '__dir__',\n '__doc__',\n '__eq__',\n '__format__',\n '__ge__',\n '__getattr__',\n '__getattribute__',\n '__getstate__',\n '__gt__',\n '__hash__',\n '__init__',\n '__init_subclass__',\n '__le__',\n '__lt__',\n '__module__',\n '__ne__',\n '__new__',\n '__reduce__',\n '__reduce_ex__',\n '__repr__',\n '__setattr__',\n '__setstate__',\n '__sizeof__',\n '__str__',\n '__subclasshook__',\n '__weakref__',\n '_apply',\n '_backward_hooks',\n '_backward_pre_hooks',\n '_buffers',\n '_call_impl',\n '_compiled_call_impl',\n '_forward_hooks',\n '_forward_hooks_always_called',\n '_forward_hooks_with_kwargs',\n '_forward_pre_hooks',\n '_forward_pre_hooks_with_kwargs',\n '_get_backward_hooks',\n '_get_backward_pre_hooks',\n '_get_name',\n '_is_full_backward_hook',\n '_load_from_state_dict',\n '_load_state_dict_post_hooks',\n '_load_state_dict_pre_hooks',\n '_maybe_warn_non_full_backward_hook',\n '_modules',\n '_named_members',\n '_non_persistent_buffers_set',\n '_parameters',\n '_register_load_state_dict_pre_hook',\n '_register_state_dict_hook',\n '_replicate_for_data_parallel',\n '_save_to_state_dict',\n '_slow_forward',\n '_state_dict_hooks',\n '_state_dict_pre_hooks',\n '_version',\n '_wrapped_call_impl',\n 'act1',\n 'add_module',\n 'apply',\n 'bfloat16',\n 'bn1',\n 'buffers',\n 'call_super_init',\n 'children',\n 'compile',\n 'conv1',\n 'cpu',\n 'cuda',\n 'default_cfg',\n 'double',\n 'drop_rate',\n 'dump_patches',\n 'eval',\n 'extra_repr',\n 'fc',\n 'feature_info',\n 'float',\n 'forward',\n 'forward_features',\n 'forward_head',\n 'get_buffer',\n 'get_classifier',\n 'get_extra_state',\n 'get_parameter',\n 'get_submodule',\n 'global_pool',\n 'grad_checkpointing',\n 'group_matcher',\n 'half',\n 'init_weights',\n 'ipu',\n 'layer1',\n 'layer2',\n 'layer3',\n 'layer4',\n 'load_state_dict',\n 'maxpool',\n 'modules',\n 'named_buffers',\n 'named_children',\n 'named_modules',\n 'named_parameters',\n 'num_classes',\n 'num_features',\n 'parameters',\n 'pretrained_cfg',\n 'register_backward_hook',\n 'register_buffer',\n 'register_forward_hook',\n 'register_forward_pre_hook',\n 'register_full_backward_hook',\n 'register_full_backward_pre_hook',\n 'register_load_state_dict_post_hook',\n 'register_module',\n 'register_parameter',\n 'register_state_dict_pre_hook',\n 'requires_grad_',\n 'reset_classifier',\n 'set_extra_state',\n 'set_grad_checkpointing',\n 'share_memory',\n 'state_dict',\n 'to',\n 'to_empty',\n 'train',\n 'training',\n 'type',\n 'xpu',\n 'zero_grad']"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import timm\n",
    "\n",
    "m = timm.create_model(\"resnet34\", pretrained=False)\n",
    "dir(m)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:09:39.184103200Z",
     "start_time": "2024-01-03T20:09:37.840851400Z"
    }
   },
   "id": "3c323215df105d95"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'ResNet' object has no attribute 'device'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[70], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\torch\\nn\\modules\\module.py:1695\u001B[0m, in \u001B[0;36mModule.__getattr__\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   1693\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m name \u001B[38;5;129;01min\u001B[39;00m modules:\n\u001B[0;32m   1694\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m modules[name]\n\u001B[1;32m-> 1695\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mAttributeError\u001B[39;00m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(\u001B[38;5;28mself\u001B[39m)\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m object has no attribute \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mname\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'ResNet' object has no attribute 'device'"
     ]
    }
   ],
   "source": [
    "m.device"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T20:09:47.493697Z",
     "start_time": "2024-01-03T20:09:46.350697200Z"
    }
   },
   "id": "6225b924eace6e9c"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0.01011109, 0.00971837, 0.00985839, ..., 0.00944635, 0.01012211,\n        0.00957291],\n       [0.00997305, 0.00958568, 0.00972379, ..., 0.00931739, 0.00964165,\n        0.0103318 ],\n       [0.01025263, 0.0098544 , 0.00999638, ..., 0.00957858, 0.00991193,\n        0.00970691],\n       ...,\n       [0.01017627, 0.00978102, 0.00992194, ..., 0.00950725, 0.00903583,\n        0.00963462],\n       [0.01016838, 0.00977342, 0.00991424, ..., 0.00949987, 0.00983048,\n        0.00962714],\n       [0.01022303, 0.00982595, 0.00996752, ..., 0.01263243, 0.00988332,\n        0.00967888]], dtype=float32)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb.predict_proba(X_test_transformed)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T18:24:45.446062700Z",
     "start_time": "2024-01-03T18:24:44.654997Z"
    }
   },
   "id": "6ebee0aa37f62d76"
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       ...,\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.],\n       [0., 0., 0., ..., 0., 0., 0.]])"
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "knn.fit(X_train, y_train)\n",
    "y_proba = knn.predict(X_test)\n",
    "y_proba"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:44:44.195782500Z",
     "start_time": "2024-01-03T17:44:40.631226300Z"
    }
   },
   "id": "1a617590c70fb537"
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0., 1.])"
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_proba)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:44:54.787316800Z",
     "start_time": "2024-01-03T17:44:54.280781600Z"
    }
   },
   "id": "5a89acb0f78d43bd"
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "outputs": [
    {
     "data": {
      "text/plain": "(4711, 199)"
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.concatenate(y_proba, axis=1).shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:41:36.195819300Z",
     "start_time": "2024-01-03T17:41:36.140716800Z"
    }
   },
   "id": "9ccee69ff745720b"
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "outputs": [
    {
     "data": {
      "text/plain": "[(4711, 1),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2),\n (4711, 2)]"
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[a.shape for a in y_proba]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:41:36.219745800Z",
     "start_time": "2024-01-03T17:41:36.153715700Z"
    }
   },
   "id": "9bed6461b363d42c"
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1.],\n       [1.],\n       [1.],\n       ...,\n       [1.],\n       [1.],\n       [1.]])"
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:41:49.197773900Z",
     "start_time": "2024-01-03T17:41:48.244710200Z"
    }
   },
   "id": "34b441a0fcfc2674"
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[1. , 0. ],\n       [0.8, 0.2],\n       [1. , 0. ],\n       ...,\n       [1. , 0. ],\n       [1. , 0. ],\n       [1. , 0. ]])"
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_proba[16]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:42:36.318791Z",
     "start_time": "2024-01-03T17:42:35.765660800Z"
    }
   },
   "id": "46314b91cc007fae"
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[113], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtimeit\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain_test(algo=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mknn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2417\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[1;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[0;32m   2415\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[0;32m   2416\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m-> 2417\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2419\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[0;32m   2420\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[0;32m   2421\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[0;32m   2422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\magics\\execution.py:1170\u001B[0m, in \u001B[0;36mExecutionMagics.timeit\u001B[1;34m(self, line, cell, local_ns)\u001B[0m\n\u001B[0;32m   1168\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m   1169\u001B[0m     number \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m index\n\u001B[1;32m-> 1170\u001B[0m     time_number \u001B[38;5;241m=\u001B[39m \u001B[43mtimer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1171\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m time_number \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m:\n\u001B[0;32m   1172\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\magics\\execution.py:158\u001B[0m, in \u001B[0;36mTimer.timeit\u001B[1;34m(self, number)\u001B[0m\n\u001B[0;32m    156\u001B[0m gc\u001B[38;5;241m.\u001B[39mdisable()\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 158\u001B[0m     timing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gcold:\n",
      "File \u001B[1;32m<magic-timeit>:1\u001B[0m, in \u001B[0;36minner\u001B[1;34m(_it, _timer)\u001B[0m\n",
      "Cell \u001B[1;32mIn[112], line 35\u001B[0m, in \u001B[0;36mtrain_test\u001B[1;34m(algo, pca_n_comp, knn_n_neighbors)\u001B[0m\n\u001B[0;32m     28\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124munknown algo\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m     29\u001B[0m \u001B[38;5;66;03m# missing_labels = np.sort(np.array(list(set(range(100)).difference(set(np.unique(y_train))))))\u001B[39;00m\n\u001B[0;32m     30\u001B[0m \u001B[38;5;66;03m# if len(missing_labels) > 0:\u001B[39;00m\n\u001B[0;32m     31\u001B[0m \u001B[38;5;66;03m#     missing_labels = missing_labels - np.arange(len(missing_labels))\u001B[39;00m\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m#     print(y_proba.shape)\u001B[39;00m\n\u001B[0;32m     33\u001B[0m \u001B[38;5;66;03m#     y_proba = np.insert(y_proba, missing_labels, 0, axis=1)\u001B[39;00m\n\u001B[0;32m     34\u001B[0m \u001B[38;5;66;03m#     print(y_proba.shape)\u001B[39;00m\n\u001B[1;32m---> 35\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43my_proba\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mshape\u001B[49m)\n\u001B[0;32m     36\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(y_proba, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     37\u001B[0m \u001B[38;5;66;03m# print(y_hat.shape)\u001B[39;00m\n\u001B[0;32m     38\u001B[0m \u001B[38;5;66;03m# print(y_proba.shape)\u001B[39;00m\n",
      "\u001B[1;31mAttributeError\u001B[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "%timeit train_test(algo=\"knn\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:38:32.094021500Z",
     "start_time": "2024-01-03T17:38:28.017232400Z"
    }
   },
   "id": "464056757809dac8"
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:20:35.902801100Z",
     "start_time": "2024-01-03T17:20:35.456959400Z"
    }
   },
   "id": "33d47e6313a42e0f"
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n       35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51,\n       52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68,\n       69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85,\n       86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:20:05.948611700Z",
     "start_time": "2024-01-03T17:20:05.485614100Z"
    }
   },
   "id": "c76d7bf87d20cb65"
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [
    {
     "data": {
      "text/plain": "array({1, 2, 3}, dtype=object)"
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array({1, 2, 3})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:17:43.978246200Z",
     "start_time": "2024-01-03T17:17:43.678948400Z"
    }
   },
   "id": "a7c1501efdea2fa2"
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n       17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n       34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50,\n       51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67,\n       68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84,\n       85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99])"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:05:55.247974200Z",
     "start_time": "2024-01-03T17:05:55.201971400Z"
    }
   },
   "id": "8fddaf4756a457e5"
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "2048"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape[1]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:05:56.339010600Z",
     "start_time": "2024-01-03T17:05:56.319010900Z"
    }
   },
   "id": "b38db0e9551ea060"
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1757: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of given labels, 100, not equal to the number of columns in 'y_score', 101",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[66], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[43mget_ipython\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun_line_magic\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtimeit\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[38;5;124;43mtrain_test(algo=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mknn\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43m)\u001B[39;49m\u001B[38;5;124;43m'\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\interactiveshell.py:2417\u001B[0m, in \u001B[0;36mInteractiveShell.run_line_magic\u001B[1;34m(self, magic_name, line, _stack_depth)\u001B[0m\n\u001B[0;32m   2415\u001B[0m     kwargs[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlocal_ns\u001B[39m\u001B[38;5;124m'\u001B[39m] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_local_scope(stack_depth)\n\u001B[0;32m   2416\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mbuiltin_trap:\n\u001B[1;32m-> 2417\u001B[0m     result \u001B[38;5;241m=\u001B[39m fn(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m   2419\u001B[0m \u001B[38;5;66;03m# The code below prevents the output from being displayed\u001B[39;00m\n\u001B[0;32m   2420\u001B[0m \u001B[38;5;66;03m# when using magics with decodator @output_can_be_silenced\u001B[39;00m\n\u001B[0;32m   2421\u001B[0m \u001B[38;5;66;03m# when the last Python token in the expression is a ';'.\u001B[39;00m\n\u001B[0;32m   2422\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(fn, magic\u001B[38;5;241m.\u001B[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001B[38;5;28;01mFalse\u001B[39;00m):\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\magics\\execution.py:1170\u001B[0m, in \u001B[0;36mExecutionMagics.timeit\u001B[1;34m(self, line, cell, local_ns)\u001B[0m\n\u001B[0;32m   1168\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m index \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m10\u001B[39m):\n\u001B[0;32m   1169\u001B[0m     number \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m10\u001B[39m \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m index\n\u001B[1;32m-> 1170\u001B[0m     time_number \u001B[38;5;241m=\u001B[39m \u001B[43mtimer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimeit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnumber\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1171\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m time_number \u001B[38;5;241m>\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.2\u001B[39m:\n\u001B[0;32m   1172\u001B[0m         \u001B[38;5;28;01mbreak\u001B[39;00m\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\IPython\\core\\magics\\execution.py:158\u001B[0m, in \u001B[0;36mTimer.timeit\u001B[1;34m(self, number)\u001B[0m\n\u001B[0;32m    156\u001B[0m gc\u001B[38;5;241m.\u001B[39mdisable()\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m--> 158\u001B[0m     timing \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minner\u001B[49m\u001B[43m(\u001B[49m\u001B[43mit\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtimer\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    159\u001B[0m \u001B[38;5;28;01mfinally\u001B[39;00m:\n\u001B[0;32m    160\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m gcold:\n",
      "File \u001B[1;32m<magic-timeit>:1\u001B[0m, in \u001B[0;36minner\u001B[1;34m(_it, _timer)\u001B[0m\n",
      "Cell \u001B[1;32mIn[65], line 38\u001B[0m, in \u001B[0;36mtrain_test\u001B[1;34m(algo, pca_n_comp, knn_n_neighbors)\u001B[0m\n\u001B[0;32m     32\u001B[0m     y_proba \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39minsert(y_proba, missing_labels, \u001B[38;5;241m0\u001B[39m, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     33\u001B[0m y_hat \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39margmax(y_proba, axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m     35\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m {\n\u001B[0;32m     36\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124macc\u001B[39m\u001B[38;5;124m\"\u001B[39m: accuracy_score(y_test, y_hat),\n\u001B[0;32m     37\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mf1\u001B[39m\u001B[38;5;124m\"\u001B[39m: f1_score(y_test, y_hat, average\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmacro\u001B[39m\u001B[38;5;124m\"\u001B[39m, labels\u001B[38;5;241m=\u001B[39mlabels),\n\u001B[1;32m---> 38\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mauc\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[43mroc_auc_score\u001B[49m\u001B[43m(\u001B[49m\u001B[43my_test\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_proba\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_class\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43movr\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlabels\u001B[49m\u001B[43m)\u001B[49m,\n\u001B[0;32m     39\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124map\u001B[39m\u001B[38;5;124m\"\u001B[39m: average_precision_score(y_test, y_proba, labels\u001B[38;5;241m=\u001B[39mlabels),\n\u001B[0;32m     40\u001B[0m }\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:211\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    205\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m    206\u001B[0m     \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m    207\u001B[0m         skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m    208\u001B[0m             prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m    209\u001B[0m         )\n\u001B[0;32m    210\u001B[0m     ):\n\u001B[1;32m--> 211\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    212\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m InvalidParameterError \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m    213\u001B[0m     \u001B[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001B[39;00m\n\u001B[0;32m    214\u001B[0m     \u001B[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001B[39;00m\n\u001B[0;32m    215\u001B[0m     \u001B[38;5;66;03m# the name of the estimator by the name of the function in the error\u001B[39;00m\n\u001B[0;32m    216\u001B[0m     \u001B[38;5;66;03m# message to avoid confusion.\u001B[39;00m\n\u001B[0;32m    217\u001B[0m     msg \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39msub(\n\u001B[0;32m    218\u001B[0m         \u001B[38;5;124mr\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124mw+ must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    219\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mparameter of \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__qualname__\u001B[39m\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m must be\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m    220\u001B[0m         \u001B[38;5;28mstr\u001B[39m(e),\n\u001B[0;32m    221\u001B[0m     )\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:620\u001B[0m, in \u001B[0;36mroc_auc_score\u001B[1;34m(y_true, y_score, average, sample_weight, max_fpr, multi_class, labels)\u001B[0m\n\u001B[0;32m    618\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m multi_class \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mraise\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    619\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmulti_class must be in (\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124movo\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124movr\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m--> 620\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43m_multiclass_roc_auc_score\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    621\u001B[0m \u001B[43m        \u001B[49m\u001B[43my_true\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43my_score\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlabels\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmulti_class\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maverage\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43msample_weight\u001B[49m\n\u001B[0;32m    622\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    623\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m y_type \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbinary\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    624\u001B[0m     labels \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39munique(y_true)\n",
      "File \u001B[1;32m~\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\metrics\\_ranking.py:728\u001B[0m, in \u001B[0;36m_multiclass_roc_auc_score\u001B[1;34m(y_true, y_score, labels, multi_class, average, sample_weight)\u001B[0m\n\u001B[0;32m    726\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mParameter \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m must be ordered\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m    727\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(classes) \u001B[38;5;241m!=\u001B[39m y_score\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]:\n\u001B[1;32m--> 728\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    729\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mNumber of given labels, \u001B[39m\u001B[38;5;132;01m{0}\u001B[39;00m\u001B[38;5;124m, not equal to the number \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    730\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mof columns in \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_score\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m, \u001B[39m\u001B[38;5;132;01m{1}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;241m.\u001B[39mformat(\u001B[38;5;28mlen\u001B[39m(classes), y_score\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])\n\u001B[0;32m    731\u001B[0m     )\n\u001B[0;32m    732\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(np\u001B[38;5;241m.\u001B[39msetdiff1d(y_true, classes)):\n\u001B[0;32m    733\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124my_true\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m contains labels not in parameter \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mlabels\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mValueError\u001B[0m: Number of given labels, 100, not equal to the number of columns in 'y_score', 101"
     ]
    }
   ],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:19:14.858474600Z",
     "start_time": "2024-01-03T17:19:11.282647800Z"
    }
   },
   "id": "7171d6adad6cbead"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "970091408e9e9275"
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "array([[ 0,  0,  1,  2,  3,  0,  0,  4],\n       [ 5,  0,  6,  7,  8,  0,  0,  9],\n       [10,  0, 11, 12, 13,  0,  0, 14],\n       [15,  0, 16, 17, 18,  0,  0, 19],\n       [20,  0, 21, 22, 23,  0,  0, 24]])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.insert(np.arange(25).reshape(5, 5), np.array([1, 4, 4]), 0, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:18:51.532769800Z",
     "start_time": "2024-01-03T17:18:51.057560600Z"
    }
   },
   "id": "2de84d93066ae42"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 1, 2, 3])"
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T17:16:47.920496400Z",
     "start_time": "2024-01-03T17:16:47.460981600Z"
    }
   },
   "id": "cbe96b5df31c37e7"
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.min()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:56:10.808615100Z",
     "start_time": "2024-01-03T16:56:10.183544700Z"
    }
   },
   "id": "15dcdf32fade08f4"
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.min()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:56:16.323847500Z",
     "start_time": "2024-01-03T16:56:16.285668900Z"
    }
   },
   "id": "99d7e0effddd7ace"
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "(4711,)"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_hat.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:56:19.019295300Z",
     "start_time": "2024-01-03T16:56:18.978811600Z"
    }
   },
   "id": "e84cdbf534844736"
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "(4711,)"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T16:56:27.082894900Z",
     "start_time": "2024-01-03T16:56:26.554536600Z"
    }
   },
   "id": "41f41e5ed46aae9e"
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"C:/Data/AAIT/task1/train_data/annotations.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T19:49:37.339054300Z",
     "start_time": "2024-01-03T19:49:36.119313800Z"
    }
   },
   "id": "b28a790d830389a5"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "data": {
      "text/plain": "                                           sample  label\n0          task1/train_data/images/labeled/0.jpeg      0\n1          task1/train_data/images/labeled/1.jpeg      1\n2          task1/train_data/images/labeled/2.jpeg      2\n3          task1/train_data/images/labeled/3.jpeg      3\n4          task1/train_data/images/labeled/4.jpeg      4\n...                                           ...    ...\n23550  task1/train_data/images/labeled/23550.jpeg     97\n23551  task1/train_data/images/labeled/23551.jpeg     28\n23552  task1/train_data/images/labeled/23552.jpeg     53\n23553  task1/train_data/images/labeled/23553.jpeg      9\n23554  task1/train_data/images/labeled/23554.jpeg     90\n\n[23555 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>task1/train_data/images/labeled/0.jpeg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>task1/train_data/images/labeled/1.jpeg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>task1/train_data/images/labeled/2.jpeg</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>task1/train_data/images/labeled/3.jpeg</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>task1/train_data/images/labeled/4.jpeg</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>23550</th>\n      <td>task1/train_data/images/labeled/23550.jpeg</td>\n      <td>97</td>\n    </tr>\n    <tr>\n      <th>23551</th>\n      <td>task1/train_data/images/labeled/23551.jpeg</td>\n      <td>28</td>\n    </tr>\n    <tr>\n      <th>23552</th>\n      <td>task1/train_data/images/labeled/23552.jpeg</td>\n      <td>53</td>\n    </tr>\n    <tr>\n      <th>23553</th>\n      <td>task1/train_data/images/labeled/23553.jpeg</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>23554</th>\n      <td>task1/train_data/images/labeled/23554.jpeg</td>\n      <td>90</td>\n    </tr>\n  </tbody>\n</table>\n<p>23555 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T19:49:38.187524Z",
     "start_time": "2024-01-03T19:49:38.129490700Z"
    }
   },
   "id": "2b0f973c9c3b7402"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "       sample\nlabel        \n96         75\n89        149\n92        156\n75        167\n83        170\n...       ...\n82        280\n8         280\n3         286\n5         289\n52        289\n\n[100 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>96</th>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>149</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>156</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>170</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>280</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>280</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>286</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>289</td>\n    </tr>\n    <tr>\n      <th>52</th>\n      <td>289</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"label\").count().sort_values(\"sample\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T19:50:27.351143500Z",
     "start_time": "2024-01-03T19:50:26.101787300Z"
    }
   },
   "id": "8159e098eec5476d"
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "data": {
      "text/plain": "       sample\nlabel        \n52        289\n5         289\n3         286\n8         280\n82        280\n...       ...\n83        170\n75        167\n92        156\n89        149\n96         75\n\n[100 rows x 1 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sample</th>\n    </tr>\n    <tr>\n      <th>label</th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>52</th>\n      <td>289</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>289</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>286</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>280</td>\n    </tr>\n    <tr>\n      <th>82</th>\n      <td>280</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>83</th>\n      <td>170</td>\n    </tr>\n    <tr>\n      <th>75</th>\n      <td>167</td>\n    </tr>\n    <tr>\n      <th>92</th>\n      <td>156</td>\n    </tr>\n    <tr>\n      <th>89</th>\n      <td>149</td>\n    </tr>\n    <tr>\n      <th>96</th>\n      <td>75</td>\n    </tr>\n  </tbody>\n</table>\n<p>100 rows × 1 columns</p>\n</div>"
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(\"label\").count().sort_values(\"sample\", ascending=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T19:50:41.145325100Z",
     "start_time": "2024-01-03T19:50:41.067230Z"
    }
   },
   "id": "ad55987ec36a428a"
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1\n",
      "1/1\n",
      "2/0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\David\\.conda\\envs\\ml\\lib\\site-packages\\sklearn\\model_selection\\_split.py:725: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAssertionError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[67], line 12\u001B[0m\n\u001B[0;32m     10\u001B[0m train_zeros \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39msum(y[train_index] \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m)\n\u001B[0;32m     11\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtrain_zeros\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m/\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mtest_zeros\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m test_zeros \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m train_zeros \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m\n",
      "\u001B[1;31mAssertionError\u001B[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "X = np.arange(400).reshape((80, 5))\n",
    "y = np.sqrt(np.random.randint(0, 16, size=(80,))).astype(int)\n",
    "\n",
    "skf = StratifiedKFold(n_splits=3)\n",
    "for i, (train_index, test_index) in enumerate(skf.split(X, y)):\n",
    "    test_zeros = np.sum(y[test_index] == 0)\n",
    "    train_zeros = np.sum(y[train_index] == 0)\n",
    "    print(f\"{train_zeros}/{test_zeros}\")\n",
    "    assert test_zeros > 0 and train_zeros > 0"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T19:58:59.294775Z",
     "start_time": "2024-01-03T19:58:58.787604300Z"
    }
   },
   "id": "a2288065cd958e89"
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "data": {
      "text/plain": "array([0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n       2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n       3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3])"
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(y)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-03T19:58:17.085019600Z",
     "start_time": "2024-01-03T19:58:16.590833400Z"
    }
   },
   "id": "87c0738554e1a90e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "a4b5ead502a524cb"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
